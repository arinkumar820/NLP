{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd4d446",
   "metadata": {},
   "source": [
    "https://collectionofbestporn.com/video/girlfriend-takes-revenge-by-taping-threesome-2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b5718",
   "metadata": {},
   "source": [
    "SpaCy and NLTK  \n",
    "\n",
    "\n",
    "\n",
    "Feature\t                          SpaCy\t                                                    NLTK\n",
    "Programming Paradigm\t     Object-oriented\t                                          String processing\n",
    "Ease of Use\t                More user-friendly, intuitive API\t                  User-friendly but requires more manual setup\n",
    "Algorithm Selection\t    Automatically selects the best algorithm\t               Allows user to customize and select algorithms\n",
    "Performance\t               Efficient out-of-the-box\t                                Powerful but requires tweaking\n",
    "Community\t                  very active\t                                            Older, less active\n",
    "Target Users\t            App developers\t                                       Researchers and advanced users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3f7600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctor Strange loves power.\n",
      "Doctor\n",
      "Strange\n",
      "loves\n",
      "power\n",
      ".\n",
      "Budget Mumbai and Hull clause chart.\n",
      "Budget\n",
      "Mumbai\n",
      "and\n",
      "Hull\n",
      "clause\n",
      "chart\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Doctor Strange loves power. Budget Mumbai and Hull clause chart.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)  # Prints sentences\n",
    "    for token in sent:\n",
    "        print(token.text)  # Prints words in sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880d7d8",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into meaningful segments: sentence tokenization splits paragraphs into sentences, and word tokenization splits sentences into words. This is a key pre-processing step in NLP pipelines\n",
    "\n",
    "Token and Span Objects:-\n",
    "    SpaCy represents tokens as Token objects and slices of tokens as Span objects, enabling extraction of substrings or token subsequences with Python-like slice syntax (e.g., doc[1:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afe098",
   "metadata": {},
   "source": [
    "Here's a clear explanation of the key code snippets and concepts from the video on SpaCy tokenization:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Installing SpaCy**\n",
    "```bash\n",
    "pip install spacy\n",
    "```\n",
    "- Installs the SpaCy library for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Creating a Blank Language Object**\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")  # \"en\" for English\n",
    "```\n",
    "- Creates an **NLP pipeline object** for English.\n",
    "- This object contains a **tokenizer** by default but no other components like tagger or parser.\n",
    "- You can also create for other languages by changing `\"en\"` to `\"de\"` (German), `\"fr\"` (French), `\"hi\"` (Hindi), etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Creating a Document**\n",
    "```python\n",
    "text = \"Doctor Strange's visit in Mumbai.\"\n",
    "doc = nlp(text)\n",
    "```\n",
    "- The `doc` object represents the processed text.\n",
    "- SpaCy automatically tokenizes the text into tokens (words, punctuation, symbols).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Iterating Over Tokens**\n",
    "```python\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "- Prints each token in the text.\n",
    "- Tokens include words, punctuation, currency symbols, etc.\n",
    "- SpaCy handles special cases (e.g., splits \"Doctor Strange's\" into tokens like `\"Doctor\"`, `\"Strange\"`, `\"'s\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Accessing Token Attributes**\n",
    "```python\n",
    "token = doc[0]\n",
    "print(token.text)       # Text of the token\n",
    "print(token.is_alpha)   # Is the token alphabetic?\n",
    "print(token.is_digit)   # Is the token a digit?\n",
    "print(token.is_currency) # Is the token a currency symbol?\n",
    "```\n",
    "- Tokens have many useful attributes for analysis:\n",
    "  - `is_alpha`: True if token is alphabetic.\n",
    "  - `is_digit`: True if token is a number.\n",
    "  - `is_currency`: True if token is a currency symbol.\n",
    "- These help in filtering or classifying tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Customizing Tokenization (Special Cases)**\n",
    "```python\n",
    "from spacy.symbols import ORTH\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "doc = nlp(\"gimme the book\")\n",
    "print([token.text for token in doc])\n",
    "```\n",
    "- You can customize how SpaCy tokenizes specific words or slang.\n",
    "- Here, \"gimme\" is split into two tokens: `\"gim\"` and `\"me\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Sentence Tokenization**\n",
    "```python\n",
    "from spacy.pipeline import Sentencizer\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "text = \"Hulk and Strange both are loving their trip. Enjoying the food.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "```\n",
    "- Adds a **sentence boundary detector** to the pipeline.\n",
    "- Splits paragraphs into sentences.\n",
    "- Useful for sentence-level analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Extracting Emails from Text File**\n",
    "```python\n",
    "with open(\"student.txt\") as f:\n",
    "    text = \" \".join(f.readlines())\n",
    "\n",
    "doc = nlp(text)\n",
    "emails = [token.text for token in doc if token.like_email]\n",
    "print(emails)\n",
    "```\n",
    "- Reads a text file and joins lines into one string.\n",
    "- Extracts tokens recognized as emails using `token.like_email`.\n",
    "- Shows SpaCy‚Äôs ability to identify special token types.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Working with Other Languages (Hindi Example)**\n",
    "```python\n",
    "nlp_hi = spacy.blank(\"hi\")\n",
    "text_hi = \"‡§â‡§∏‡§®‡•á ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§™‡•à‡§∏‡•á ‡§â‡§ß‡§æ‡§∞ ‡§≤‡§ø‡§è ‡§π‡•à‡§Ç‡•§\"\n",
    "doc_hi = nlp_hi(text_hi)\n",
    "\n",
    "for token in doc_hi:\n",
    "    print(token.text, token.is_currency, token.like_num)\n",
    "```\n",
    "- Creates a blank Hindi tokenizer.\n",
    "- Tokenizes Hindi text.\n",
    "- Checks token attributes like currency and number.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table of Key Concepts\n",
    "\n",
    "| Code Concept                  | Purpose                                    |\n",
    "|------------------------------|--------------------------------------------|\n",
    "| `spacy.blank(\"en\")`           | Create blank English NLP pipeline          |\n",
    "| `doc = nlp(text)`             | Process text and tokenize                   |\n",
    "| `for token in doc:`           | Iterate over tokens                         |\n",
    "| `token.is_alpha`              | Check if token is alphabetic                |\n",
    "| `token.is_currency`           | Check if token is currency symbol           |\n",
    "| `add_special_case`            | Customize tokenization rules                 |\n",
    "| `add_pipe(\"sentencizer\")`    | Add sentence segmentation to pipeline       |\n",
    "| `token.like_email`            | Detect email tokens                          |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81915b54",
   "metadata": {},
   "source": [
    "## Tokenization Concepts and Importance in NLP\n",
    "\n",
    "- **Tokenization** is the process of splitting text into meaningful segments: *sentence tokenization* splits paragraphs into sentences, and *word tokenization* splits sentences into words. This is a key pre-processing step in NLP pipelines    .\n",
    "- Simple rules like splitting sentences by a period (`.`) are insufficient due to language complexities (e.g., \"Dr.\" does not indicate sentence end). Tokenization requires language-specific rules and understanding to handle such exceptions correctly   .\n",
    "- SpaCy provides built-in tokenizers that intelligently handle prefixes, suffixes, exceptions, punctuation, and currency symbols, making tokenization more accurate than naive splitting by spaces     .\n",
    "\n",
    "## Using SpaCy for Tokenization\n",
    "\n",
    "- To use SpaCy, install it via `pip install spacy`, then import and create a language object with `spacy.blank('en')` for English or other language codes (e.g., `de` for German, `hi` for Hindi)    .\n",
    "- Creating a `Doc` object by passing text to an `nlp` object automatically tokenizes the text into word tokens accessible via iteration or indexing, e.g., `for token in doc` or `doc[0]`    .\n",
    "- Tokens in SpaCy are objects with many useful attributes and methods such as `.is_alpha`, `.is_digit`, `.is_currency`, `.text`, etc., allowing detailed text analysis beyond simple token text extraction      .\n",
    "- SpaCy treats symbols and punctuation as separate tokens, enabling precise token boundaries (e.g., `$` and `2` are distinct tokens)   .\n",
    "\n",
    "## Token and Span Objects\n",
    "\n",
    "- SpaCy represents tokens as `Token` objects and slices of tokens as `Span` objects, enabling extraction of substrings or token subsequences with Python-like slice syntax (e.g., `doc[1:5]`)   .\n",
    "\n",
    "## Practical Use Case: Extracting Emails\n",
    "\n",
    "- SpaCy can be used to process large text files by converting lines to a single string and creating a `Doc` for tokenization. Using token attributes (like an email detector), you can extract entities such as email addresses efficiently, sometimes more conveniently than regex      .\n",
    "\n",
    "## Tokenization in Different Languages\n",
    "\n",
    "- SpaCy supports multiple languages, and tokenization respects language-specific rules. For example, Hindi tokenization can detect currency symbols and numbers correctly, though some languages may lack full pipeline support yet (e.g., Hindi currently has no pipeline)    .\n",
    "\n",
    "## Customizing the Tokenizer\n",
    "\n",
    "- SpaCy's tokenizer can be customized to handle special cases or slang by adding rules that split tokens differently (e.g., splitting \"gimme\" into \"gim\" and \"me\") using SpaCy symbols and special case handlers      .\n",
    "- Customization respects that the original text should not be altered, only the tokenization behavior changes.\n",
    "\n",
    "## Sentence Tokenization with SpaCy Pipelines\n",
    "\n",
    "- Blank SpaCy pipelines include only tokenization. To enable sentence tokenization, a `sentencizer` component must be added manually to the pipeline using `nlp.add_pipe('sentencizer')`   .\n",
    "- Full pipelines loaded from SpaCy models include multiple components (tagger, parser, named entity recognizer, etc.) for richer language understanding and better sentence splitting   .\n",
    "- The basic sentencizer may not fully resolve complex sentence boundary cases (e.g., splitting after \"Dr.\") but full models improve this accuracy  .\n",
    "\n",
    "## Exercises and Practical Advice\n",
    "\n",
    "- Exercises include extracting URLs and monetary transactions from given text paragraphs using SpaCy tokenization and attribute methods, reinforcing practical NLP skills    .\n",
    "- Emphasis on hands-on practice: watching tutorials alone is insufficient; active coding and problem solving are crucial for mastering NLP and becoming a successful NLP engineer    .\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Key Insight:** Tokenization is foundational for all NLP tasks, but requires nuanced language-specific rules beyond simple splitting. SpaCy's tokenizer and pipeline components provide powerful, customizable tools for accurate token and sentence segmentation, enabling advanced text analysis and extraction tasks.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293b709",
   "metadata": {},
   "source": [
    "## Language Processing Pipeline in spaCy\n",
    "\n",
    "- The **language processing pipeline** in spaCy is a sequence of components that process text after tokenization. A blank pipeline contains only the tokenizer by default and no additional components   .\n",
    "- Components include:\n",
    "  - **Tagger:** assigns part-of-speech tags to tokens.\n",
    "  - **Parser:** analyzes syntactic dependencies.\n",
    "  - **NER (Named Entity Recognition):** identifies entities like persons, organizations, and monetary values in text.\n",
    "  - **Lemmatizer:** finds the base form of words (lemmas)    .\n",
    "\n",
    "## Creating and Using Pipelines\n",
    "\n",
    "- Creating a blank pipeline (`spacy.blank()`) provides only tokenization without additional processing components   .\n",
    "- Pre-trained pipelines can be downloaded for different languages using commands like `spacy download en_core_web_sm`, which installs an English small model pipeline with multiple components included   .\n",
    "- Loading a pre-trained pipeline with `spacy.load()` automatically includes components such as tagger, parser, NER, and lemmatizer, enabling rich linguistic annotations   .\n",
    "\n",
    "## Components and Their Functions\n",
    "\n",
    "| Component     | Function                                                                                   |\n",
    "|---------------|--------------------------------------------------------------------------------------------|\n",
    "| Tokenizer     | Splits text into tokens                                                                    |\n",
    "| Tagger        | Assigns part-of-speech (POS) tags (e.g., noun, verb, proper noun)                          |\n",
    "| Parser        | Analyzes syntactic structure (not detailed in this video)                                 |\n",
    "| Lemmatizer   | Extracts lemma (base form) of words                                                        |\n",
    "| Named Entity Recognition (NER) | Detects named entities such as persons, organizations, and monetary values         |\n",
    "\n",
    "- **Part-of-Speech (POS)** tagging assigns grammatical categories to words, e.g., \"double\" can be a noun or verb, \"person\" is a noun, \"Tesla\" as a proper noun    .\n",
    "- **Lemmatization** converts inflected forms to their base form, e.g., \"said\" ‚Üí \"say\"  .\n",
    "- **NER** identifies entities in text and classifies them into categories like organizations (ORG), monetary values (MONEY), or persons (PERSON)    .\n",
    "\n",
    "## Visualizing Named Entities\n",
    "\n",
    "- spaCy provides a module (`spacy.displacy`) to render entities visually in an easily interpretable format, highlighting different entity types in the text  .\n",
    "\n",
    "## Customizing Pipelines\n",
    "\n",
    "- You can customize blank pipelines by adding specific components from pre-trained pipelines; for example, adding only the NER component from an English pipeline to a blank pipeline   .\n",
    "- This allows solving specific problems without loading the full pipeline, improving efficiency and control over processing   .\n",
    "\n",
    "## Multilingual Support\n",
    "\n",
    "- spaCy supports multiple languages, and pre-trained pipelines can be downloaded for languages like French, Chinese, etc. If a language pipeline is not downloaded, an error occurs when loading it, so it must be installed first    .\n",
    "- Tokenization is available for languages even if a full pipeline is not present, e.g., Hindi has tokenization but no full pipeline yet  .\n",
    "- Using language-appropriate pipelines improves accuracy of components like NER, as shown by better entity recognition when using an English pipeline for English sentences compared to mismatched languages   .\n",
    "\n",
    "## Practical Notes and Tools\n",
    "\n",
    "- spaCy pipelines can be run locally with required compute resources or accessed via cloud APIs such as firstlanguage.in, which simplifies NLP tasks through API calls without heavy local computation    .\n",
    "- Understanding each pipeline component is essential for building NLP applications; upcoming videos promise detailed explanations of POS tagging, NER, and other components    .\n",
    "\n",
    "> **üí° Key Insight:** The spaCy language processing pipeline is modular and customizable, allowing users to tailor NLP workflows for specific tasks by adding or removing components, supporting multiple languages, and leveraging pre-trained models for efficient and accurate analysis.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d78d672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9822902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | 383 | Companies, agencies, institutions, etc.\n",
      "Twitter | 380 | People, including fictional\n",
      "45 billion dollars | 394 | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for 45 billion dollars.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"|\",ent.label,\"|\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0fadb",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization in NLP Preprocessing\n",
    "\n",
    "- **Purpose:** Both stemming and lemmatization reduce words to their base or root form, which helps in NLP tasks like text classification and search (e.g., mapping \"talking,\" \"talked,\" and \"talk\" to the base word \"talk\")   .\n",
    "- **Benefit:** Mapping different word forms to a base form improves consistency and accuracy in NLP applications such as sentiment analysis by treating variations of a word as the same token  .\n",
    "\n",
    "## Stemming: Rule-Based Reduction\n",
    "\n",
    "- **Definition:** Stemming applies a fixed set of simple heuristic rules to strip suffixes or prefixes from words to obtain a root form (e.g., removing \"ing\" from \"talking\" to get \"talk\") without language understanding   .\n",
    "- **Examples of suffix removal:**  \n",
    "  - \"ing\" ‚Üí talking, walking, running ‚Üí talk, walk, run  \n",
    "  - \"able\" ‚Üí adjustable, removable ‚Üí adjust, remove  \n",
    "- **Limitations:** Stemming can produce non-words (e.g., \"ability\" reduced to \"abil\") because it does not consider linguistic context or meaning  .\n",
    "- **Tools:** NLTK supports multiple stemmers like Porter and Snowball stemmers and is often used because it is fast and simple, though less accurate than lemmatization    .\n",
    "\n",
    "## Lemmatization: Linguistically Informed Reduction\n",
    "\n",
    "- **Definition:** Lemmatization reduces words to their lemma (base form) using linguistic knowledge and rules of the language, resulting in meaningful base forms (e.g., \"ate\" ‚Üí \"eat\")   .\n",
    "- **Advantages:** Produces valid base words that consider vocabulary and language context, avoiding nonsensical roots produced by stemming (e.g., \"ability\" remains \"ability\")   .\n",
    "- **Implementation:** Libraries like spaCy provide lemmatization based on pre-trained language models that map words to lemmas using learned rules and vocabulary   .\n",
    "\n",
    "## Comparison of Stemming and Lemmatization\n",
    "\n",
    "| Aspect           | Stemming                           | Lemmatization                           |\n",
    "|------------------|----------------------------------|---------------------------------------|\n",
    "| Approach         | Rule-based heuristic stripping   | Linguistic knowledge and vocabulary   |\n",
    "| Output          | May be non-words (e.g., \"abil\")  | Valid dictionary base forms            |\n",
    "| Complexity      | Simple, fast                     | More complex, slower                   |\n",
    "| Tools           | NLTK stemmers                   | spaCy lemmatizer                      |\n",
    "| Use cases       | When speed is critical, approximate results suffice | When accuracy and meaningful roots are important |\n",
    "\n",
    "     \n",
    "\n",
    "## Demonstrations and Practical Usage\n",
    "\n",
    "- **NLTK Stemming Example:**  \n",
    "  Creating a Porter stemmer object and applying `.stem()` to words removes suffixes based on fixed rules, e.g., \"eating\" ‚Üí \"eat,\" \"adjustable\" ‚Üí \"adjust\" (though sometimes resulting in invalid roots)    .\n",
    "- **spaCy Lemmatization Example:**  \n",
    "  Loading an English model and iterating over tokens allows access to `token.lemma_` which returns the lemma, e.g., \"ate\" ‚Üí \"eat,\" \"adjustable\" ‚Üí \"adjustable,\" preserving meaningful forms based on the model's vocabulary and rules    .\n",
    "\n",
    "## Customizing Lemmatization Behavior in spaCy\n",
    "\n",
    "- **Problem:** Default language models may not recognize slang or custom words, e.g., \"bro\" remains \"bro\" as lemma  .\n",
    "- **Solution:** Customize the attribute ruler in spaCy's pipeline to assign specific lemmas to custom tokens (e.g., mapping \"bro\" and \"bra\" to lemma \"brother\") by adding custom rules to the attribute ruler component   .\n",
    "- **Effect:** After customization, queries for lemma return the assigned base word (e.g., \"bro\" ‚Üí \"brother\") enhancing model adaptability to domain-specific vocabulary  .\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- **API and Cloud Services:** Platforms like firstlanguage.in offer cloud-based NLP APIs for tasks like text classification without needing local compute resources, making NLP accessible without deep knowledge or hardware    .\n",
    "- **Resource Recommendations:** NLTK for stemming and spaCy for lemmatization are recommended tools; spaCy authors prefer lemmatization due to its linguistic accuracy, hence it lacks stemming support  .\n",
    "- **Learning Tips:** Check video descriptions for exercises, corrections, and additional resources to deepen understanding and practice stemming and lemmatization   .\n",
    "\n",
    "---\n",
    "\n",
    "> **‚ùó Important:** Stemming is faster and simpler but can produce invalid roots, whereas lemmatization is more accurate but requires linguistic knowledge and more computation, making both useful depending on the NLP application context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dae4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talking | talk\n",
      "walked | walk\n",
      "eating | eat\n",
      "adjustable | adjust\n",
      "ability | abil\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer =PorterStemmer()\n",
    "words = [\"talking\", \"walked\", \"eating\", \"adjustable\", \"ability\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1d4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talking | talk | 13939146775466599234\n",
      "walked | walk | 1674876016505392235\n",
      "eating | eat | 9837207709914848172\n",
      "adjustable | adjustable | 6033511944150694480\n",
      "ability | ability | 11565809527369121409\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"talking walked eating adjustable ability\")\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.lemma_,\"|\",token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4890691",
   "metadata": {},
   "source": [
    "## NLP Platforms and API Usage\n",
    "\n",
    "- NLP tasks can be performed using libraries such as spaCy, which can be run locally or via cloud platforms offering HTTP API calls, eliminating the need for high local compute resources like GPUs   .\n",
    "- Cloud platforms provide free tiers where users can sign up, obtain API keys, and use SDKs in Python or TypeScript for easy NLP integration without deep expertise  .\n",
    "\n",
    "## Parts of Speech Overview\n",
    "\n",
    "- **Adjectives** add meaning to nouns; **adverbs** modify verbs, adjectives, or other adverbs, providing more detail about the action (e.g., \"quickly,\" \"slowly\")   .\n",
    "- The eight basic parts of speech include pronouns, adverbs, verbs, adjectives, nouns, interjections, conjunctions, and prepositions     .\n",
    "- **Interjections** express strong emotions or reactions (e.g., \"alas,\" \"wow\")  .\n",
    "- **Conjunctions** (e.g., \"and,\" \"but,\" \"or\") connect groups of words or phrases  .\n",
    "- **Prepositions** link nouns to other words, affecting sentence meaning based on the preposition used (e.g., \"in,\" \"on,\" \"at\")   .\n",
    "\n",
    "## Detailed POS Tagging with spaCy\n",
    "\n",
    "- spaCy's POS tags extend beyond the basic eight parts of speech to include finer categories such as numerals, articles, and determiners, reflecting subcategories in English grammar   .\n",
    "- The library provides explanations for POS tags (e.g., \"PROPN\" for proper noun, \"VERB\" for verb) using `spacy.explain()` for better understanding   .\n",
    "- Proper nouns refer to specific entities (e.g., \"Elon\"), while common nouns refer to general items (e.g., \"person\")  .\n",
    "- spaCy categorizes tokens beyond just POS tags, including special tags like \"X\" for unknown or miscellaneous tokens  .\n",
    "\n",
    "## POS Tagging and Verb Tense Identification\n",
    "\n",
    "- spaCy supports detailed tagging that identifies verb tenses, such as past tense (\"VBD\") or third person singular present tense (\"VBZ\"), accessible via `token.tag_`     .\n",
    "- This capability helps in NLP applications requiring tense recognition, improving text understanding and processing accuracy   .\n",
    "\n",
    "## Practical Text Processing Examples\n",
    "\n",
    "- Real-world text (e.g., Microsoft's earnings report) can be processed with spaCy to remove unwanted tokens like punctuation, spaces, or miscellaneous characters (\"X\") based on their POS tags for cleaner analysis      .\n",
    "- Filtering tokens by POS tags allows extraction of meaningful words while discarding noise, which is critical for building effective NLP pipelines   .\n",
    "\n",
    "## Counting POS Tags in Text\n",
    "\n",
    "- spaCy provides a `count_by` API to count tokens by attributes such as POS or tag, facilitating analysis of text composition (e.g., number of nouns, proper nouns, punctuation)    .\n",
    "- This counting helps understand the structure of texts, useful in applications like text summarization or content analysis  .\n",
    "\n",
    "## Learning and Practice Recommendations\n",
    "\n",
    "- Exercises involving POS tagging, such as extracting nouns and numbers from news stories, are recommended to reinforce learning  .\n",
    "- Contributions to exercise repositories on platforms like GitHub are encouraged to expand learning resources, but students should attempt solutions independently before viewing provided answers   .\n",
    "\n",
    "> **üí° Key Insight:** Understanding and utilizing detailed POS tagging, including verb tense and subcategories, enhances NLP applications by enabling nuanced text analysis beyond basic grammar rules.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32116291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | NNP | proper noun\n",
      "went | VERB | VBD | verb\n",
      "to | ADP | IN | adposition\n",
      "Mars | PROPN | NNP | proper noun\n",
      "arin | NOUN | NN | noun\n",
      "was | AUX | VBD | auxiliary\n",
      "great | ADJ | JJ | adjective\n",
      "in | ADP | IN | adposition\n",
      "sports | NOUN | NNS | noun\n",
      "or | CCONJ | CC | coordinating conjunction\n",
      "in | ADP | IN | adposition\n",
      "coding | NOUN | NN | noun\n",
      ", | PUNCT | , | punctuation\n",
      "arin | PROPN | NNP | proper noun\n",
      "papa | PROPN | NNP | proper noun\n",
      "hai | PROPN | NNP | proper noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon went to Mars arin was great in sports or in coding, arin papa hai \")\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.pos_,\"|\",token.tag_,\"|\",spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287aa21",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) Overview and Use Cases\n",
    "- **NER** extracts and classifies entities (e.g., persons, companies, locations, products, money) from text, distinguishing between ambiguous terms like \"Tesla\" the company vs. \"Tesla\" the person. This helps identify the precise entity type within text data.    \n",
    "- **Search**: NER enables news aggregators (e.g., Google News) to tag entities in articles automatically, improving search relevance by recognizing companies, products, or persons mentioned in the text.    \n",
    "- **Recommendation Systems**: By extracting entities from user-read articles (e.g., persons, locations, production houses), recommendation engines suggest related content based on user preferences for these entities, similar to personalized movie or news suggestions.   \n",
    "- **Customer Care**: NER can automatically identify entities like course names from free-text customer queries, enabling routing to specialized support teams without requiring manual dropdown selections.    \n",
    "\n",
    "## Using spaCy for NER\n",
    "- spaCy's English model includes an NER component that identifies entities such as organizations, money, products, etc., from text using pre-trained statistical models and rule-based patterns.    \n",
    "- Entities can be accessed via `doc.ents` and their labels printed. For example, \"Tesla\" may be labeled as an organization and \"45 billion\" as money. spaCy provides explanations for entity labels (e.g., ORG = organization).    \n",
    "- Capitalization and suffixes like \".inc\" affect recognition accuracy; spaCy's default NER may miss some entity mentions or misclassify (e.g., \"Twitter\" not recognized unless capitalized). This reflects limitations of out-of-the-box models.   \n",
    "- Different pre-trained models (e.g., Hugging Face transformers) may support a different set of entity types and vary in accuracy, highlighting dependence on training data and model architecture.    \n",
    "\n",
    "## Customizing and Extending NER in spaCy\n",
    "- **Span Class**: Used to define slices of tokens as entities. For example, `Span(doc, start, end, label)` creates a custom entity span. This allows manual tagging of entities such as \"Tesla\" or \"Twitter\" as organizations.    \n",
    "- `doc.set_ents()` method updates the document‚Äôs entity annotations, enabling addition or correction of entity labels while optionally preserving existing ones.   \n",
    "- Custom entities improve recognition when the default model misses domain-specific or new entities.  \n",
    "\n",
    "## Approaches to Building Your Own NER System\n",
    "| Approach         | Description                                                                                  | Use Case/Notes                                      |\n",
    "|------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| Lookup-based     | Use a database of known entities (e.g., companies, drugs) and match tokens against it.        | Simple, naive, but practical for controlled domains.|\n",
    "| Rule-based       | Define linguistic or pattern-based rules (e.g., regex, POS tags) to identify entities.        | spaCy‚Äôs EntityRuler supports this; good for phone numbers, dates, etc.|\n",
    "| Machine Learning | Train models (e.g., Conditional Random Fields, BERT) to learn entity patterns from annotated data.| More flexible and accurate but requires annotated data and training.|\n",
    "\n",
    "- Lookup and rule-based methods can be effective for specific, controlled vocabularies or patterns without requiring complex ML models.     \n",
    "- Machine learning approaches like CRF or BERT offer better generalization but involve more complexity and resources.  \n",
    "\n",
    "## Practical Tips and Insights\n",
    "> **‚ÑπÔ∏è Note:** Out-of-the-box NER models are not perfect; they make errors stemming from training data or heuristic rules. Customization or retraining is often needed for domain-specific accuracy.    \n",
    ">\n",
    "> **üí° Key Insight:** Combining multiple approaches (lookup, rules, ML) can yield practical and efficient NER systems tailored to specific applications.    \n",
    "\n",
    "## Example spaCy NER Code Snippet\n",
    "```python\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Tesla is going to acquire Twitter.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_)\n",
    "\n",
    "# Custom entity addition\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")  # Tesla\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")  # Twitter\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_)\n",
    "```\n",
    "- This snippet shows basic extraction and manual addition of entities using spaCy's Span and set_ents methods.    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b752afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | ORG\n",
      "Twitter | PERSON\n",
      "45 billion dollars | MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Tesla is going to acquire Twitter for 45 billion dollars\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182af3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | ORG\n",
      "Twitter | ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Tesla is going to acquire Twitter\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")  # 'Tesla'\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")  # 'Twitter'\n",
    "\n",
    "doc.set_ents([s1,s2],default=\"unmodified\")\n",
    "\n",
    "# Print\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f38435",
   "metadata": {},
   "source": [
    "## Feature Engineering in Machine Learning\n",
    "\n",
    "- **Feature engineering** is a crucial step in the machine learning pipeline where data scientists spend considerable time extracting meaningful features from raw data to improve model performance  .\n",
    "- A **feature** is an individual measurable property or characteristic of a phenomenon being observed. For example, in a property price prediction problem, features include area, facilities, age of the home, and location   .\n",
    "- In image classification, features correspond to distinguishable parts such as eyes, nose, ears, and whiskers that help identify objects like cats or dogs   .\n",
    "- Neural networks identify features by assigning specific tasks to individual neurons, such as detecting cat ears or noses; if all relevant features are detected, the network confirms the presence of a cat   .\n",
    "- Human intuition works similarly when recognizing images, sometimes rejecting images if features like ears or nose do not match expectations, illustrating how feature detection influences classification  .\n",
    "\n",
    "## Text Representation as Feature Engineering in NLP\n",
    "\n",
    "- Text data must be converted into numerical features because machine learning models cannot directly process raw text; this conversion is a core part of **feature engineering** in NLP, often called **text representation**   .\n",
    "- Unlike images or tabular data, text features are less obvious; one approach is to define handcrafted features such as whether a word represents a person, location, or other categories, assigning binary values (e.g., \"Dhoni\" as person = 1, location = 0)   .\n",
    "- Words can be represented as **vectors** (sets of numbers) rather than single numbers, enabling mathematical operations like **cosine similarity** to measure semantic similarity between words   .\n",
    "- For example, vectors for \"Dhoni\" and \"comments\" may have a high cosine similarity, indicating semantic closeness, while \"Dhoni\" and \"Australia\" may be less similar; this helps NLP models understand relationships between words beyond exact matches   .\n",
    "- This vector representation is also known as the **Vector Space Model**, which can be applied to various text units such as words, phrases, sentences, or paragraphs   .\n",
    "\n",
    "## Importance and Approaches to Text Representation\n",
    "\n",
    "- Effective text representation is fundamental to NLP success; better feature extraction from text often leads to better model results than using sophisticated algorithms on poor representations   .\n",
    "- Common approaches to text representation include **one-hot encoding**, **bag of words**, and **TF-IDF**; one-hot encoding is less popular due to sparsity issues, while bag of words is widely used in classic NLP tasks like spam detection   .\n",
    "- Feature engineering in NLP is about transforming raw text into meaningful numerical vectors that capture semantic and syntactic properties, enabling machine learning algorithms to perform tasks like classification, sentiment analysis, and more   .\n",
    "\n",
    "> **üí° Key Insight:** \"Feeding a good representation to an ordinary algorithm will get you much farther than applying top-notch algorithm to an ordinary text representation file.\" This highlights the critical role of feature engineering over model complexity in NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f864e25",
   "metadata": {},
   "source": [
    "## Introduction to Spam Detection and Text Classification\n",
    "\n",
    "- Spam detection is a classical text classification problem within the domain of Natural Language Processing (NLP). Gmail uses machine learning to accurately classify emails as spam or ham (non-spam) by analyzing keywords and urgency cues in the text.   \n",
    "- Machine learning models only understand numerical data, so text must be converted into numerical vectors, a process known as text representation or feature engineering. This vectorization enables models like Naive Bayes classifiers to distinguish spam from ham.   \n",
    "\n",
    "## Basic Text Representation Approaches: Label Encoding and One Hot Encoding\n",
    "\n",
    "- **Label Encoding:** Create a vocabulary from all unique words in the training emails, assign each word a unique numeric index, and represent a text by a list (vector) of these indices. This is a primitive way to convert text to numbers but is simple and intuitive.     \n",
    "- **One Hot Encoding:** Using the vocabulary, represent each word as a vector where the position corresponding to the word‚Äôs index is set to 1 and all other positions are 0. This creates sparse vectors indicating presence or absence of words.    \n",
    "\n",
    "## Disadvantages of Label Encoding and One Hot Encoding\n",
    "\n",
    "| Disadvantage | Explanation |\n",
    "|--------------|-------------|\n",
    "| **1. Lack of semantic similarity** | One hot vectors treat all words as independent; similar words like \"help\" and \"assistance\" have completely different vectors, failing to capture meaning or similarity.    |\n",
    "| **2. High memory consumption** | Large vocabularies (e.g., 100,000 words) create very large vectors for each word, leading to excessive memory usage, especially for long texts with many words.    |\n",
    "| **3. Out-of-vocabulary (OOV) problem** | Words not in the vocabulary (e.g., new or rare words like \"bahubali\") cannot be accurately represented, often lumped into a generic \"unknown\" token, losing distinctiveness.    |\n",
    "| **4. Variable input size for models** | Different texts have different lengths, so flattening one hot vectors leads to inconsistent input sizes, which is problematic for fixed-size input requirements in machine learning models like neural networks.    |\n",
    "\n",
    "- Label encoding avoids the large vector size issue but still does not capture semantic relationships or solve the OOV problem. Both approaches are considered \"dumb\" or primitive for text representation in modern NLP.    \n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "- Label encoding and one hot encoding are foundational but outdated methods for text representation due to their inability to encode meaning, inefficiency, and inflexibility.  \n",
    "- Modern NLP typically uses more advanced techniques such as word embeddings and TF-IDF to overcome these issues.\n",
    "- The next topic to explore is the \"Bag of Words\" model, which will be accompanied by coding exercises to deepen practical understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7c3f6",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW) Technique in NLP\n",
    "\n",
    "- Bag of Words is a text representation technique where a vocabulary of unique words is created from a corpus, and each document is represented as a vector of word counts based on that vocabulary. This vector is called a **count vectorizer**    .\n",
    "- Example: For news articles about companies like Tesla and Apple, key terms such as \"Elon Musk,\" \"Model 3,\" and \"iPhone\" help identify the company. Counting occurrences of these words in articles allows classification of the document by company    .\n",
    "- Vocabulary is built by collecting all unique words from documents (after stemming or lemmatization), which can be very large (e.g., 10,000+ words), leading to high-dimensional vectors   .\n",
    "\n",
    "## Processing and Limitations of Bag of Words\n",
    "\n",
    "- Each document is transformed into a vector where each element is the frequency of a vocabulary word in that document. This results in a **sparse vector** because most words do not appear in every document, leading to many zero values   .\n",
    "- BOW does not capture word meanings or semantic similarity. For example, \"help\" and \"assistance\" are treated as different words despite similar meanings, resulting in different vector representations  .\n",
    "- BOW vectors are generally smaller than one-hot encoding vectors for individual words but still consume significant memory and computational resources due to sparsity   .\n",
    "\n",
    "## Practical Application: Spam Email Classification\n",
    "\n",
    "- The tutorial uses BOW with a dataset of over 5,000 emails labeled as spam or non-spam (ham). The dataset is imbalanced with more non-spam emails   .\n",
    "- Labels are converted to numeric form (spam = 1, ham = 0) using pandas `apply` method with a lambda function or a custom function for transformation    .\n",
    "- Data is split into training (80%) and test (20%) sets using scikit-learn's `train_test_split` to avoid biased model evaluation    .\n",
    "\n",
    "## Using CountVectorizer in Scikit-learn\n",
    "\n",
    "- `CountVectorizer` converts text documents into count vectors representing word frequencies. It builds the vocabulary and transforms emails into sparse matrices    .\n",
    "- The sparse matrix can be converted to a dense NumPy array for inspection. The vocabulary size in the example is 7,675 unique words    .\n",
    "- Vocabulary words can be accessed and indexed to understand which word corresponds to which position in the vector, aiding interpretation of the feature vectors    .\n",
    "- Non-zero elements in vectors correspond to words present in the email; their indices map back to vocabulary words, showing how the vector represents the text content      .\n",
    "\n",
    "## Building and Evaluating a Naive Bayes Classifier\n",
    "\n",
    "- A multinomial Naive Bayes classifier is used because it suits discrete count data like word frequencies. The model is trained by calling the `fit` method on training vectors and labels    .\n",
    "- The test emails are transformed using the same `CountVectorizer` and predictions are made. Performance is evaluated using `classification_report` from scikit-learn, which provides precision, recall, and F1-score, important metrics especially for imbalanced datasets    .\n",
    "- Example: Emails with phrases like \"55 million dollars\" and \"exclusive offer\" are typically predicted as spam, demonstrating the model's practical utility   .\n",
    "\n",
    "## Simplifying Workflow with Scikit-learn Pipeline\n",
    "\n",
    "- Scikit-learn's `Pipeline` allows chaining of preprocessing (`CountVectorizer`) and classification (`MultinomialNB`) into a single object, simplifying code and avoiding manual vector transformations during training and prediction    .\n",
    "- Pipeline usage improves code readability and reduces errors by automating the sequence of transformations and model fitting/prediction steps    .\n",
    "\n",
    "---\n",
    "\n",
    "> **‚ÑπÔ∏è Note:** Bag of Words is simple and interpretable but has limitations in semantic understanding and high dimensionality. Despite this, it can achieve good accuracy on classical tasks like spam detection when combined with suitable classifiers like Naive Bayes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1dd166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv(\"C:/Users/arink/Downloads/spam.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e247eb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13fadca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     0\n",
       "1      ham                      Ok lar... Joking wif u oni...     0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3      ham  U dun say so early hor... U c already then say...     0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"spam\"]=data[\"Category\"].apply(lambda x: 1 if x==\"spam\" else 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a466cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.Message, data.spam, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55aac98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5517    Miles and smiles r made frm same letters but d...\n",
       "3120                             Stop knowing me so well!\n",
       "1683    HI BABE U R MOST LIKELY TO BE IN BED BUT IM SO...\n",
       "3261    I'm always looking for an excuse to be in the ...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c5e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _cs_matrix.toarray of <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 59312 stored elements and shape (4457, 7753)>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "X_train_cv=v.fit_transform(X_train.values)\n",
    "X_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637a969d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.toarray()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c213d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a0db25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       944\n",
      "           1       0.99      0.89      0.94       171\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_test_cv = v.transform(X_test)\n",
    "y_pred = model.predict(X_test_cv)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13b00c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help find yoga mat website\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "chat_text = \"Can you help me find a yoga mat on your website?\"\n",
    "clean_text = remove_stopwords(chat_text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dabd06",
   "metadata": {},
   "source": [
    "## Bag of n-grams and Vocabulary Challenges\n",
    "- Traditional bag of words models do not solve the out-of-vocabulary (OOV) problem, where new words in prediction are unseen during training and thus hard to vectorize. This limitation affects performance and memory usage  .\n",
    "- The `CountVectorizer` class from sklearn supports an `n_gram_range` parameter, allowing generation of n-grams beyond unigrams (single words). Setting `n_gram_range=(2,2)` creates bigrams; `(1,3)` includes unigrams, bigrams, and trigrams in the vocabulary   .\n",
    "- N-grams capture sequences of tokens, e.g., bigrams are pairs of consecutive words, which can provide richer contextual information than single tokens  .\n",
    "\n",
    "## Text Preprocessing for Vectorization\n",
    "- Preprocessing includes removing stop words and lemmatization (reducing words to their base form), implemented using the spaCy library. Tokens identified as stop words or punctuation are filtered out, and lemmas are joined back into a string for vectorization     .\n",
    "- Example: \"Loki is eating pizza\" preprocesses to \"Loki eat pizza,\" removing \"is\" (stop word) and converting \"eating\" to its lemma \"eat\"  .\n",
    "- Preprocessing improves the quality of input text before applying n-gram vectorization, making vectors more representative of the core content  .\n",
    "\n",
    "## Vectorization and Representation\n",
    "- After preprocessing, applying `CountVectorizer` with `n_gram_range=(1,2)` generates a vocabulary including unigrams and bigrams, with each token or n-gram assigned an index for vector representation  .\n",
    "- Text is converted to numeric vectors using the vector space model, essential because machine learning models require numeric input rather than raw text  .\n",
    "- Example: A sentence like \"Thor eat pizza\" is transformed into a sparse vector indicating presence (1) or absence (0) of vocabulary tokens or n-grams     .\n",
    "- OOV words (e.g., \"Hulk\" not in training vocabulary) cannot be represented, illustrating the OOV problem in practice  .\n",
    "\n",
    "## News Category Classification Dataset and Handling Imbalance\n",
    "- The dataset used contains news articles labeled into six categories (e.g., business, sports, science), loaded into a pandas DataFrame from JSON format   .\n",
    "- Exploratory analysis shows class imbalance; some categories (like science) have fewer samples than others (business, sports)  .\n",
    "- To address imbalance, under-sampling is applied by randomly selecting the minimum number of samples present among classes (138 in this case) to create a balanced dataset, acknowledging that discarding data is generally not ideal in real scenarios    .\n",
    "- Balanced subsets for each category are concatenated row-wise using `pd.concat` to form a balanced DataFrame for training    .\n",
    "\n",
    "## Preparing Data for Machine Learning\n",
    "- Categories (strings) are mapped to numeric labels using pandas `.map()` to allow model training, as models expect numeric targets    .\n",
    "- The balanced dataset is split into training and testing sets with an 80-20 ratio, using a `random_state` for reproducibility and `stratify` to maintain class distribution in both sets  .\n",
    "- Stratification ensures equal representation of all classes in train and test, preventing model bias towards majority classes   .\n",
    "\n",
    "## Model Training and Evaluation\n",
    "- A pipeline is created combining the vectorizer (bag of words or n-grams) and a classifier (Multinomial Naive Bayes recommended for text classification, though other classifiers like KNN, Random Forest, Decision Trees can be compared)   .\n",
    "- The model is trained on training data and predictions are made on the test set; performance is evaluated with a classification report showing metrics like precision, recall, and F1-score   .\n",
    "- Experimentation shows bag of words (unigrams) often performs better than bigrams or trigrams on this dataset, but results may vary based on problem specifics and require trial and error   .\n",
    "- Example predictions demonstrate correct classification of news categories, showing the model's practical application   .\n",
    "\n",
    "## Effect of Preprocessing on Model Performance\n",
    "- Training the same model with preprocessed text (stop words removed, lemmatized) improves classification metrics compared to using raw text, as shown by higher F1-scores across most classes      .\n",
    "- Preprocessing is generally recommended for NLP tasks, though some cases may not benefit; it depends on the problem context  .\n",
    "\n",
    "## Practice and Further Learning\n",
    "- Exercises and additional resources (like confusion matrix plotting code) are provided in the video description and notebook to encourage hands-on practice, emphasizing that active coding is essential for mastering NLP and machine learning   .\n",
    "\n",
    "---\n",
    "\n",
    "> **‚ÑπÔ∏è Note:** The bag of n-grams approach extends bag of words by capturing contiguous sequences of tokens, improving contextual representation but increasing vocabulary size and computational cost. Preprocessing (stop word removal, lemmatization) enhances vector quality and model performance, especially in text classification tasks with imbalanced data handled via sampling techniques.  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86830e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 12,\n",
       " 'ate': 0,\n",
       " 'pizza': 8,\n",
       " 'or': 5,\n",
       " 'eat': 3,\n",
       " 'samosa': 11,\n",
       " 'thor ate': 13,\n",
       " 'ate pizza': 1,\n",
       " 'pizza or': 9,\n",
       " 'or eat': 6,\n",
       " 'eat samosa': 4,\n",
       " 'thor ate pizza': 14,\n",
       " 'ate pizza or': 2,\n",
       " 'pizza or eat': 10,\n",
       " 'or eat samosa': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v=CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\"Thor ate pizza or eat samosa\"])\n",
    "v.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Loki is eating pizza\",\n",
    "    \"Thor ate pizza\",\n",
    "    \"Hulk likes pizza\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ec9c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arin greet person greate code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocesse(text):\n",
    "    doc=nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)\n",
    "preprocesse(\"arin was a greet person and he doing greate in coding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebfa06",
   "metadata": {},
   "source": [
    "## TF-IDF Representation in NLP\n",
    "\n",
    "- **Background:** The video builds on the Bag of Words (BoW) and Bag of n-grams models used for text classification, focusing on classifying news articles by company mentions such as Tesla or Apple   .\n",
    "- **Issue with BoW:** Generic terms like \"price,\" \"market,\" \"investor\" appear frequently across documents and suppress meaningful, relevant terms (e.g., \"iphone,\" \"musk\"), misleading the model into thinking unrelated articles are similar due to these common words    .\n",
    "- **Stop Words Limitation:** Removing stop words helps but does not fully solve the problem because many generic but domain-relevant terms still affect the model's accuracy   .\n",
    "\n",
    "## Document Frequency and Inverse Document Frequency (IDF)\n",
    "\n",
    "- **Document Frequency (DF):** Measures in how many documents a term appears, not how many times per document. For example, \"gigafactory\" appears in only one document, while \"price\" appears in three out of four documents  .\n",
    "- **Inverse Document Frequency (IDF):** To reduce the influence of common terms, IDF is calculated as the logarithm of the ratio of total documents to the number of documents containing the term:\n",
    "  \n",
    "  $$\n",
    "  \\text{IDF}(t) = \\log \\frac{N}{n_t}\n",
    "  $$\n",
    "  \n",
    "  where \\(N\\) is total documents and \\(n_t\\) is documents containing term \\(t\\). Terms appearing in fewer documents get higher IDF scores, emphasizing their importance     .\n",
    "- **Use of Logarithm:** Logarithm dampens the effect of extremely high term frequencies, preventing rare terms from dominating and common terms from being overly penalized. The log function flattens as frequency increases, stabilizing influence    .\n",
    "\n",
    "## Term Frequency (TF) and Combining with IDF\n",
    "\n",
    "- **Term Frequency (TF):** Normalizes word count by the total number of tokens in a document to account for document length differences:\n",
    "  \n",
    "  $$\n",
    "  \\text{TF}(t,d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}\n",
    "  $$\n",
    "  \n",
    "  This prevents bias toward longer documents   .\n",
    "- **TF-IDF Score:** Calculated as the product of TF and IDF, balancing term importance within a document and across the corpus. Relevant terms have higher TF-IDF scores, while common terms get lower scores   .\n",
    "\n",
    "## Limitations of TF-IDF\n",
    "\n",
    "- **Sparsity:** As vocabulary size grows, TF-IDF vectors become sparse, which can affect model performance  .\n",
    "- **Lack of Semantic Relationship:** TF-IDF does not capture relationships between words; unlike word or sentence embeddings, it only counts term frequency and document distribution  .\n",
    "- **Out-of-Vocabulary Problem:** Words not in the training vocabulary cannot be represented, limiting model generalization  .\n",
    "\n",
    "## Practical TF-IDF Implementation (Using scikit-learn)\n",
    "\n",
    "- **TF-IDF Vectorizer:** Create an instance, then fit and transform a corpus (collection of documents) to generate TF-IDF vectors   .\n",
    "- **Vocabulary and IDF Scores:** Vocabulary maps terms to indices; IDF scores can be accessed via the vectorizer object to inspect term importance (e.g., common terms like \"is\" have low IDF, rare terms like \"apple\" have high IDF)      .\n",
    "- **TF-IDF Vectors:** Transformed output is a sparse matrix; converting it to an array allows inspection of TF-IDF scores per document and term      .\n",
    "\n",
    "## Application: E-Commerce Text Classification\n",
    "\n",
    "- **Dataset:** Uses Amazon item descriptions labeled into four categories (e.g., electronics, household, books, clothing) with balanced classes (6000 items each)     .\n",
    "- **Label Encoding:** Converts categorical labels to numerical form for machine learning compatibility using pandas mapping functions   .\n",
    "- **Train-Test Split:** Splits data into 80% training and 20% testing sets using stratified sampling to preserve class balance   .\n",
    "\n",
    "## Model Training and Evaluation\n",
    "\n",
    "- **Classifiers Used:** K-Nearest Neighbors (KNN), Random Forest, and Multinomial Naive Bayes classifiers are trained to compare performance   .\n",
    "- **Pipeline:** Utilizes sklearn pipelines to chain TF-IDF vectorization with classifiers for streamlined training and prediction   .\n",
    "- **Performance Metrics:** Classification reports show precision, recall, and F1 scores around 95-98%, indicating strong model performance across categories    .\n",
    "- **Prediction Inspection:** Individual predictions align well with true labels, showing the model's effectiveness in classifying e-commerce product descriptions    .\n",
    "- **Classifier Comparison:** Random Forest often yields the best performance, but choice depends on data and problem context; Naive Bayes is a common starting point for text classification    .\n",
    "\n",
    "## Text Preprocessing Impact\n",
    "\n",
    "- **Preprocessing Steps:** Removal of stop words, punctuation, and lemmatization to normalize text improves model accuracy   .\n",
    "- **Implementation:** Applies preprocessing function to text column using pandas `.apply()` method, creating a new preprocessed text column   .\n",
    "- **Retraining:** Models trained on preprocessed text show slightly better F1 scores (up to 99%), confirming the value of preprocessing in text classification tasks   .\n",
    "- **General Guideline:** Preprocessing is recommended but effectiveness may vary depending on dataset and task  .\n",
    "\n",
    "## Learning and Practice Advice\n",
    "\n",
    "> **üí° Key Insight:** Mastery of machine learning and NLP requires consistent practice beyond watching tutorials; coding along and completing exercises is essential for skill development.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e04764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
    "    \"Apple is announcing new iphone tomorrow\",\n",
    "    \"Tesla is announcing new model-3 tomorrow\",\n",
    "    \"Google is announcing new pixel-6 tomorrow\",\n",
    "    \"Microsoft is announcing new surface tomorrow\",\n",
    "    \"Amazon is announcing new eco-dot tomorrow\",\n",
    "    \"I am eating biryani and you are eating grapes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593a55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer()\n",
    "v.fit(corpus)\n",
    "transform_output = v.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb59b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n"
     ]
    }
   ],
   "source": [
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c47214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already : 2.386294361119891\n",
      "am : 2.386294361119891\n",
      "amazon : 2.386294361119891\n",
      "and : 2.386294361119891\n",
      "announcing : 1.2876820724517808\n",
      "apple : 2.386294361119891\n",
      "are : 2.386294361119891\n",
      "ate : 2.386294361119891\n",
      "biryani : 2.386294361119891\n",
      "dot : 2.386294361119891\n",
      "eating : 1.9808292530117262\n",
      "eco : 2.386294361119891\n",
      "google : 2.386294361119891\n",
      "grapes : 2.386294361119891\n",
      "iphone : 2.386294361119891\n",
      "ironman : 2.386294361119891\n",
      "is : 1.1335313926245225\n",
      "loki : 2.386294361119891\n",
      "microsoft : 2.386294361119891\n",
      "model : 2.386294361119891\n",
      "new : 1.2876820724517808\n",
      "pixel : 2.386294361119891\n",
      "pizza : 2.386294361119891\n",
      "surface : 2.386294361119891\n",
      "tesla : 2.386294361119891\n",
      "thor : 2.386294361119891\n",
      "tomorrow : 1.2876820724517808\n",
      "you : 2.386294361119891\n"
     ]
    }
   ],
   "source": [
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "    \n",
    "    #let's get the index in the vocabulary\n",
    "    indx = v.vocabulary_.get(word)\n",
    "    \n",
    "    #get the score\n",
    "    idf_score = v.idf_[indx]\n",
    "    \n",
    "    print(f\"{word} : {idf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8358a82",
   "metadata": {},
   "source": [
    "## Shortcomings of Traditional Text Vectorization Techniques\n",
    "- Traditional text vectorization methods often produce high-dimensional sparse vectors with many zeros and fail to capture the semantic meaning of words properly. For example, two similar sentences might have very different vectors under these methods, limiting their usefulness in understanding language meaning.  \n",
    "\n",
    "## Word Embeddings: Concept and Advantages\n",
    "- **Word embeddings** are dense, lower-dimensional vector representations of words (commonly 50, 100, or 300 dimensions) where similar words have similar vectors, effectively capturing semantic relationships. For instance, the vectors for \"good\" and \"great\" are close but not identical, reflecting their similarity.   \n",
    "- These dense vectors contrast with sparse vectors from older techniques, which had many zero values, improving efficiency and expressiveness.  \n",
    "\n",
    "## Popular Word Embedding Techniques\n",
    "- Common word embedding methods include **Word2Vec**, **GloVe**, and **FastText**, which rely on approaches like Continuous Bag of Words (CBOW) and Skip-Gram models to learn word representations from large corpora.   \n",
    "- More recent transformer-based embeddings, such as **BERT** and **GPT**, represent advanced NLP models that capture context more effectively and are used in applications like Google Search.  \n",
    "\n",
    "## Additional Embedding Models and Their Foundations\n",
    "- Models like **ELMo** use LSTM-based architectures to generate embeddings, showing the diversity of techniques for capturing word meanings.  \n",
    "- These models convert words or sentences into vectors that not only represent meaning but also allow for arithmetic operations on vectors, revealing semantic relationships (e.g., \"king\" - \"man\" + \"woman\" ‚âà \"queen\"). This arithmetic property is a powerful demonstration of embedding capabilities.   \n",
    "\n",
    "## Training Variations Based on Data Corpora\n",
    "- Word embedding models can be trained on different datasets (corpora), resulting in variations tailored to specific domains or language styles. For example, Word2Vec trained on Google News differs from one trained on Twitter data, the latter better capturing slang and informal language.  \n",
    "- Similarly, transformer-based models like BERT have domain-specific versions such as **BioBERT** for biomedical text and **FinBERT** for financial data, illustrating how training data influences embeddings.  \n",
    "\n",
    "## Overview of Jargon and Further Learning Recommendations\n",
    "- Numerous jargons like Word2Vec, GloVe, FastText, BERT, BioBERT, FinBERT, ALBERT, RoBERTa, and Virtuac represent various embedding techniques or model variants, often reflecting differences in architecture or training data.    \n",
    "- For a deeper understanding, especially of Word2Vec, dedicated tutorials are recommended as these foundational techniques underpin many NLP applications.   \n",
    "\n",
    "## Summary of Purpose and Scope of Word Embeddings\n",
    "- The primary goal of word embedding techniques is to convert text‚Äîwhether a single word, sentence, paragraph, or entire document‚Äîinto numerical vectors that machines can process. This enables machine learning models to understand and work with textual data effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb292a9",
   "metadata": {},
   "source": [
    "## Accessing Word Vectors in SpaCy\n",
    "\n",
    "- To use word vectors in SpaCy, you must load either the **medium** (`md`) or **large** (`lg`) English model, as the **small** model has no vectors (zero keys, zero unique vectors)   .\n",
    "- The medium model contains about 514 keys and vectors of 300 dimensions; the large model has many more unique vectors (around 20k) with the same dimension size   .\n",
    "- Install the large model via command line with `python -m spacy download en_core_web_lg`; expect a large download size and some time for installation  .\n",
    "\n",
    "## Working with Tokens and Vectors\n",
    "\n",
    "- You can iterate over tokens in a document and check properties like `token.has_vector` (boolean if the token has a vector) and `token.is_oov` (out-of-vocabulary flag)  .\n",
    "- Common words like \"dog\", \"cat\", and \"banana\" have vectors, while random or regional words (e.g., \"kem's\") do not, because they were not seen in the training corpus   .\n",
    "- SpaCy uses **GloVe embeddings** trained on large English datasets (e.g., Wikipedia, news articles) to capture general English language knowledge  .\n",
    "\n",
    "## Vector Representation and Dimensions\n",
    "\n",
    "- Each word vector is a 300-dimensional numpy array accessible by `token.vector` with shape `(300,)`   .\n",
    "- Sentence vectors in SpaCy are computed as the average of the individual word vectors in the sentence, so a single-word sentence vector equals that word‚Äôs vector  .\n",
    "\n",
    "## Measuring Similarity Between Words\n",
    "\n",
    "- Similarity between two tokens can be computed using `token1.similarity(token2)`, which returns a value between 0 and 1, where 1 means identical vectors   .\n",
    "- Words appearing in similar contexts tend to have higher similarity scores. For example, \"bread\" and \"sandwich\" have a higher similarity than \"bread\" and \"car\" because they co-occur in related contexts   .\n",
    "- Similarity reflects **contextual closeness**, not semantic equivalence. Antonyms like \"profit\" and \"loss\" may have high similarity because they appear in similar contexts despite opposite meanings   .\n",
    "\n",
    "## Practical Function to Print Similarities\n",
    "\n",
    "- A reusable function can be created to compare a base word against a list of words, printing their similarity scores, facilitating quick exploration of vector similarities in SpaCy  .\n",
    "\n",
    "## Examples of Similarity Observations\n",
    "\n",
    "| Word Pair          | Similarity Score | Explanation                                   |\n",
    "|--------------------|------------------|-----------------------------------------------|\n",
    "| bread - sandwich   | ~0.6             | High contextual similarity                    |\n",
    "| bread - burger     | ~0.4             | Moderate similarity                           |\n",
    "| bread - car        | ~0.06            | Very low similarity, unrelated contexts      |\n",
    "| iPhone - Samsung   | ~0.67            | Often compared together in news/Wikipedia    |\n",
    "| iPhone - apple     | ~0.43            | Less similarity due to context in training data |\n",
    "| dog - iPhone       | ~0.08            | Minimal similarity, unrelated concepts       |\n",
    "\n",
    "- Similarity scores depend heavily on the training corpus and context frequency, not just conceptual relationships    .\n",
    "\n",
    "## Out-of-Vocabulary (OOV) Words\n",
    "\n",
    "- Words not present in the training corpus (OOV) have no vector and `token.has_vector` is `False`. For example, regional language words like Gujarati terms will be OOV in the English SpaCy model   .\n",
    "\n",
    "## Vector Arithmetic and Analogies\n",
    "\n",
    "- Vector arithmetic can capture semantic relationships, e.g., `king - man + woman ‚âà queen`  .\n",
    "- Cosine similarity is used to measure how close the resulting vector is to the target vector (e.g., queen) with values above 0.5 considered reasonably similar   .\n",
    "\n",
    "> **‚ùó Important:** Word vector similarity measures contextual similarity based on co-occurrence patterns in training data, not dictionary definitions or antonymy. Interpret results accordingly.  \n",
    ">    \n",
    "\n",
    "## Summary of Key Concepts\n",
    "\n",
    "| Concept                       | Description                                                                                       |\n",
    "|------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| Word vectors                 | Dense 300-dimensional vectors representing words, capturing semantic and contextual info          |\n",
    "| Medium vs Large SpaCy models | Medium: ~514 vectors; Large: ~20k vectors; both 300-dimensional                                   |\n",
    "| Token properties             | `has_vector` (bool), `is_oov` (bool)                                                             |\n",
    "| Similarity measure           | `token1.similarity(token2)` returns context-based similarity (0 to 1)                             |\n",
    "| Sentence vector              | Average of constituent word vectors                                                               |\n",
    "| OOV words                   | Tokens without vectors due to absence in training corpus                                          |\n",
    "| Vector arithmetic            | Enables semantic analogies, e.g., king - man + woman ‚âà queen                                      |\n",
    "\n",
    "## Recommended Further Topics\n",
    "\n",
    "- Explore cosine similarity in detail to understand vector comparison metrics better  .\n",
    "- Future tutorials will cover word vectors in Gensim and practical text classification applications using embeddings  ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319ad44",
   "metadata": {},
   "source": [
    "## Label Encoding for Classification\n",
    "- Convert categorical labels (e.g., \"fake\" or \"real\") into numerical format using a mapping function: fake ‚Üí 0, real ‚Üí 1, creating a new label number column for model training.  \n",
    "\n",
    "## Using SpaCy Word Vectors\n",
    "- Load a large SpaCy model that includes pre-trained 300-dimensional word vectors to convert text data into dense vector representations.   \n",
    "- Convert each news article text into a SpaCy document object, then extract its `.vector` attribute to get the 300-element word vector.  \n",
    "- Apply this vectorization to every row in the pandas DataFrame using `apply()` with a lambda function, creating a new column storing vectors for each text.  \n",
    "- This vectorization process is computationally expensive and may take several minutes to complete for large datasets.  \n",
    "\n",
    "## Preparing Data for Model Training\n",
    "- Use `train_test_split` to divide data into training and testing sets, specifying features (X as vectors) and labels (Y as label numbers), with a typical test size of 20%. Set a random state for reproducibility.  \n",
    "- Extract numpy arrays of vectors with `.values` but note that the result is an array of arrays, which is not suitable for scikit-learn classifiers expecting a 2D array.  \n",
    "- Convert the nested array structure into a proper 2D numpy array using `numpy.stack()`, ensuring compatibility with classifiers.   \n",
    "\n",
    "## Model Training and Challenges\n",
    "- Import and train a multinomial Naive Bayes classifier, commonly used for NLP tasks due to its effectiveness with text data.  \n",
    "- Multinomial Naive Bayes requires non-negative feature values; SpaCy vectors contain negative values causing errors.  \n",
    "- Apply Min-Max scaling to transform vectors into a positive range using `MinMaxScaler` from scikit-learn. Use `fit_transform` on training data and `transform` on test data to maintain consistency.   \n",
    "\n",
    "## Model Evaluation\n",
    "- After training, predict labels on the test set and generate a classification report showing precision, recall, and F1 score to evaluate model performance.  \n",
    "- The multinomial Naive Bayes model achieves robust results with over 90% in key metrics, demonstrating effectiveness using SpaCy embeddings.   \n",
    "\n",
    "## Alternative Classifier: K-Nearest Neighbors (KNN)\n",
    "- Train a KNN classifier (with k=5 neighbors) on the same SpaCy vector features; KNN typically struggles with high-dimensional sparse data but performs well with dense 300-dimensional vectors.  \n",
    "- KNN achieves near-perfect precision and recall (~99%), outperforming previous TF-IDF or bag-of-ngrams approaches where KNN struggled due to high dimensionality.  \n",
    "- Dense embeddings reduce dimensionality issues, allowing KNN to excel compared to sparse vector representations.   \n",
    "\n",
    "## Summary and Practical Notes\n",
    "| Step                     | Description                                                                                  |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------|\n",
    "| Label Encoding           | Map text labels to numeric values for classification                                        |\n",
    "| Vectorization            | Convert texts to 300-dimensional SpaCy word vectors                                         |\n",
    "| Data Preparation         | Use `numpy.stack()` to create 2D arrays compatible with scikit-learn                         |\n",
    "| Scaling                  | Apply Min-Max scaling to ensure non-negative features for multinomial Naive Bayes            |\n",
    "| Model Training           | Train multinomial Naive Bayes and KNN classifiers                                           |\n",
    "| Evaluation               | Evaluate with precision, recall, F1 score; KNN shows superior performance with dense vectors |\n",
    "\n",
    "- Using pre-trained SpaCy embeddings simplifies feature engineering compared to manual TF-IDF or bag-of-ngrams approaches, speeding up the NLP classification pipeline.  \n",
    "- Exercises related to this tutorial are often posted separately; reviewing them is recommended for deeper learning and practice.  \n",
    "\n",
    "> **üí° Key Insight:** Dense vector representations from SpaCy enable simpler and more effective classification, especially benefiting models like KNN that struggle with high-dimensional sparse data.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ceabfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake\n",
       "1  U.S. conservative leader optimistic of common ...  Real\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df =pd.read_csv(r\"C:\\Users\\arink\\Downloads\\Fake_Real_Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81cba19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Fake    5000\n",
       "Real    4900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ea0015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0\n",
       "1  U.S. conservative leader optimistic of common ...  Real          1\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_num']=df['label'].map({'Fake':0,'Real':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ba0b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/400.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.5/400.7 MB 2.1 MB/s eta 0:03:11\n",
      "     ---------------------------------------- 0.8/400.7 MB 1.8 MB/s eta 0:03:47\n",
      "     ---------------------------------------- 1.3/400.7 MB 2.0 MB/s eta 0:03:23\n",
      "     ---------------------------------------- 2.1/400.7 MB 2.4 MB/s eta 0:02:47\n",
      "     ---------------------------------------- 3.1/400.7 MB 2.8 MB/s eta 0:02:25\n",
      "     ---------------------------------------- 3.9/400.7 MB 3.0 MB/s eta 0:02:12\n",
      "     ---------------------------------------- 4.7/400.7 MB 3.1 MB/s eta 0:02:10\n",
      "      --------------------------------------- 5.2/400.7 MB 3.1 MB/s eta 0:02:08\n",
      "      --------------------------------------- 6.0/400.7 MB 3.2 MB/s eta 0:02:03\n",
      "      --------------------------------------- 6.8/400.7 MB 3.2 MB/s eta 0:02:03\n",
      "      --------------------------------------- 7.6/400.7 MB 3.3 MB/s eta 0:01:59\n",
      "      --------------------------------------- 8.1/400.7 MB 3.2 MB/s eta 0:02:02\n",
      "      --------------------------------------- 8.4/400.7 MB 3.2 MB/s eta 0:02:05\n",
      "      --------------------------------------- 8.7/400.7 MB 3.1 MB/s eta 0:02:09\n",
      "      --------------------------------------- 9.2/400.7 MB 2.9 MB/s eta 0:02:16\n",
      "      --------------------------------------- 9.4/400.7 MB 2.9 MB/s eta 0:02:18\n",
      "      -------------------------------------- 10.2/400.7 MB 2.8 MB/s eta 0:02:19\n",
      "     - ------------------------------------- 10.5/400.7 MB 2.8 MB/s eta 0:02:21\n",
      "     - ------------------------------------- 11.3/400.7 MB 2.8 MB/s eta 0:02:19\n",
      "     - ------------------------------------- 11.8/400.7 MB 2.9 MB/s eta 0:02:17\n",
      "     - ------------------------------------- 12.6/400.7 MB 2.8 MB/s eta 0:02:17\n",
      "     - ------------------------------------- 12.8/400.7 MB 2.8 MB/s eta 0:02:17\n",
      "     - ------------------------------------- 13.1/400.7 MB 2.8 MB/s eta 0:02:20\n",
      "     - ------------------------------------- 13.4/400.7 MB 2.7 MB/s eta 0:02:22\n",
      "     - ------------------------------------- 13.6/400.7 MB 2.7 MB/s eta 0:02:25\n",
      "     - ------------------------------------- 14.2/400.7 MB 2.6 MB/s eta 0:02:30\n",
      "     - ------------------------------------- 14.4/400.7 MB 2.6 MB/s eta 0:02:30\n",
      "     - ------------------------------------- 14.7/400.7 MB 2.5 MB/s eta 0:02:34\n",
      "     - ------------------------------------- 14.7/400.7 MB 2.5 MB/s eta 0:02:34\n",
      "     - ------------------------------------- 14.9/400.7 MB 2.5 MB/s eta 0:02:37\n",
      "     - ------------------------------------- 15.2/400.7 MB 2.4 MB/s eta 0:02:41\n",
      "     - ------------------------------------- 15.5/400.7 MB 2.4 MB/s eta 0:02:44\n",
      "     - ------------------------------------- 15.5/400.7 MB 2.4 MB/s eta 0:02:44\n",
      "     - ------------------------------------- 15.7/400.7 MB 2.3 MB/s eta 0:02:50\n",
      "     - ------------------------------------- 16.3/400.7 MB 2.2 MB/s eta 0:02:53\n",
      "     - ------------------------------------- 16.5/400.7 MB 2.2 MB/s eta 0:02:53\n",
      "     - ------------------------------------- 16.8/400.7 MB 2.2 MB/s eta 0:02:54\n",
      "     - ------------------------------------- 17.0/400.7 MB 2.2 MB/s eta 0:02:56\n",
      "     - ------------------------------------- 17.3/400.7 MB 2.2 MB/s eta 0:02:58\n",
      "     - ------------------------------------- 17.6/400.7 MB 2.1 MB/s eta 0:03:00\n",
      "     - ------------------------------------- 18.1/400.7 MB 2.1 MB/s eta 0:03:02\n",
      "     - ------------------------------------- 18.4/400.7 MB 2.1 MB/s eta 0:03:02\n",
      "     - ------------------------------------- 18.6/400.7 MB 2.1 MB/s eta 0:03:03\n",
      "     - ------------------------------------- 18.9/400.7 MB 2.1 MB/s eta 0:03:03\n",
      "     - ------------------------------------- 19.1/400.7 MB 2.1 MB/s eta 0:03:05\n",
      "     - ------------------------------------- 19.4/400.7 MB 2.0 MB/s eta 0:03:07\n",
      "     - ------------------------------------- 19.7/400.7 MB 2.0 MB/s eta 0:03:11\n",
      "     - ------------------------------------- 19.9/400.7 MB 2.0 MB/s eta 0:03:13\n",
      "     - ------------------------------------- 19.9/400.7 MB 2.0 MB/s eta 0:03:13\n",
      "     - ------------------------------------- 20.2/400.7 MB 1.9 MB/s eta 0:03:17\n",
      "     - ------------------------------------- 20.4/400.7 MB 1.9 MB/s eta 0:03:19\n",
      "     -- ------------------------------------ 20.7/400.7 MB 1.9 MB/s eta 0:03:19\n",
      "     -- ------------------------------------ 21.0/400.7 MB 1.9 MB/s eta 0:03:20\n",
      "     -- ------------------------------------ 21.2/400.7 MB 1.9 MB/s eta 0:03:20\n",
      "     -- ------------------------------------ 21.5/400.7 MB 1.9 MB/s eta 0:03:21\n",
      "     -- ------------------------------------ 21.8/400.7 MB 1.9 MB/s eta 0:03:22\n",
      "     -- ------------------------------------ 22.0/400.7 MB 1.9 MB/s eta 0:03:23\n",
      "     -- ------------------------------------ 22.3/400.7 MB 1.9 MB/s eta 0:03:24\n",
      "     -- ------------------------------------ 22.5/400.7 MB 1.9 MB/s eta 0:03:25\n",
      "     -- ------------------------------------ 23.1/400.7 MB 1.8 MB/s eta 0:03:25\n",
      "     -- ------------------------------------ 23.3/400.7 MB 1.8 MB/s eta 0:03:25\n",
      "     -- ------------------------------------ 23.6/400.7 MB 1.8 MB/s eta 0:03:27\n",
      "     -- ------------------------------------ 24.1/400.7 MB 1.8 MB/s eta 0:03:25\n",
      "     -- ------------------------------------ 24.4/400.7 MB 1.8 MB/s eta 0:03:27\n",
      "     -- ------------------------------------ 24.6/400.7 MB 1.8 MB/s eta 0:03:27\n",
      "     -- ------------------------------------ 24.9/400.7 MB 1.8 MB/s eta 0:03:28\n",
      "     -- ------------------------------------ 25.2/400.7 MB 1.8 MB/s eta 0:03:29\n",
      "     -- ------------------------------------ 25.2/400.7 MB 1.8 MB/s eta 0:03:29\n",
      "     -- ------------------------------------ 25.2/400.7 MB 1.8 MB/s eta 0:03:29\n",
      "     -- ------------------------------------ 25.4/400.7 MB 1.8 MB/s eta 0:03:33\n",
      "     -- ------------------------------------ 25.7/400.7 MB 1.8 MB/s eta 0:03:34\n",
      "     -- ------------------------------------ 26.0/400.7 MB 1.8 MB/s eta 0:03:35\n",
      "     -- ------------------------------------ 26.2/400.7 MB 1.7 MB/s eta 0:03:37\n",
      "     -- ------------------------------------ 26.2/400.7 MB 1.7 MB/s eta 0:03:37\n",
      "     -- ------------------------------------ 26.2/400.7 MB 1.7 MB/s eta 0:03:37\n",
      "     -- ------------------------------------ 26.5/400.7 MB 1.7 MB/s eta 0:03:42\n",
      "     -- ------------------------------------ 26.5/400.7 MB 1.7 MB/s eta 0:03:42\n",
      "     -- ------------------------------------ 26.7/400.7 MB 1.7 MB/s eta 0:03:46\n",
      "     -- ------------------------------------ 27.0/400.7 MB 1.7 MB/s eta 0:03:46\n",
      "     -- ------------------------------------ 27.3/400.7 MB 1.7 MB/s eta 0:03:46\n",
      "     -- ------------------------------------ 27.5/400.7 MB 1.6 MB/s eta 0:03:48\n",
      "     -- ------------------------------------ 27.5/400.7 MB 1.6 MB/s eta 0:03:48\n",
      "     -- ------------------------------------ 27.8/400.7 MB 1.6 MB/s eta 0:03:49\n",
      "     -- ------------------------------------ 28.0/400.7 MB 1.6 MB/s eta 0:03:50\n",
      "     -- ------------------------------------ 28.3/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 28.6/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 29.1/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 29.4/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 29.4/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 29.6/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     -- ------------------------------------ 30.1/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     -- ------------------------------------ 30.4/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     -- ------------------------------------ 30.7/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     --- ----------------------------------- 30.9/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 31.5/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     --- ----------------------------------- 31.7/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     --- ----------------------------------- 32.0/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 32.2/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 32.5/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 32.8/400.7 MB 1.6 MB/s eta 0:03:53\n",
      "     --- ----------------------------------- 33.0/400.7 MB 1.6 MB/s eta 0:03:53\n",
      "     --- ----------------------------------- 33.0/400.7 MB 1.6 MB/s eta 0:03:53\n",
      "     --- ----------------------------------- 33.3/400.7 MB 1.6 MB/s eta 0:03:54\n",
      "     --- ----------------------------------- 33.6/400.7 MB 1.6 MB/s eta 0:03:54\n",
      "     --- ----------------------------------- 34.1/400.7 MB 1.6 MB/s eta 0:03:54\n",
      "     --- ----------------------------------- 34.6/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 34.6/400.7 MB 1.6 MB/s eta 0:03:52\n",
      "     --- ----------------------------------- 34.9/400.7 MB 1.6 MB/s eta 0:03:53\n",
      "     --- ----------------------------------- 34.9/400.7 MB 1.6 MB/s eta 0:03:53\n",
      "     --- ----------------------------------- 35.1/400.7 MB 1.6 MB/s eta 0:03:56\n",
      "     --- ----------------------------------- 35.4/400.7 MB 1.5 MB/s eta 0:03:57\n",
      "     --- ----------------------------------- 35.4/400.7 MB 1.5 MB/s eta 0:03:57\n",
      "     --- ----------------------------------- 35.7/400.7 MB 1.5 MB/s eta 0:03:59\n",
      "     --- ----------------------------------- 35.9/400.7 MB 1.5 MB/s eta 0:04:00\n",
      "     --- ----------------------------------- 35.9/400.7 MB 1.5 MB/s eta 0:04:00\n",
      "     --- ----------------------------------- 36.4/400.7 MB 1.5 MB/s eta 0:04:00\n",
      "     --- ----------------------------------- 36.7/400.7 MB 1.5 MB/s eta 0:04:00\n",
      "     --- ----------------------------------- 37.0/400.7 MB 1.5 MB/s eta 0:04:00\n",
      "     --- ----------------------------------- 37.5/400.7 MB 1.5 MB/s eta 0:03:59\n",
      "     --- ----------------------------------- 37.7/400.7 MB 1.5 MB/s eta 0:03:59\n",
      "     --- ----------------------------------- 38.0/400.7 MB 1.5 MB/s eta 0:03:59\n",
      "     --- ----------------------------------- 38.5/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 38.8/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 39.1/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 39.3/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 39.6/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 39.8/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 40.1/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     --- ----------------------------------- 40.4/400.7 MB 1.5 MB/s eta 0:03:57\n",
      "     --- ----------------------------------- 40.6/400.7 MB 1.5 MB/s eta 0:03:58\n",
      "     ---- ---------------------------------- 41.2/400.7 MB 1.5 MB/s eta 0:03:56\n",
      "     ---- ---------------------------------- 41.4/400.7 MB 1.5 MB/s eta 0:03:56\n",
      "     ---- ---------------------------------- 41.7/400.7 MB 1.5 MB/s eta 0:03:57\n",
      "     ---- ---------------------------------- 42.5/400.7 MB 1.5 MB/s eta 0:03:54\n",
      "     ---- ---------------------------------- 43.3/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 43.5/400.7 MB 1.6 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 43.8/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 44.0/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 44.3/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 44.3/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 44.8/400.7 MB 1.5 MB/s eta 0:03:52\n",
      "     ---- ---------------------------------- 45.4/400.7 MB 1.5 MB/s eta 0:03:50\n",
      "     ---- ---------------------------------- 45.6/400.7 MB 1.5 MB/s eta 0:03:50\n",
      "     ---- ---------------------------------- 46.1/400.7 MB 1.6 MB/s eta 0:03:49\n",
      "     ---- ---------------------------------- 46.7/400.7 MB 1.6 MB/s eta 0:03:48\n",
      "     ---- ---------------------------------- 46.9/400.7 MB 1.6 MB/s eta 0:03:47\n",
      "     ---- ---------------------------------- 47.2/400.7 MB 1.6 MB/s eta 0:03:47\n",
      "     ---- ---------------------------------- 47.2/400.7 MB 1.6 MB/s eta 0:03:47\n",
      "     ---- ---------------------------------- 47.2/400.7 MB 1.6 MB/s eta 0:03:47\n",
      "     ---- ---------------------------------- 47.2/400.7 MB 1.6 MB/s eta 0:03:47\n",
      "     ---- ---------------------------------- 48.2/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 48.2/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 48.2/400.7 MB 1.5 MB/s eta 0:03:51\n",
      "     ---- ---------------------------------- 48.5/400.7 MB 1.5 MB/s eta 0:04:03\n",
      "     ---- ---------------------------------- 48.5/400.7 MB 1.5 MB/s eta 0:04:03\n",
      "     ---- ---------------------------------- 48.5/400.7 MB 1.5 MB/s eta 0:04:03\n",
      "     ---- ---------------------------------- 48.5/400.7 MB 1.5 MB/s eta 0:04:03\n",
      "     ---- ---------------------------------- 49.3/400.7 MB 1.4 MB/s eta 0:04:16\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 1.4 MB/s eta 0:04:17\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.3/400.7 MB 1.3 MB/s eta 0:04:35\n",
      "     ---- ---------------------------------- 50.9/400.7 MB 1.2 MB/s eta 0:04:46\n",
      "     ---- ---------------------------------- 50.9/400.7 MB 1.2 MB/s eta 0:04:46\n",
      "     ---- ---------------------------------- 51.1/400.7 MB 1.2 MB/s eta 0:04:48\n",
      "     ---- ---------------------------------- 51.1/400.7 MB 1.2 MB/s eta 0:04:48\n",
      "     ----- --------------------------------- 51.4/400.7 MB 1.2 MB/s eta 0:04:48\n",
      "     ----- --------------------------------- 51.4/400.7 MB 1.2 MB/s eta 0:04:48\n",
      "     ----- --------------------------------- 51.4/400.7 MB 1.2 MB/s eta 0:04:48\n",
      "     ----- --------------------------------- 51.6/400.7 MB 1.2 MB/s eta 0:04:51\n",
      "     ----- --------------------------------- 51.6/400.7 MB 1.2 MB/s eta 0:04:51\n",
      "     ----- --------------------------------- 51.9/400.7 MB 1.2 MB/s eta 0:04:53\n",
      "     ----- --------------------------------- 51.9/400.7 MB 1.2 MB/s eta 0:04:53\n",
      "     ----- --------------------------------- 52.2/400.7 MB 1.2 MB/s eta 0:04:56\n",
      "     ----- --------------------------------- 52.4/400.7 MB 1.2 MB/s eta 0:04:56\n",
      "     ----- --------------------------------- 52.4/400.7 MB 1.2 MB/s eta 0:04:56\n",
      "     ----- --------------------------------- 52.4/400.7 MB 1.2 MB/s eta 0:04:56\n",
      "     ----- --------------------------------- 52.4/400.7 MB 1.2 MB/s eta 0:04:56\n",
      "     ----- --------------------------------- 52.7/400.7 MB 1.1 MB/s eta 0:05:04\n",
      "     ----- --------------------------------- 52.7/400.7 MB 1.1 MB/s eta 0:05:04\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.0/400.7 MB 1.1 MB/s eta 0:05:07\n",
      "     ----- --------------------------------- 53.2/400.7 MB 1.1 MB/s eta 0:05:20\n",
      "     ----- --------------------------------- 53.2/400.7 MB 1.1 MB/s eta 0:05:20\n",
      "     ----- --------------------------------- 53.2/400.7 MB 1.1 MB/s eta 0:05:20\n",
      "     ----- --------------------------------- 53.2/400.7 MB 1.1 MB/s eta 0:05:20\n",
      "     ----- --------------------------------- 53.5/400.7 MB 1.1 MB/s eta 0:05:30\n",
      "     ----- --------------------------------- 53.5/400.7 MB 1.1 MB/s eta 0:05:30\n",
      "     ----- --------------------------------- 53.5/400.7 MB 1.1 MB/s eta 0:05:30\n",
      "     ----- --------------------------------- 53.7/400.7 MB 1.0 MB/s eta 0:05:34\n",
      "     ----- --------------------------------- 53.7/400.7 MB 1.0 MB/s eta 0:05:34\n",
      "     ----- --------------------------------- 54.0/400.7 MB 1.0 MB/s eta 0:05:40\n",
      "     ----- --------------------------------- 54.0/400.7 MB 1.0 MB/s eta 0:05:40\n",
      "     ----- --------------------------------- 54.3/400.7 MB 1.0 MB/s eta 0:05:44\n",
      "     ----- --------------------------------- 54.3/400.7 MB 1.0 MB/s eta 0:05:44\n",
      "     ----- ------------------------------- 54.5/400.7 MB 999.3 kB/s eta 0:05:47\n",
      "     ----- ------------------------------- 54.5/400.7 MB 999.3 kB/s eta 0:05:47\n",
      "     ----- ------------------------------- 54.8/400.7 MB 990.5 kB/s eta 0:05:50\n",
      "     ----- ------------------------------- 55.1/400.7 MB 996.2 kB/s eta 0:05:47\n",
      "     ----- ------------------------------- 55.1/400.7 MB 996.2 kB/s eta 0:05:47\n",
      "     ----- ------------------------------- 55.3/400.7 MB 989.5 kB/s eta 0:05:50\n",
      "     ----- ------------------------------- 55.3/400.7 MB 989.5 kB/s eta 0:05:50\n",
      "     ----- ------------------------------- 55.6/400.7 MB 985.9 kB/s eta 0:05:51\n",
      "     ----- ------------------------------- 55.8/400.7 MB 993.2 kB/s eta 0:05:48\n",
      "     ----- ------------------------------- 55.8/400.7 MB 993.2 kB/s eta 0:05:48\n",
      "     ----- ------------------------------- 56.1/400.7 MB 991.0 kB/s eta 0:05:48\n",
      "     ----- ------------------------------- 56.1/400.7 MB 991.0 kB/s eta 0:05:48\n",
      "     ----- ------------------------------- 56.1/400.7 MB 991.0 kB/s eta 0:05:48\n",
      "     ----- ------------------------------- 56.4/400.7 MB 984.3 kB/s eta 0:05:50\n",
      "     ----- ------------------------------- 56.4/400.7 MB 984.3 kB/s eta 0:05:50\n",
      "     ----- ------------------------------- 56.6/400.7 MB 979.2 kB/s eta 0:05:52\n",
      "     ----- ------------------------------- 56.9/400.7 MB 980.7 kB/s eta 0:05:51\n",
      "     ----- ------------------------------- 56.9/400.7 MB 980.7 kB/s eta 0:05:51\n",
      "     ----- ------------------------------- 57.1/400.7 MB 975.0 kB/s eta 0:05:53\n",
      "     ----- ------------------------------- 57.4/400.7 MB 973.0 kB/s eta 0:05:53\n",
      "     ----- ------------------------------- 57.4/400.7 MB 973.0 kB/s eta 0:05:53\n",
      "     ----- ------------------------------- 57.7/400.7 MB 962.7 kB/s eta 0:05:57\n",
      "     ----- ------------------------------- 57.7/400.7 MB 962.7 kB/s eta 0:05:57\n",
      "     ----- ------------------------------- 57.9/400.7 MB 953.0 kB/s eta 0:06:00\n",
      "     ----- ------------------------------- 58.2/400.7 MB 952.5 kB/s eta 0:06:00\n",
      "     ----- ------------------------------- 58.2/400.7 MB 952.5 kB/s eta 0:06:00\n",
      "     ----- ------------------------------- 58.5/400.7 MB 937.4 kB/s eta 0:06:06\n",
      "     ----- ------------------------------- 58.7/400.7 MB 938.4 kB/s eta 0:06:05\n",
      "     ----- ------------------------------- 58.7/400.7 MB 938.4 kB/s eta 0:06:05\n",
      "     ----- ------------------------------- 59.0/400.7 MB 935.0 kB/s eta 0:06:06\n",
      "     ----- ------------------------------- 59.2/400.7 MB 929.1 kB/s eta 0:06:08\n",
      "     ----- ------------------------------- 59.5/400.7 MB 927.7 kB/s eta 0:06:08\n",
      "     ----- ------------------------------- 59.8/400.7 MB 926.7 kB/s eta 0:06:08\n",
      "     ----- ------------------------------- 60.0/400.7 MB 921.8 kB/s eta 0:06:10\n",
      "     ----- ------------------------------- 60.0/400.7 MB 921.8 kB/s eta 0:06:10\n",
      "     ----- ------------------------------- 60.3/400.7 MB 920.8 kB/s eta 0:06:10\n",
      "     ----- ------------------------------- 60.3/400.7 MB 920.8 kB/s eta 0:06:10\n",
      "     ----- ------------------------------- 60.3/400.7 MB 920.8 kB/s eta 0:06:10\n",
      "     ----- ------------------------------- 60.6/400.7 MB 904.3 kB/s eta 0:06:17\n",
      "     ----- ------------------------------- 60.6/400.7 MB 904.3 kB/s eta 0:06:17\n",
      "     ----- ------------------------------- 60.6/400.7 MB 904.3 kB/s eta 0:06:17\n",
      "     ----- ------------------------------- 61.1/400.7 MB 879.3 kB/s eta 0:06:27\n",
      "     ----- ------------------------------- 61.3/400.7 MB 885.8 kB/s eta 0:06:24\n",
      "     ----- ------------------------------- 61.6/400.7 MB 889.0 kB/s eta 0:06:22\n",
      "     ----- ------------------------------- 61.6/400.7 MB 889.0 kB/s eta 0:06:22\n",
      "     ----- ------------------------------- 61.9/400.7 MB 891.3 kB/s eta 0:06:21\n",
      "     ----- ------------------------------- 62.1/400.7 MB 892.7 kB/s eta 0:06:20\n",
      "     ----- ------------------------------- 62.4/400.7 MB 893.2 kB/s eta 0:06:19\n",
      "     ----- ------------------------------- 62.7/400.7 MB 897.8 kB/s eta 0:06:17\n",
      "     ----- ------------------------------- 62.9/400.7 MB 897.8 kB/s eta 0:06:17\n",
      "     ----- ------------------------------- 63.2/400.7 MB 894.1 kB/s eta 0:06:18\n",
      "     ----- ------------------------------- 63.2/400.7 MB 894.1 kB/s eta 0:06:18\n",
      "     ----- ------------------------------- 63.4/400.7 MB 893.6 kB/s eta 0:06:18\n",
      "     ----- ------------------------------- 63.7/400.7 MB 892.2 kB/s eta 0:06:18\n",
      "     ----- ------------------------------- 64.0/400.7 MB 892.7 kB/s eta 0:06:18\n",
      "     ----- ------------------------------- 64.5/400.7 MB 885.8 kB/s eta 0:06:20\n",
      "     ----- ------------------------------- 64.5/400.7 MB 885.8 kB/s eta 0:06:20\n",
      "     ----- ------------------------------- 64.7/400.7 MB 876.1 kB/s eta 0:06:24\n",
      "     ------ ------------------------------ 65.0/400.7 MB 877.5 kB/s eta 0:06:23\n",
      "     ------ ------------------------------ 65.3/400.7 MB 876.6 kB/s eta 0:06:23\n",
      "     ------ ------------------------------ 65.3/400.7 MB 876.6 kB/s eta 0:06:23\n",
      "     ------ ------------------------------ 65.5/400.7 MB 859.9 kB/s eta 0:06:30\n",
      "     ------ ------------------------------ 65.5/400.7 MB 859.9 kB/s eta 0:06:30\n",
      "     ------ ------------------------------ 65.8/400.7 MB 856.3 kB/s eta 0:06:32\n",
      "     ------ ------------------------------ 66.1/400.7 MB 851.1 kB/s eta 0:06:34\n",
      "     ------ ------------------------------ 66.3/400.7 MB 849.8 kB/s eta 0:06:34\n",
      "     ------ ------------------------------ 66.6/400.7 MB 848.1 kB/s eta 0:06:34\n",
      "     ------ ------------------------------ 66.8/400.7 MB 847.6 kB/s eta 0:06:34\n",
      "     ------ ------------------------------ 67.1/400.7 MB 841.0 kB/s eta 0:06:37\n",
      "     ------ ------------------------------ 67.4/400.7 MB 822.7 kB/s eta 0:06:46\n",
      "     ------ ------------------------------ 67.6/400.7 MB 806.8 kB/s eta 0:06:53\n",
      "     ------ ------------------------------ 68.2/400.7 MB 815.6 kB/s eta 0:06:48\n",
      "     ------ ------------------------------ 68.4/400.7 MB 816.5 kB/s eta 0:06:47\n",
      "     ------ ------------------------------ 68.7/400.7 MB 821.4 kB/s eta 0:06:45\n",
      "     ------ ------------------------------ 68.9/400.7 MB 822.2 kB/s eta 0:06:44\n",
      "     ------ ------------------------------ 69.5/400.7 MB 835.3 kB/s eta 0:06:37\n",
      "     ------ ------------------------------ 70.0/400.7 MB 831.9 kB/s eta 0:06:38\n",
      "     ------ ------------------------------ 70.3/400.7 MB 830.5 kB/s eta 0:06:38\n",
      "     ------ ------------------------------ 70.8/400.7 MB 831.4 kB/s eta 0:06:37\n",
      "     ------ ------------------------------ 71.3/400.7 MB 835.8 kB/s eta 0:06:35\n",
      "     ------ ------------------------------ 71.6/400.7 MB 841.0 kB/s eta 0:06:32\n",
      "     ------ ------------------------------ 72.1/400.7 MB 832.7 kB/s eta 0:06:35\n",
      "     ------ ------------------------------ 72.9/400.7 MB 865.2 kB/s eta 0:06:19\n",
      "     ------ ------------------------------ 73.1/400.7 MB 870.4 kB/s eta 0:06:17\n",
      "     ------ ------------------------------ 73.4/400.7 MB 873.7 kB/s eta 0:06:15\n",
      "     ------ ------------------------------ 73.9/400.7 MB 884.9 kB/s eta 0:06:10\n",
      "     ------ ------------------------------ 74.4/400.7 MB 880.2 kB/s eta 0:06:11\n",
      "     ------ ------------------------------ 75.0/400.7 MB 891.4 kB/s eta 0:06:06\n",
      "     ------ ------------------------------ 75.2/400.7 MB 895.9 kB/s eta 0:06:04\n",
      "     ------ ------------------------------ 75.5/400.7 MB 913.1 kB/s eta 0:05:57\n",
      "     ------ ------------------------------ 75.8/400.7 MB 917.2 kB/s eta 0:05:55\n",
      "     ------- ----------------------------- 76.0/400.7 MB 918.8 kB/s eta 0:05:54\n",
      "     ------- ----------------------------- 76.3/400.7 MB 920.4 kB/s eta 0:05:53\n",
      "     ------- ----------------------------- 76.5/400.7 MB 907.6 kB/s eta 0:05:58\n",
      "     ------- ----------------------------- 76.8/400.7 MB 943.8 kB/s eta 0:05:44\n",
      "     ------- ----------------------------- 77.1/400.7 MB 945.7 kB/s eta 0:05:43\n",
      "     ------- ----------------------------- 77.3/400.7 MB 948.1 kB/s eta 0:05:42\n",
      "     ------- ----------------------------- 77.6/400.7 MB 950.0 kB/s eta 0:05:41\n",
      "     ------- ----------------------------- 77.9/400.7 MB 952.3 kB/s eta 0:05:39\n",
      "     ------- ----------------------------- 77.9/400.7 MB 952.3 kB/s eta 0:05:39\n",
      "     ------- ----------------------------- 78.4/400.7 MB 957.0 kB/s eta 0:05:37\n",
      "     ------- ----------------------------- 78.6/400.7 MB 974.0 kB/s eta 0:05:31\n",
      "     ------- ----------------------------- 78.9/400.7 MB 975.2 kB/s eta 0:05:30\n",
      "     ------- ----------------------------- 78.9/400.7 MB 975.2 kB/s eta 0:05:30\n",
      "     ------- ----------------------------- 79.2/400.7 MB 976.3 kB/s eta 0:05:30\n",
      "     ------- ----------------------------- 79.4/400.7 MB 977.5 kB/s eta 0:05:29\n",
      "     ------- ----------------------------- 79.7/400.7 MB 979.1 kB/s eta 0:05:28\n",
      "     ------- ----------------------------- 80.2/400.7 MB 978.1 kB/s eta 0:05:28\n",
      "     ------- ----------------------------- 80.5/400.7 MB 981.2 kB/s eta 0:05:27\n",
      "     ------- ----------------------------- 80.7/400.7 MB 985.9 kB/s eta 0:05:25\n",
      "     ------- ----------------------------- 81.0/400.7 MB 988.5 kB/s eta 0:05:24\n",
      "     ------- ------------------------------- 81.3/400.7 MB 1.0 MB/s eta 0:05:19\n",
      "     ------- ------------------------------- 81.5/400.7 MB 1.0 MB/s eta 0:05:18\n",
      "     ------- ------------------------------- 81.8/400.7 MB 1.0 MB/s eta 0:05:17\n",
      "     ------- ------------------------------- 81.8/400.7 MB 1.0 MB/s eta 0:05:17\n",
      "     -------- ------------------------------ 82.3/400.7 MB 1.0 MB/s eta 0:05:14\n",
      "     -------- ------------------------------ 82.8/400.7 MB 1.0 MB/s eta 0:05:09\n",
      "     -------- ------------------------------ 83.4/400.7 MB 1.0 MB/s eta 0:05:05\n",
      "     -------- ------------------------------ 83.6/400.7 MB 1.0 MB/s eta 0:05:05\n",
      "     -------- ------------------------------ 83.9/400.7 MB 1.1 MB/s eta 0:04:58\n",
      "     -------- ------------------------------ 83.9/400.7 MB 1.1 MB/s eta 0:04:58\n",
      "     -------- ------------------------------ 84.1/400.7 MB 1.1 MB/s eta 0:04:58\n",
      "     -------- ------------------------------ 84.7/400.7 MB 1.1 MB/s eta 0:04:56\n",
      "     -------- ------------------------------ 84.9/400.7 MB 1.1 MB/s eta 0:04:54\n",
      "     -------- ------------------------------ 85.5/400.7 MB 1.1 MB/s eta 0:04:50\n",
      "     -------- ------------------------------ 85.7/400.7 MB 1.1 MB/s eta 0:04:49\n",
      "     -------- ------------------------------ 86.0/400.7 MB 1.1 MB/s eta 0:04:35\n",
      "     -------- ------------------------------ 86.5/400.7 MB 1.2 MB/s eta 0:04:33\n",
      "     -------- ------------------------------ 87.0/400.7 MB 1.2 MB/s eta 0:04:31\n",
      "     -------- ------------------------------ 87.6/400.7 MB 1.2 MB/s eta 0:04:28\n",
      "     -------- ------------------------------ 87.6/400.7 MB 1.2 MB/s eta 0:04:28\n",
      "     -------- ------------------------------ 88.1/400.7 MB 1.2 MB/s eta 0:04:27\n",
      "     -------- ------------------------------ 88.3/400.7 MB 1.2 MB/s eta 0:04:27\n",
      "     -------- ------------------------------ 88.9/400.7 MB 1.2 MB/s eta 0:04:18\n",
      "     -------- ------------------------------ 89.4/400.7 MB 1.2 MB/s eta 0:04:16\n",
      "     -------- ------------------------------ 89.7/400.7 MB 1.2 MB/s eta 0:04:15\n",
      "     -------- ------------------------------ 89.7/400.7 MB 1.2 MB/s eta 0:04:15\n",
      "     -------- ------------------------------ 89.9/400.7 MB 1.2 MB/s eta 0:04:16\n",
      "     -------- ------------------------------ 90.2/400.7 MB 1.2 MB/s eta 0:04:14\n",
      "     -------- ------------------------------ 90.4/400.7 MB 1.2 MB/s eta 0:04:14\n",
      "     -------- ------------------------------ 91.0/400.7 MB 1.2 MB/s eta 0:04:09\n",
      "     -------- ------------------------------ 91.5/400.7 MB 1.3 MB/s eta 0:04:07\n",
      "     -------- ------------------------------ 92.3/400.7 MB 1.3 MB/s eta 0:04:02\n",
      "     --------- ----------------------------- 92.8/400.7 MB 1.3 MB/s eta 0:04:00\n",
      "     --------- ----------------------------- 92.8/400.7 MB 1.3 MB/s eta 0:04:00\n",
      "     --------- ----------------------------- 92.8/400.7 MB 1.3 MB/s eta 0:04:00\n",
      "     --------- ----------------------------- 93.8/400.7 MB 1.3 MB/s eta 0:03:54\n",
      "     --------- ----------------------------- 93.8/400.7 MB 1.3 MB/s eta 0:03:54\n",
      "     --------- ----------------------------- 94.6/400.7 MB 1.3 MB/s eta 0:03:51\n",
      "     --------- ----------------------------- 94.9/400.7 MB 1.3 MB/s eta 0:03:50\n",
      "     --------- ----------------------------- 95.2/400.7 MB 1.3 MB/s eta 0:03:48\n",
      "     --------- ----------------------------- 95.7/400.7 MB 1.3 MB/s eta 0:03:47\n",
      "     --------- ----------------------------- 95.9/400.7 MB 1.4 MB/s eta 0:03:45\n",
      "     --------- ----------------------------- 96.2/400.7 MB 1.4 MB/s eta 0:03:45\n",
      "     --------- ----------------------------- 96.7/400.7 MB 1.4 MB/s eta 0:03:43\n",
      "     --------- ----------------------------- 97.0/400.7 MB 1.4 MB/s eta 0:03:42\n",
      "     --------- ----------------------------- 97.3/400.7 MB 1.4 MB/s eta 0:03:41\n",
      "     --------- ----------------------------- 97.8/400.7 MB 1.4 MB/s eta 0:03:38\n",
      "     --------- ----------------------------- 98.3/400.7 MB 1.4 MB/s eta 0:03:36\n",
      "     --------- ----------------------------- 98.6/400.7 MB 1.4 MB/s eta 0:03:35\n",
      "     --------- ----------------------------- 99.6/400.7 MB 1.4 MB/s eta 0:03:30\n",
      "     --------- ----------------------------- 99.9/400.7 MB 1.4 MB/s eta 0:03:29\n",
      "     --------- ---------------------------- 100.4/400.7 MB 1.5 MB/s eta 0:03:27\n",
      "     --------- ---------------------------- 100.9/400.7 MB 1.5 MB/s eta 0:03:25\n",
      "     --------- ---------------------------- 101.4/400.7 MB 1.5 MB/s eta 0:03:23\n",
      "     --------- ---------------------------- 101.7/400.7 MB 1.5 MB/s eta 0:03:23\n",
      "     --------- ---------------------------- 102.0/400.7 MB 1.5 MB/s eta 0:03:21\n",
      "     --------- ---------------------------- 102.2/400.7 MB 1.5 MB/s eta 0:03:20\n",
      "     --------- ---------------------------- 102.8/400.7 MB 1.5 MB/s eta 0:03:19\n",
      "     --------- ---------------------------- 103.3/400.7 MB 1.5 MB/s eta 0:03:17\n",
      "     --------- ---------------------------- 103.8/400.7 MB 1.5 MB/s eta 0:03:15\n",
      "     --------- ---------------------------- 104.3/400.7 MB 1.5 MB/s eta 0:03:13\n",
      "     --------- ---------------------------- 105.1/400.7 MB 1.6 MB/s eta 0:03:11\n",
      "     --------- ---------------------------- 105.4/400.7 MB 1.6 MB/s eta 0:03:10\n",
      "     ---------- --------------------------- 105.9/400.7 MB 1.6 MB/s eta 0:03:08\n",
      "     ---------- --------------------------- 106.2/400.7 MB 1.6 MB/s eta 0:03:08\n",
      "     ---------- --------------------------- 106.7/400.7 MB 1.6 MB/s eta 0:03:07\n",
      "     ---------- --------------------------- 107.0/400.7 MB 1.6 MB/s eta 0:03:06\n",
      "     ---------- --------------------------- 107.5/400.7 MB 1.6 MB/s eta 0:03:04\n",
      "     ---------- --------------------------- 107.7/400.7 MB 1.6 MB/s eta 0:03:04\n",
      "     ---------- --------------------------- 108.0/400.7 MB 1.6 MB/s eta 0:03:04\n",
      "     ---------- --------------------------- 108.3/400.7 MB 1.6 MB/s eta 0:03:01\n",
      "     ---------- --------------------------- 108.5/400.7 MB 1.6 MB/s eta 0:03:01\n",
      "     ---------- --------------------------- 109.1/400.7 MB 1.6 MB/s eta 0:03:00\n",
      "     ---------- --------------------------- 109.3/400.7 MB 1.6 MB/s eta 0:02:58\n",
      "     ---------- --------------------------- 109.8/400.7 MB 1.6 MB/s eta 0:02:57\n",
      "     ---------- --------------------------- 110.4/400.7 MB 1.7 MB/s eta 0:02:56\n",
      "     ---------- --------------------------- 110.9/400.7 MB 1.7 MB/s eta 0:02:55\n",
      "     ---------- --------------------------- 111.4/400.7 MB 1.7 MB/s eta 0:02:53\n",
      "     ---------- --------------------------- 111.9/400.7 MB 1.7 MB/s eta 0:02:52\n",
      "     ---------- --------------------------- 112.7/400.7 MB 1.7 MB/s eta 0:02:50\n",
      "     ---------- --------------------------- 113.0/400.7 MB 1.7 MB/s eta 0:02:49\n",
      "     ---------- --------------------------- 113.2/400.7 MB 1.7 MB/s eta 0:02:49\n",
      "     ---------- --------------------------- 113.8/400.7 MB 1.7 MB/s eta 0:02:48\n",
      "     ---------- --------------------------- 114.0/400.7 MB 1.7 MB/s eta 0:02:48\n",
      "     ---------- --------------------------- 114.3/400.7 MB 1.7 MB/s eta 0:02:47\n",
      "     ---------- --------------------------- 114.8/400.7 MB 1.7 MB/s eta 0:02:46\n",
      "     ---------- --------------------------- 114.8/400.7 MB 1.7 MB/s eta 0:02:46\n",
      "     ---------- --------------------------- 115.3/400.7 MB 1.7 MB/s eta 0:02:45\n",
      "     ---------- --------------------------- 115.6/400.7 MB 1.7 MB/s eta 0:02:45\n",
      "     ----------- -------------------------- 116.1/400.7 MB 1.7 MB/s eta 0:02:44\n",
      "     ----------- -------------------------- 116.4/400.7 MB 1.7 MB/s eta 0:02:44\n",
      "     ----------- -------------------------- 116.9/400.7 MB 1.7 MB/s eta 0:02:43\n",
      "     ----------- -------------------------- 117.4/400.7 MB 1.8 MB/s eta 0:02:42\n",
      "     ----------- -------------------------- 118.0/400.7 MB 1.8 MB/s eta 0:02:40\n",
      "     ----------- -------------------------- 118.5/400.7 MB 1.8 MB/s eta 0:02:38\n",
      "     ----------- -------------------------- 118.8/400.7 MB 1.8 MB/s eta 0:02:38\n",
      "     ----------- -------------------------- 119.3/400.7 MB 1.8 MB/s eta 0:02:37\n",
      "     ----------- -------------------------- 119.5/400.7 MB 1.8 MB/s eta 0:02:37\n",
      "     ----------- -------------------------- 120.1/400.7 MB 1.8 MB/s eta 0:02:35\n",
      "     ----------- -------------------------- 120.3/400.7 MB 1.8 MB/s eta 0:02:35\n",
      "     ----------- -------------------------- 120.8/400.7 MB 1.8 MB/s eta 0:02:35\n",
      "     ----------- -------------------------- 121.4/400.7 MB 1.8 MB/s eta 0:02:34\n",
      "     ----------- -------------------------- 121.4/400.7 MB 1.8 MB/s eta 0:02:34\n",
      "     ----------- -------------------------- 121.6/400.7 MB 1.8 MB/s eta 0:02:34\n",
      "     ----------- -------------------------- 121.9/400.7 MB 1.8 MB/s eta 0:02:33\n",
      "     ----------- -------------------------- 122.7/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 122.9/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 123.2/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 123.5/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 123.7/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 124.3/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 124.5/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 125.0/400.7 MB 1.8 MB/s eta 0:02:31\n",
      "     ----------- -------------------------- 125.3/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 125.6/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 126.1/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ----------- -------------------------- 126.4/400.7 MB 1.8 MB/s eta 0:02:31\n",
      "     ------------ ------------------------- 126.9/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ------------ ------------------------- 126.9/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ------------ ------------------------- 127.1/400.7 MB 1.8 MB/s eta 0:02:33\n",
      "     ------------ ------------------------- 127.9/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ------------ ------------------------- 128.5/400.7 MB 1.8 MB/s eta 0:02:32\n",
      "     ------------ ------------------------- 128.7/400.7 MB 1.8 MB/s eta 0:02:31\n",
      "     ------------ ------------------------- 129.2/400.7 MB 1.8 MB/s eta 0:02:31\n",
      "     ------------ ------------------------- 129.8/400.7 MB 1.8 MB/s eta 0:02:30\n",
      "     ------------ ------------------------- 130.3/400.7 MB 1.8 MB/s eta 0:02:29\n",
      "     ------------ ------------------------- 130.5/400.7 MB 1.8 MB/s eta 0:02:29\n",
      "     ------------ ------------------------- 130.8/400.7 MB 1.8 MB/s eta 0:02:29\n",
      "     ------------ ------------------------- 131.3/400.7 MB 1.8 MB/s eta 0:02:28\n",
      "     ------------ ------------------------- 131.6/400.7 MB 1.8 MB/s eta 0:02:27\n",
      "     ------------ ------------------------- 131.9/400.7 MB 1.8 MB/s eta 0:02:27\n",
      "     ------------ ------------------------- 132.1/400.7 MB 1.8 MB/s eta 0:02:27\n",
      "     ------------ ------------------------- 132.6/400.7 MB 1.8 MB/s eta 0:02:27\n",
      "     ------------ ------------------------- 133.2/400.7 MB 1.9 MB/s eta 0:02:25\n",
      "     ------------ ------------------------- 133.7/400.7 MB 1.9 MB/s eta 0:02:24\n",
      "     ------------ ------------------------- 134.0/400.7 MB 1.9 MB/s eta 0:02:24\n",
      "     ------------ ------------------------- 134.2/400.7 MB 1.9 MB/s eta 0:02:24\n",
      "     ------------ ------------------------- 134.7/400.7 MB 1.9 MB/s eta 0:02:23\n",
      "     ------------ ------------------------- 135.0/400.7 MB 1.9 MB/s eta 0:02:23\n",
      "     ------------ ------------------------- 135.5/400.7 MB 1.9 MB/s eta 0:02:22\n",
      "     ------------ ------------------------- 136.1/400.7 MB 1.9 MB/s eta 0:02:20\n",
      "     ------------ ------------------------- 136.3/400.7 MB 1.9 MB/s eta 0:02:21\n",
      "     ------------ ------------------------- 136.6/400.7 MB 1.9 MB/s eta 0:02:20\n",
      "     ------------- ------------------------ 137.1/400.7 MB 1.9 MB/s eta 0:02:20\n",
      "     ------------- ------------------------ 137.4/400.7 MB 1.9 MB/s eta 0:02:19\n",
      "     ------------- ------------------------ 138.1/400.7 MB 1.9 MB/s eta 0:02:18\n",
      "     ------------- ------------------------ 138.4/400.7 MB 1.9 MB/s eta 0:02:18\n",
      "     ------------- ------------------------ 138.9/400.7 MB 1.9 MB/s eta 0:02:17\n",
      "     ------------- ------------------------ 139.5/400.7 MB 1.9 MB/s eta 0:02:16\n",
      "     ------------- ------------------------ 140.0/400.7 MB 1.9 MB/s eta 0:02:15\n",
      "     ------------- ------------------------ 140.5/400.7 MB 1.9 MB/s eta 0:02:14\n",
      "     ------------- ------------------------ 141.0/400.7 MB 1.9 MB/s eta 0:02:14\n",
      "     ------------- ------------------------ 141.6/400.7 MB 1.9 MB/s eta 0:02:14\n",
      "     ------------- ------------------------ 141.8/400.7 MB 1.9 MB/s eta 0:02:14\n",
      "     ------------- ------------------------ 142.3/400.7 MB 2.0 MB/s eta 0:02:13\n",
      "     ------------- ------------------------ 142.9/400.7 MB 2.0 MB/s eta 0:02:12\n",
      "     ------------- ------------------------ 142.9/400.7 MB 2.0 MB/s eta 0:02:12\n",
      "     ------------- ------------------------ 143.9/400.7 MB 2.0 MB/s eta 0:02:10\n",
      "     ------------- ------------------------ 144.4/400.7 MB 2.0 MB/s eta 0:02:10\n",
      "     ------------- ------------------------ 144.7/400.7 MB 2.0 MB/s eta 0:02:10\n",
      "     ------------- ------------------------ 145.2/400.7 MB 2.0 MB/s eta 0:02:09\n",
      "     ------------- ------------------------ 145.8/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     ------------- ------------------------ 146.3/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     ------------- ------------------------ 146.5/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     ------------- ------------------------ 146.8/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     ------------- ------------------------ 147.3/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     ------------- ------------------------ 147.6/400.7 MB 2.0 MB/s eta 0:02:08\n",
      "     -------------- ----------------------- 148.1/400.7 MB 2.0 MB/s eta 0:02:07\n",
      "     -------------- ----------------------- 148.1/400.7 MB 2.0 MB/s eta 0:02:07\n",
      "     -------------- ----------------------- 149.2/400.7 MB 2.0 MB/s eta 0:02:07\n",
      "     -------------- ----------------------- 149.2/400.7 MB 2.0 MB/s eta 0:02:07\n",
      "     -------------- ----------------------- 149.4/400.7 MB 2.0 MB/s eta 0:02:07\n",
      "     -------------- ----------------------- 149.9/400.7 MB 2.0 MB/s eta 0:02:06\n",
      "     -------------- ----------------------- 149.9/400.7 MB 2.0 MB/s eta 0:02:06\n",
      "     -------------- ----------------------- 150.7/400.7 MB 2.0 MB/s eta 0:02:05\n",
      "     -------------- ----------------------- 151.3/400.7 MB 2.0 MB/s eta 0:02:05\n",
      "     -------------- ----------------------- 151.8/400.7 MB 2.0 MB/s eta 0:02:04\n",
      "     -------------- ----------------------- 152.0/400.7 MB 2.0 MB/s eta 0:02:05\n",
      "     -------------- ----------------------- 152.3/400.7 MB 2.0 MB/s eta 0:02:05\n",
      "     -------------- ----------------------- 152.8/400.7 MB 2.0 MB/s eta 0:02:04\n",
      "     -------------- ----------------------- 153.4/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 153.6/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 154.1/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 154.7/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     -------------- ----------------------- 154.9/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 155.5/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 155.7/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 156.0/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 156.0/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     -------------- ----------------------- 157.0/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     -------------- ----------------------- 157.3/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     -------------- ----------------------- 157.8/400.7 MB 2.0 MB/s eta 0:02:01\n",
      "     -------------- ----------------------- 158.1/400.7 MB 2.0 MB/s eta 0:02:01\n",
      "     --------------- ---------------------- 158.3/400.7 MB 2.0 MB/s eta 0:02:01\n",
      "     --------------- ---------------------- 158.6/400.7 MB 2.0 MB/s eta 0:02:01\n",
      "     --------------- ---------------------- 158.9/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 159.1/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 159.9/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 160.2/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 160.4/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 160.4/400.7 MB 2.0 MB/s eta 0:02:02\n",
      "     --------------- ---------------------- 161.0/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     --------------- ---------------------- 161.0/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     --------------- ---------------------- 161.2/400.7 MB 2.0 MB/s eta 0:02:03\n",
      "     --------------- ---------------------- 161.5/400.7 MB 1.9 MB/s eta 0:02:04\n",
      "     --------------- ---------------------- 161.5/400.7 MB 1.9 MB/s eta 0:02:04\n",
      "     --------------- ---------------------- 161.7/400.7 MB 1.9 MB/s eta 0:02:05\n",
      "     --------------- ---------------------- 162.0/400.7 MB 1.9 MB/s eta 0:02:06\n",
      "     --------------- ---------------------- 162.3/400.7 MB 1.9 MB/s eta 0:02:07\n",
      "     --------------- ---------------------- 162.3/400.7 MB 1.9 MB/s eta 0:02:07\n",
      "     --------------- ---------------------- 162.5/400.7 MB 1.9 MB/s eta 0:02:07\n",
      "     --------------- ---------------------- 162.8/400.7 MB 1.9 MB/s eta 0:02:08\n",
      "     --------------- ---------------------- 162.8/400.7 MB 1.9 MB/s eta 0:02:08\n",
      "     --------------- ---------------------- 163.1/400.7 MB 1.9 MB/s eta 0:02:08\n",
      "     --------------- ---------------------- 163.3/400.7 MB 1.9 MB/s eta 0:02:09\n",
      "     --------------- ---------------------- 163.3/400.7 MB 1.9 MB/s eta 0:02:09\n",
      "     --------------- ---------------------- 163.6/400.7 MB 1.8 MB/s eta 0:02:09\n",
      "     --------------- ---------------------- 163.6/400.7 MB 1.8 MB/s eta 0:02:09\n",
      "     --------------- ---------------------- 163.8/400.7 MB 1.8 MB/s eta 0:02:10\n",
      "     --------------- ---------------------- 163.8/400.7 MB 1.8 MB/s eta 0:02:10\n",
      "     --------------- ---------------------- 164.1/400.7 MB 1.8 MB/s eta 0:02:12\n",
      "     --------------- ---------------------- 164.1/400.7 MB 1.8 MB/s eta 0:02:12\n",
      "     --------------- ---------------------- 164.4/400.7 MB 1.8 MB/s eta 0:02:13\n",
      "     --------------- ---------------------- 164.4/400.7 MB 1.8 MB/s eta 0:02:13\n",
      "     --------------- ---------------------- 164.6/400.7 MB 1.8 MB/s eta 0:02:15\n",
      "     --------------- ---------------------- 164.6/400.7 MB 1.8 MB/s eta 0:02:15\n",
      "     --------------- ---------------------- 164.9/400.7 MB 1.7 MB/s eta 0:02:16\n",
      "     --------------- ---------------------- 164.9/400.7 MB 1.7 MB/s eta 0:02:16\n",
      "     --------------- ---------------------- 165.2/400.7 MB 1.7 MB/s eta 0:02:18\n",
      "     --------------- ---------------------- 165.4/400.7 MB 1.7 MB/s eta 0:02:18\n",
      "     --------------- ---------------------- 165.7/400.7 MB 1.7 MB/s eta 0:02:18\n",
      "     --------------- ---------------------- 165.9/400.7 MB 1.7 MB/s eta 0:02:18\n",
      "     --------------- ---------------------- 165.9/400.7 MB 1.7 MB/s eta 0:02:18\n",
      "     --------------- ---------------------- 166.2/400.7 MB 1.7 MB/s eta 0:02:19\n",
      "     --------------- ---------------------- 166.2/400.7 MB 1.7 MB/s eta 0:02:19\n",
      "     --------------- ---------------------- 166.7/400.7 MB 1.7 MB/s eta 0:02:19\n",
      "     --------------- ---------------------- 167.0/400.7 MB 1.7 MB/s eta 0:02:19\n",
      "     --------------- ---------------------- 167.2/400.7 MB 1.7 MB/s eta 0:02:19\n",
      "     --------------- ---------------------- 167.5/400.7 MB 1.7 MB/s eta 0:02:20\n",
      "     --------------- ---------------------- 167.8/400.7 MB 1.7 MB/s eta 0:02:20\n",
      "     --------------- ---------------------- 168.3/400.7 MB 1.7 MB/s eta 0:02:20\n",
      "     --------------- ---------------------- 168.6/400.7 MB 1.7 MB/s eta 0:02:20\n",
      "     ---------------- --------------------- 168.8/400.7 MB 1.7 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 169.1/400.7 MB 1.7 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 169.3/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 169.6/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 170.1/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 170.1/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 170.4/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 170.7/400.7 MB 1.6 MB/s eta 0:02:22\n",
      "     ---------------- --------------------- 170.9/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 171.2/400.7 MB 1.6 MB/s eta 0:02:21\n",
      "     ---------------- --------------------- 171.4/400.7 MB 1.6 MB/s eta 0:02:23\n",
      "     ---------------- --------------------- 171.7/400.7 MB 1.6 MB/s eta 0:02:23\n",
      "     ---------------- --------------------- 172.0/400.7 MB 1.6 MB/s eta 0:02:22\n",
      "     ---------------- --------------------- 172.0/400.7 MB 1.6 MB/s eta 0:02:22\n",
      "     ---------------- --------------------- 172.2/400.7 MB 1.6 MB/s eta 0:02:23\n",
      "     ---------------- --------------------- 172.5/400.7 MB 1.6 MB/s eta 0:02:23\n",
      "     ---------------- --------------------- 172.8/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 172.8/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 173.0/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 173.3/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 173.5/400.7 MB 1.6 MB/s eta 0:02:25\n",
      "     ---------------- --------------------- 173.8/400.7 MB 1.6 MB/s eta 0:02:25\n",
      "     ---------------- --------------------- 174.1/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 174.1/400.7 MB 1.6 MB/s eta 0:02:24\n",
      "     ---------------- --------------------- 174.6/400.7 MB 1.6 MB/s eta 0:02:26\n",
      "     ---------------- --------------------- 175.1/400.7 MB 1.6 MB/s eta 0:02:25\n",
      "     ---------------- --------------------- 175.1/400.7 MB 1.6 MB/s eta 0:02:25\n",
      "     ---------------- --------------------- 175.4/400.7 MB 1.5 MB/s eta 0:02:27\n",
      "     ---------------- --------------------- 175.6/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 175.9/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.2/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.4/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.7/400.7 MB 1.5 MB/s eta 0:02:29\n",
      "     ---------------- --------------------- 176.9/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.9/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.9/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 176.9/400.7 MB 1.5 MB/s eta 0:02:28\n",
      "     ---------------- --------------------- 178.0/400.7 MB 1.5 MB/s eta 0:02:30\n",
      "     ---------------- --------------------- 178.0/400.7 MB 1.5 MB/s eta 0:02:30\n",
      "     ---------------- --------------------- 178.3/400.7 MB 1.5 MB/s eta 0:02:31\n",
      "     ---------------- --------------------- 178.3/400.7 MB 1.5 MB/s eta 0:02:31\n",
      "     ---------------- --------------------- 178.3/400.7 MB 1.5 MB/s eta 0:02:31\n",
      "     ---------------- --------------------- 178.5/400.7 MB 1.5 MB/s eta 0:02:33\n",
      "     ---------------- --------------------- 178.8/400.7 MB 1.4 MB/s eta 0:02:34\n",
      "     ---------------- --------------------- 178.8/400.7 MB 1.4 MB/s eta 0:02:34\n",
      "     ---------------- --------------------- 179.0/400.7 MB 1.4 MB/s eta 0:02:35\n",
      "     ----------------- -------------------- 179.3/400.7 MB 1.4 MB/s eta 0:02:36\n",
      "     ----------------- -------------------- 179.6/400.7 MB 1.4 MB/s eta 0:02:36\n",
      "     ----------------- -------------------- 180.1/400.7 MB 1.4 MB/s eta 0:02:36\n",
      "     ----------------- -------------------- 180.1/400.7 MB 1.4 MB/s eta 0:02:36\n",
      "     ----------------- -------------------- 180.1/400.7 MB 1.4 MB/s eta 0:02:36\n",
      "     ----------------- -------------------- 180.4/400.7 MB 1.4 MB/s eta 0:02:39\n",
      "     ----------------- -------------------- 180.6/400.7 MB 1.4 MB/s eta 0:02:41\n",
      "     ----------------- -------------------- 180.9/400.7 MB 1.4 MB/s eta 0:02:42\n",
      "     ----------------- -------------------- 180.9/400.7 MB 1.4 MB/s eta 0:02:42\n",
      "     ----------------- -------------------- 180.9/400.7 MB 1.4 MB/s eta 0:02:42\n",
      "     ----------------- -------------------- 181.1/400.7 MB 1.3 MB/s eta 0:02:46\n",
      "     ----------------- -------------------- 181.1/400.7 MB 1.3 MB/s eta 0:02:46\n",
      "     ----------------- -------------------- 181.9/400.7 MB 1.3 MB/s eta 0:02:46\n",
      "     ----------------- -------------------- 182.2/400.7 MB 1.3 MB/s eta 0:02:46\n",
      "     ----------------- -------------------- 182.5/400.7 MB 1.3 MB/s eta 0:02:45\n",
      "     ----------------- -------------------- 182.5/400.7 MB 1.3 MB/s eta 0:02:45\n",
      "     ----------------- -------------------- 182.7/400.7 MB 1.3 MB/s eta 0:02:50\n",
      "     ----------------- -------------------- 183.0/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 183.5/400.7 MB 1.3 MB/s eta 0:02:50\n",
      "     ----------------- -------------------- 183.8/400.7 MB 1.3 MB/s eta 0:02:50\n",
      "     ----------------- -------------------- 184.0/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 184.3/400.7 MB 1.3 MB/s eta 0:02:52\n",
      "     ----------------- -------------------- 184.5/400.7 MB 1.3 MB/s eta 0:02:52\n",
      "     ----------------- -------------------- 184.8/400.7 MB 1.3 MB/s eta 0:02:52\n",
      "     ----------------- -------------------- 185.1/400.7 MB 1.3 MB/s eta 0:02:52\n",
      "     ----------------- -------------------- 185.3/400.7 MB 1.3 MB/s eta 0:02:53\n",
      "     ----------------- -------------------- 185.9/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 186.1/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 186.1/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 186.1/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 186.1/400.7 MB 1.3 MB/s eta 0:02:51\n",
      "     ----------------- -------------------- 186.9/400.7 MB 1.2 MB/s eta 0:02:55\n",
      "     ----------------- -------------------- 187.2/400.7 MB 1.2 MB/s eta 0:02:56\n",
      "     ----------------- -------------------- 187.4/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ----------------- -------------------- 187.7/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ----------------- -------------------- 188.0/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ----------------- -------------------- 188.5/400.7 MB 1.2 MB/s eta 0:02:56\n",
      "     ----------------- -------------------- 189.0/400.7 MB 1.2 MB/s eta 0:02:56\n",
      "     ----------------- -------------------- 189.3/400.7 MB 1.2 MB/s eta 0:02:55\n",
      "     ------------------ ------------------- 189.8/400.7 MB 1.2 MB/s eta 0:02:55\n",
      "     ------------------ ------------------- 190.1/400.7 MB 1.2 MB/s eta 0:02:55\n",
      "     ------------------ ------------------- 190.3/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ------------------ ------------------- 190.6/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ------------------ ------------------- 190.8/400.7 MB 1.2 MB/s eta 0:02:58\n",
      "     ------------------ ------------------- 191.1/400.7 MB 1.2 MB/s eta 0:02:57\n",
      "     ------------------ ------------------- 191.6/400.7 MB 1.2 MB/s eta 0:02:56\n",
      "     ------------------ ------------------- 191.9/400.7 MB 1.2 MB/s eta 0:02:54\n",
      "     ------------------ ------------------- 192.2/400.7 MB 1.2 MB/s eta 0:02:54\n",
      "     ------------------ ------------------- 193.2/400.7 MB 1.2 MB/s eta 0:02:54\n",
      "     ------------------ ------------------- 193.7/400.7 MB 1.2 MB/s eta 0:02:53\n",
      "     ------------------ ------------------- 194.0/400.7 MB 1.2 MB/s eta 0:02:53\n",
      "     ------------------ ------------------- 194.2/400.7 MB 1.2 MB/s eta 0:02:53\n",
      "     ------------------ ------------------- 194.5/400.7 MB 1.2 MB/s eta 0:02:52\n",
      "     ------------------ ------------------- 194.8/400.7 MB 1.2 MB/s eta 0:02:52\n",
      "     ------------------ ------------------- 195.0/400.7 MB 1.2 MB/s eta 0:02:53\n",
      "     ------------------ ------------------- 195.3/400.7 MB 1.2 MB/s eta 0:02:52\n",
      "     ------------------ ------------------- 195.6/400.7 MB 1.2 MB/s eta 0:02:54\n",
      "     ------------------ ------------------- 195.8/400.7 MB 1.2 MB/s eta 0:02:54\n",
      "     ------------------ ------------------- 196.1/400.7 MB 1.2 MB/s eta 0:02:53\n",
      "     ------------------ ------------------- 196.6/400.7 MB 1.2 MB/s eta 0:02:52\n",
      "     ------------------ ------------------- 196.9/400.7 MB 1.2 MB/s eta 0:02:51\n",
      "     ------------------ ------------------- 197.7/400.7 MB 1.2 MB/s eta 0:02:48\n",
      "     ------------------ ------------------- 197.9/400.7 MB 1.2 MB/s eta 0:02:47\n",
      "     ------------------ ------------------- 198.2/400.7 MB 1.2 MB/s eta 0:02:45\n",
      "     ------------------ ------------------- 198.4/400.7 MB 1.2 MB/s eta 0:02:45\n",
      "     ------------------ ------------------- 199.0/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 199.2/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 199.2/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 199.5/400.7 MB 1.2 MB/s eta 0:02:43\n",
      "     ------------------ ------------------- 199.5/400.7 MB 1.2 MB/s eta 0:02:43\n",
      "     ------------------ ------------------- 199.8/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 199.8/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 200.0/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 200.3/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------ ------------------- 200.3/400.7 MB 1.2 MB/s eta 0:02:44\n",
      "     ------------------- ------------------ 200.5/400.7 MB 1.2 MB/s eta 0:02:42\n",
      "     ------------------- ------------------ 200.8/400.7 MB 1.2 MB/s eta 0:02:43\n",
      "     ------------------- ------------------ 200.8/400.7 MB 1.2 MB/s eta 0:02:43\n",
      "     ------------------- ------------------ 200.8/400.7 MB 1.2 MB/s eta 0:02:43\n",
      "     ------------------- ------------------ 201.3/400.7 MB 1.2 MB/s eta 0:02:41\n",
      "     ------------------- ------------------ 201.3/400.7 MB 1.2 MB/s eta 0:02:41\n",
      "     ------------------- ------------------ 201.6/400.7 MB 1.2 MB/s eta 0:02:41\n",
      "     ------------------- ------------------ 201.9/400.7 MB 1.2 MB/s eta 0:02:40\n",
      "     ------------------- ------------------ 202.4/400.7 MB 1.3 MB/s eta 0:02:39\n",
      "     ------------------- ------------------ 202.9/400.7 MB 1.3 MB/s eta 0:02:35\n",
      "     ------------------- ------------------ 203.2/400.7 MB 1.3 MB/s eta 0:02:35\n",
      "     ------------------- ------------------ 203.4/400.7 MB 1.3 MB/s eta 0:02:34\n",
      "     ------------------- ------------------ 203.7/400.7 MB 1.3 MB/s eta 0:02:34\n",
      "     ------------------- ------------------ 203.9/400.7 MB 1.3 MB/s eta 0:02:34\n",
      "     ------------------- ------------------ 204.2/400.7 MB 1.3 MB/s eta 0:02:34\n",
      "     ------------------- ------------------ 204.7/400.7 MB 1.3 MB/s eta 0:02:32\n",
      "     ------------------- ------------------ 205.0/400.7 MB 1.3 MB/s eta 0:02:32\n",
      "     ------------------- ------------------ 205.3/400.7 MB 1.3 MB/s eta 0:02:30\n",
      "     ------------------- ------------------ 205.3/400.7 MB 1.3 MB/s eta 0:02:30\n",
      "     ------------------- ------------------ 205.3/400.7 MB 1.3 MB/s eta 0:02:30\n",
      "     ------------------- ------------------ 206.6/400.7 MB 1.3 MB/s eta 0:02:29\n",
      "     ------------------- ------------------ 207.1/400.7 MB 1.3 MB/s eta 0:02:28\n",
      "     ------------------- ------------------ 207.4/400.7 MB 1.3 MB/s eta 0:02:27\n",
      "     ------------------- ------------------ 207.9/400.7 MB 1.3 MB/s eta 0:02:27\n",
      "     ------------------- ------------------ 208.1/400.7 MB 1.3 MB/s eta 0:02:27\n",
      "     ------------------- ------------------ 208.7/400.7 MB 1.3 MB/s eta 0:02:25\n",
      "     ------------------- ------------------ 208.9/400.7 MB 1.3 MB/s eta 0:02:25\n",
      "     ------------------- ------------------ 209.5/400.7 MB 1.3 MB/s eta 0:02:23\n",
      "     ------------------- ------------------ 209.5/400.7 MB 1.3 MB/s eta 0:02:23\n",
      "     ------------------- ------------------ 210.2/400.7 MB 1.3 MB/s eta 0:02:23\n",
      "     ------------------- ------------------ 210.5/400.7 MB 1.3 MB/s eta 0:02:22\n",
      "     -------------------- ----------------- 211.0/400.7 MB 1.4 MB/s eta 0:02:21\n",
      "     -------------------- ----------------- 211.6/400.7 MB 1.4 MB/s eta 0:02:20\n",
      "     -------------------- ----------------- 211.8/400.7 MB 1.4 MB/s eta 0:02:19\n",
      "     -------------------- ----------------- 212.3/400.7 MB 1.4 MB/s eta 0:02:17\n",
      "     -------------------- ----------------- 212.9/400.7 MB 1.4 MB/s eta 0:02:16\n",
      "     -------------------- ----------------- 213.1/400.7 MB 1.4 MB/s eta 0:02:15\n",
      "     -------------------- ----------------- 213.6/400.7 MB 1.4 MB/s eta 0:02:15\n",
      "     -------------------- ----------------- 214.2/400.7 MB 1.4 MB/s eta 0:02:13\n",
      "     -------------------- ----------------- 214.7/400.7 MB 1.4 MB/s eta 0:02:12\n",
      "     -------------------- ----------------- 215.5/400.7 MB 1.4 MB/s eta 0:02:10\n",
      "     -------------------- ----------------- 216.0/400.7 MB 1.4 MB/s eta 0:02:08\n",
      "     -------------------- ----------------- 216.0/400.7 MB 1.4 MB/s eta 0:02:08\n",
      "     -------------------- ----------------- 216.5/400.7 MB 1.5 MB/s eta 0:02:07\n",
      "     -------------------- ----------------- 217.1/400.7 MB 1.5 MB/s eta 0:02:06\n",
      "     -------------------- ----------------- 217.8/400.7 MB 1.5 MB/s eta 0:02:04\n",
      "     -------------------- ----------------- 218.6/400.7 MB 1.5 MB/s eta 0:02:03\n",
      "     -------------------- ----------------- 219.2/400.7 MB 1.5 MB/s eta 0:02:01\n",
      "     -------------------- ----------------- 219.7/400.7 MB 1.5 MB/s eta 0:02:00\n",
      "     -------------------- ----------------- 220.2/400.7 MB 1.5 MB/s eta 0:01:59\n",
      "     -------------------- ----------------- 220.7/400.7 MB 1.5 MB/s eta 0:01:59\n",
      "     --------------------- ---------------- 221.5/400.7 MB 1.5 MB/s eta 0:01:56\n",
      "     --------------------- ---------------- 221.8/400.7 MB 1.5 MB/s eta 0:01:56\n",
      "     --------------------- ---------------- 222.0/400.7 MB 1.6 MB/s eta 0:01:55\n",
      "     --------------------- ---------------- 222.3/400.7 MB 1.6 MB/s eta 0:01:55\n",
      "     --------------------- ---------------- 222.8/400.7 MB 1.6 MB/s eta 0:01:55\n",
      "     --------------------- ---------------- 223.3/400.7 MB 1.6 MB/s eta 0:01:54\n",
      "     --------------------- ---------------- 223.9/400.7 MB 1.6 MB/s eta 0:01:53\n",
      "     --------------------- ---------------- 224.4/400.7 MB 1.6 MB/s eta 0:01:52\n",
      "     --------------------- ---------------- 224.7/400.7 MB 1.6 MB/s eta 0:01:49\n",
      "     --------------------- ---------------- 225.2/400.7 MB 1.6 MB/s eta 0:01:48\n",
      "     --------------------- ---------------- 225.4/400.7 MB 1.6 MB/s eta 0:01:48\n",
      "     --------------------- ---------------- 226.0/400.7 MB 1.6 MB/s eta 0:01:47\n",
      "     --------------------- ---------------- 226.8/400.7 MB 1.6 MB/s eta 0:01:47\n",
      "     --------------------- ---------------- 227.0/400.7 MB 1.6 MB/s eta 0:01:47\n",
      "     --------------------- ---------------- 227.8/400.7 MB 1.7 MB/s eta 0:01:45\n",
      "     --------------------- ---------------- 228.1/400.7 MB 1.7 MB/s eta 0:01:44\n",
      "     --------------------- ---------------- 228.6/400.7 MB 1.7 MB/s eta 0:01:44\n",
      "     --------------------- ---------------- 229.4/400.7 MB 1.7 MB/s eta 0:01:42\n",
      "     --------------------- ---------------- 229.6/400.7 MB 1.7 MB/s eta 0:01:41\n",
      "     --------------------- ---------------- 230.2/400.7 MB 1.7 MB/s eta 0:01:40\n",
      "     --------------------- ---------------- 230.9/400.7 MB 1.7 MB/s eta 0:01:39\n",
      "     --------------------- ---------------- 231.5/400.7 MB 1.7 MB/s eta 0:01:38\n",
      "     ---------------------- --------------- 232.0/400.7 MB 1.7 MB/s eta 0:01:38\n",
      "     ---------------------- --------------- 232.5/400.7 MB 1.8 MB/s eta 0:01:36\n",
      "     ---------------------- --------------- 233.0/400.7 MB 1.8 MB/s eta 0:01:35\n",
      "     ---------------------- --------------- 233.6/400.7 MB 1.8 MB/s eta 0:01:34\n",
      "     ---------------------- --------------- 233.8/400.7 MB 1.8 MB/s eta 0:01:34\n",
      "     ---------------------- --------------- 233.8/400.7 MB 1.8 MB/s eta 0:01:34\n",
      "     ---------------------- --------------- 234.1/400.7 MB 1.8 MB/s eta 0:01:33\n",
      "     ---------------------- --------------- 234.1/400.7 MB 1.8 MB/s eta 0:01:33\n",
      "     ---------------------- --------------- 234.6/400.7 MB 1.8 MB/s eta 0:01:33\n",
      "     ---------------------- --------------- 235.1/400.7 MB 1.8 MB/s eta 0:01:33\n",
      "     ---------------------- --------------- 235.7/400.7 MB 1.8 MB/s eta 0:01:32\n",
      "     ---------------------- --------------- 235.9/400.7 MB 1.8 MB/s eta 0:01:32\n",
      "     ---------------------- --------------- 236.5/400.7 MB 1.8 MB/s eta 0:01:31\n",
      "     ---------------------- --------------- 237.2/400.7 MB 1.8 MB/s eta 0:01:30\n",
      "     ---------------------- --------------- 237.8/400.7 MB 1.8 MB/s eta 0:01:29\n",
      "     ---------------------- --------------- 238.6/400.7 MB 1.9 MB/s eta 0:01:28\n",
      "     ---------------------- --------------- 238.8/400.7 MB 1.9 MB/s eta 0:01:28\n",
      "     ---------------------- --------------- 239.6/400.7 MB 1.9 MB/s eta 0:01:27\n",
      "     ---------------------- --------------- 239.9/400.7 MB 1.9 MB/s eta 0:01:27\n",
      "     ---------------------- --------------- 240.1/400.7 MB 1.9 MB/s eta 0:01:26\n",
      "     ---------------------- --------------- 240.9/400.7 MB 1.9 MB/s eta 0:01:26\n",
      "     ---------------------- --------------- 241.4/400.7 MB 1.9 MB/s eta 0:01:25\n",
      "     ---------------------- --------------- 242.0/400.7 MB 1.9 MB/s eta 0:01:24\n",
      "     ----------------------- -------------- 242.7/400.7 MB 1.9 MB/s eta 0:01:23\n",
      "     ----------------------- -------------- 243.5/400.7 MB 1.9 MB/s eta 0:01:22\n",
      "     ----------------------- -------------- 244.3/400.7 MB 1.9 MB/s eta 0:01:21\n",
      "     ----------------------- -------------- 245.1/400.7 MB 2.0 MB/s eta 0:01:18\n",
      "     ----------------------- -------------- 245.6/400.7 MB 2.0 MB/s eta 0:01:18\n",
      "     ----------------------- -------------- 246.2/400.7 MB 2.0 MB/s eta 0:01:17\n",
      "     ----------------------- -------------- 246.7/400.7 MB 2.0 MB/s eta 0:01:17\n",
      "     ----------------------- -------------- 246.9/400.7 MB 2.0 MB/s eta 0:01:17\n",
      "     ----------------------- -------------- 247.5/400.7 MB 2.0 MB/s eta 0:01:17\n",
      "     ----------------------- -------------- 248.0/400.7 MB 2.0 MB/s eta 0:01:16\n",
      "     ----------------------- -------------- 248.3/400.7 MB 2.0 MB/s eta 0:01:16\n",
      "     ----------------------- -------------- 249.0/400.7 MB 2.0 MB/s eta 0:01:16\n",
      "     ----------------------- -------------- 249.6/400.7 MB 2.0 MB/s eta 0:01:15\n",
      "     ----------------------- -------------- 250.3/400.7 MB 2.0 MB/s eta 0:01:14\n",
      "     ----------------------- -------------- 250.6/400.7 MB 2.0 MB/s eta 0:01:14\n",
      "     ----------------------- -------------- 251.7/400.7 MB 2.1 MB/s eta 0:01:13\n",
      "     ----------------------- -------------- 251.9/400.7 MB 2.1 MB/s eta 0:01:13\n",
      "     ----------------------- -------------- 252.2/400.7 MB 2.1 MB/s eta 0:01:13\n",
      "     ----------------------- -------------- 253.0/400.7 MB 2.1 MB/s eta 0:01:12\n",
      "     ------------------------ ------------- 253.5/400.7 MB 2.1 MB/s eta 0:01:11\n",
      "     ------------------------ ------------- 254.0/400.7 MB 2.1 MB/s eta 0:01:11\n",
      "     ------------------------ ------------- 254.5/400.7 MB 2.1 MB/s eta 0:01:10\n",
      "     ------------------------ ------------- 255.1/400.7 MB 2.1 MB/s eta 0:01:10\n",
      "     ------------------------ ------------- 255.3/400.7 MB 2.1 MB/s eta 0:01:09\n",
      "     ------------------------ ------------- 255.9/400.7 MB 2.1 MB/s eta 0:01:10\n",
      "     ------------------------ ------------- 256.4/400.7 MB 2.1 MB/s eta 0:01:10\n",
      "     ------------------------ ------------- 256.9/400.7 MB 2.1 MB/s eta 0:01:09\n",
      "     ------------------------ ------------- 257.4/400.7 MB 2.1 MB/s eta 0:01:09\n",
      "     ------------------------ ------------- 258.2/400.7 MB 2.1 MB/s eta 0:01:08\n",
      "     ------------------------ ------------- 258.2/400.7 MB 2.1 MB/s eta 0:01:08\n",
      "     ------------------------ ------------- 259.0/400.7 MB 2.1 MB/s eta 0:01:07\n",
      "     ------------------------ ------------- 259.8/400.7 MB 2.1 MB/s eta 0:01:06\n",
      "     ------------------------ ------------- 260.3/400.7 MB 2.2 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 260.6/400.7 MB 2.2 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 260.8/400.7 MB 2.2 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 261.1/400.7 MB 2.2 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 261.6/400.7 MB 2.1 MB/s eta 0:01:06\n",
      "     ------------------------ ------------- 261.9/400.7 MB 2.1 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 262.4/400.7 MB 2.1 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 262.7/400.7 MB 2.1 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 262.9/400.7 MB 2.1 MB/s eta 0:01:05\n",
      "     ------------------------ ------------- 263.2/400.7 MB 2.1 MB/s eta 0:01:05\n",
      "     ------------------------- ------------ 263.7/400.7 MB 2.2 MB/s eta 0:01:04\n",
      "     ------------------------- ------------ 264.0/400.7 MB 2.2 MB/s eta 0:01:04\n",
      "     ------------------------- ------------ 264.5/400.7 MB 2.2 MB/s eta 0:01:03\n",
      "     ------------------------- ------------ 265.0/400.7 MB 2.2 MB/s eta 0:01:02\n",
      "     ------------------------- ------------ 265.6/400.7 MB 2.2 MB/s eta 0:01:02\n",
      "     ------------------------- ------------ 265.6/400.7 MB 2.2 MB/s eta 0:01:02\n",
      "     ------------------------- ------------ 265.8/400.7 MB 2.2 MB/s eta 0:01:02\n",
      "     ------------------------- ------------ 266.3/400.7 MB 2.2 MB/s eta 0:01:01\n",
      "     ------------------------- ------------ 266.9/400.7 MB 2.2 MB/s eta 0:01:01\n",
      "     ------------------------- ------------ 267.1/400.7 MB 2.2 MB/s eta 0:01:01\n",
      "     ------------------------- ------------ 267.6/400.7 MB 2.2 MB/s eta 0:01:00\n",
      "     ------------------------- ------------ 267.9/400.7 MB 2.2 MB/s eta 0:01:00\n",
      "     ------------------------- ------------ 268.4/400.7 MB 2.2 MB/s eta 0:00:59\n",
      "     ------------------------- ------------ 269.0/400.7 MB 2.3 MB/s eta 0:00:59\n",
      "     ------------------------- ------------ 269.2/400.7 MB 2.3 MB/s eta 0:00:59\n",
      "     ------------------------- ------------ 269.5/400.7 MB 2.3 MB/s eta 0:00:59\n",
      "     ------------------------- ------------ 270.0/400.7 MB 2.3 MB/s eta 0:00:58\n",
      "     ------------------------- ------------ 270.5/400.7 MB 2.3 MB/s eta 0:00:58\n",
      "     ------------------------- ------------ 270.8/400.7 MB 2.3 MB/s eta 0:00:58\n",
      "     ------------------------- ------------ 271.3/400.7 MB 2.3 MB/s eta 0:00:58\n",
      "     ------------------------- ------------ 271.8/400.7 MB 2.3 MB/s eta 0:00:57\n",
      "     ------------------------- ------------ 272.4/400.7 MB 2.3 MB/s eta 0:00:57\n",
      "     ------------------------- ------------ 272.6/400.7 MB 2.3 MB/s eta 0:00:57\n",
      "     ------------------------- ------------ 272.9/400.7 MB 2.3 MB/s eta 0:00:56\n",
      "     ------------------------- ------------ 273.4/400.7 MB 2.3 MB/s eta 0:00:56\n",
      "     ------------------------- ------------ 273.9/400.7 MB 2.3 MB/s eta 0:00:56\n",
      "     -------------------------- ----------- 274.2/400.7 MB 2.3 MB/s eta 0:00:55\n",
      "     -------------------------- ----------- 274.7/400.7 MB 2.3 MB/s eta 0:00:55\n",
      "     -------------------------- ----------- 275.3/400.7 MB 2.3 MB/s eta 0:00:55\n",
      "     -------------------------- ----------- 275.8/400.7 MB 2.3 MB/s eta 0:00:55\n",
      "     -------------------------- ----------- 276.0/400.7 MB 2.3 MB/s eta 0:00:55\n",
      "     -------------------------- ----------- 276.6/400.7 MB 2.3 MB/s eta 0:00:54\n",
      "     -------------------------- ----------- 277.1/400.7 MB 2.3 MB/s eta 0:00:54\n",
      "     -------------------------- ----------- 277.6/400.7 MB 2.3 MB/s eta 0:00:54\n",
      "     -------------------------- ----------- 278.1/400.7 MB 2.3 MB/s eta 0:00:54\n",
      "     -------------------------- ----------- 278.7/400.7 MB 2.3 MB/s eta 0:00:53\n",
      "     -------------------------- ----------- 279.4/400.7 MB 2.3 MB/s eta 0:00:52\n",
      "     -------------------------- ----------- 280.0/400.7 MB 2.3 MB/s eta 0:00:52\n",
      "     -------------------------- ----------- 280.5/400.7 MB 2.3 MB/s eta 0:00:52\n",
      "     -------------------------- ----------- 281.0/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 281.8/400.7 MB 2.4 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 282.1/400.7 MB 2.4 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 282.6/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 282.9/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 283.1/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 283.4/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 283.6/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 283.9/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 283.9/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 284.4/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     -------------------------- ----------- 284.4/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 285.5/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 285.7/400.7 MB 2.3 MB/s eta 0:00:50\n",
      "     --------------------------- ---------- 286.0/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 286.3/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 286.8/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.0/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.0/400.7 MB 2.3 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.6/400.7 MB 2.2 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.6/400.7 MB 2.2 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.6/400.7 MB 2.2 MB/s eta 0:00:51\n",
      "     --------------------------- ---------- 287.8/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.1/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.4/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.4/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.6/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.6/400.7 MB 2.2 MB/s eta 0:00:52\n",
      "     --------------------------- ---------- 288.9/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 288.9/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 289.1/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 289.1/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 289.4/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 289.4/400.7 MB 2.1 MB/s eta 0:00:53\n",
      "     --------------------------- ---------- 289.7/400.7 MB 2.1 MB/s eta 0:00:54\n",
      "     --------------------------- ---------- 289.7/400.7 MB 2.1 MB/s eta 0:00:54\n",
      "     --------------------------- ---------- 289.7/400.7 MB 2.1 MB/s eta 0:00:54\n",
      "     --------------------------- ---------- 289.9/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 289.9/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 290.2/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 290.2/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 290.2/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 290.2/400.7 MB 2.0 MB/s eta 0:00:55\n",
      "     --------------------------- ---------- 290.5/400.7 MB 1.9 MB/s eta 0:00:57\n",
      "     --------------------------- ---------- 290.5/400.7 MB 1.9 MB/s eta 0:00:57\n",
      "     --------------------------- ---------- 290.7/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 290.7/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 290.7/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 290.7/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 291.0/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 291.0/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 291.0/400.7 MB 1.9 MB/s eta 0:00:58\n",
      "     --------------------------- ---------- 291.2/400.7 MB 1.9 MB/s eta 0:00:59\n",
      "     --------------------------- ---------- 291.2/400.7 MB 1.9 MB/s eta 0:00:59\n",
      "     --------------------------- ---------- 291.2/400.7 MB 1.9 MB/s eta 0:00:59\n",
      "     --------------------------- ---------- 291.2/400.7 MB 1.9 MB/s eta 0:00:59\n",
      "     --------------------------- ---------- 291.5/400.7 MB 1.8 MB/s eta 0:01:01\n",
      "     --------------------------- ---------- 291.5/400.7 MB 1.8 MB/s eta 0:01:01\n",
      "     --------------------------- ---------- 291.8/400.7 MB 1.8 MB/s eta 0:01:02\n",
      "     --------------------------- ---------- 291.8/400.7 MB 1.8 MB/s eta 0:01:02\n",
      "     --------------------------- ---------- 292.0/400.7 MB 1.7 MB/s eta 0:01:03\n",
      "     --------------------------- ---------- 292.3/400.7 MB 1.7 MB/s eta 0:01:03\n",
      "     --------------------------- ---------- 292.3/400.7 MB 1.7 MB/s eta 0:01:03\n",
      "     --------------------------- ---------- 292.6/400.7 MB 1.7 MB/s eta 0:01:04\n",
      "     --------------------------- ---------- 292.6/400.7 MB 1.7 MB/s eta 0:01:04\n",
      "     --------------------------- ---------- 292.6/400.7 MB 1.7 MB/s eta 0:01:04\n",
      "     --------------------------- ---------- 292.8/400.7 MB 1.7 MB/s eta 0:01:05\n",
      "     --------------------------- ---------- 293.1/400.7 MB 1.7 MB/s eta 0:01:05\n",
      "     --------------------------- ---------- 293.3/400.7 MB 1.6 MB/s eta 0:01:07\n",
      "     --------------------------- ---------- 293.3/400.7 MB 1.6 MB/s eta 0:01:07\n",
      "     --------------------------- ---------- 293.6/400.7 MB 1.6 MB/s eta 0:01:08\n",
      "     --------------------------- ---------- 293.9/400.7 MB 1.6 MB/s eta 0:01:08\n",
      "     --------------------------- ---------- 293.9/400.7 MB 1.6 MB/s eta 0:01:08\n",
      "     --------------------------- ---------- 294.1/400.7 MB 1.6 MB/s eta 0:01:08\n",
      "     --------------------------- ---------- 294.4/400.7 MB 1.6 MB/s eta 0:01:09\n",
      "     --------------------------- ---------- 294.4/400.7 MB 1.6 MB/s eta 0:01:09\n",
      "     --------------------------- ---------- 294.6/400.7 MB 1.5 MB/s eta 0:01:09\n",
      "     --------------------------- ---------- 294.9/400.7 MB 1.5 MB/s eta 0:01:10\n",
      "     --------------------------- ---------- 295.2/400.7 MB 1.5 MB/s eta 0:01:11\n",
      "     ---------------------------- --------- 295.4/400.7 MB 1.5 MB/s eta 0:01:11\n",
      "     ---------------------------- --------- 295.7/400.7 MB 1.5 MB/s eta 0:01:11\n",
      "     ---------------------------- --------- 295.7/400.7 MB 1.5 MB/s eta 0:01:11\n",
      "     ---------------------------- --------- 296.0/400.7 MB 1.5 MB/s eta 0:01:12\n",
      "     ---------------------------- --------- 296.2/400.7 MB 1.5 MB/s eta 0:01:12\n",
      "     ---------------------------- --------- 296.2/400.7 MB 1.5 MB/s eta 0:01:12\n",
      "     ---------------------------- --------- 296.5/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 296.7/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 297.0/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 297.3/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 297.5/400.7 MB 1.4 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 297.8/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 298.3/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 298.6/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 298.6/400.7 MB 1.4 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 298.8/400.7 MB 1.4 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 299.1/400.7 MB 1.4 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 299.4/400.7 MB 1.3 MB/s eta 0:01:17\n",
      "     ---------------------------- --------- 299.9/400.7 MB 1.3 MB/s eta 0:01:17\n",
      "     ---------------------------- --------- 300.2/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 300.4/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 300.7/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 300.9/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 301.2/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 301.7/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 302.0/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 302.0/400.7 MB 1.3 MB/s eta 0:01:16\n",
      "     ---------------------------- --------- 302.5/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 302.8/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 303.3/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ---------------------------- --------- 303.8/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 304.1/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 304.3/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 304.9/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 304.9/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 305.4/400.7 MB 1.3 MB/s eta 0:01:13\n",
      "     ---------------------------- --------- 305.7/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ---------------------------- --------- 305.7/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ----------------------------- -------- 305.9/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 305.9/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 306.2/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 306.4/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 306.7/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 307.2/400.7 MB 1.3 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 307.8/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ----------------------------- -------- 307.8/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ----------------------------- -------- 308.0/400.7 MB 1.3 MB/s eta 0:01:14\n",
      "     ----------------------------- -------- 308.3/400.7 MB 1.2 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 308.5/400.7 MB 1.2 MB/s eta 0:01:15\n",
      "     ----------------------------- -------- 308.8/400.7 MB 1.2 MB/s eta 0:01:16\n",
      "     ----------------------------- -------- 308.8/400.7 MB 1.2 MB/s eta 0:01:16\n",
      "     ----------------------------- -------- 308.8/400.7 MB 1.2 MB/s eta 0:01:16\n",
      "     ----------------------------- -------- 308.8/400.7 MB 1.2 MB/s eta 0:01:16\n",
      "     ----------------------------- -------- 309.1/400.7 MB 1.2 MB/s eta 0:01:18\n",
      "     ----------------------------- -------- 309.1/400.7 MB 1.2 MB/s eta 0:01:18\n",
      "     ----------------------------- -------- 309.3/400.7 MB 1.2 MB/s eta 0:01:20\n",
      "     ----------------------------- -------- 309.3/400.7 MB 1.2 MB/s eta 0:01:20\n",
      "     ----------------------------- -------- 309.6/400.7 MB 1.1 MB/s eta 0:01:21\n",
      "     ----------------------------- -------- 309.6/400.7 MB 1.1 MB/s eta 0:01:21\n",
      "     ----------------------------- -------- 309.6/400.7 MB 1.1 MB/s eta 0:01:21\n",
      "     ----------------------------- -------- 309.9/400.7 MB 1.1 MB/s eta 0:01:22\n",
      "     ----------------------------- -------- 310.1/400.7 MB 1.1 MB/s eta 0:01:23\n",
      "     ----------------------------- -------- 310.1/400.7 MB 1.1 MB/s eta 0:01:23\n",
      "     ----------------------------- -------- 310.4/400.7 MB 1.1 MB/s eta 0:01:24\n",
      "     ----------------------------- -------- 310.4/400.7 MB 1.1 MB/s eta 0:01:24\n",
      "     ----------------------------- -------- 310.6/400.7 MB 1.0 MB/s eta 0:01:28\n",
      "     ----------------------------- -------- 310.6/400.7 MB 1.0 MB/s eta 0:01:28\n",
      "     ----------------------------- -------- 310.6/400.7 MB 1.0 MB/s eta 0:01:28\n",
      "     ----------------------------- -------- 310.6/400.7 MB 1.0 MB/s eta 0:01:28\n",
      "     --------------------------- -------- 310.9/400.7 MB 970.9 kB/s eta 0:01:33\n",
      "     --------------------------- -------- 311.2/400.7 MB 969.9 kB/s eta 0:01:33\n",
      "     --------------------------- -------- 311.2/400.7 MB 969.9 kB/s eta 0:01:33\n",
      "     --------------------------- -------- 311.4/400.7 MB 946.7 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 311.7/400.7 MB 949.7 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 311.7/400.7 MB 949.7 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 312.0/400.7 MB 940.8 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 312.2/400.7 MB 940.4 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 312.2/400.7 MB 940.4 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 312.5/400.7 MB 940.4 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 312.7/400.7 MB 938.4 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 312.7/400.7 MB 938.4 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 313.0/400.7 MB 908.8 kB/s eta 0:01:37\n",
      "     ---------------------------- ------- 313.3/400.7 MB 901.4 kB/s eta 0:01:37\n",
      "     ---------------------------- ------- 313.3/400.7 MB 901.4 kB/s eta 0:01:37\n",
      "     ---------------------------- ------- 313.5/400.7 MB 884.9 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 313.8/400.7 MB 893.7 kB/s eta 0:01:38\n",
      "     ---------------------------- ------- 314.0/400.7 MB 896.4 kB/s eta 0:01:37\n",
      "     ---------------------------- ------- 314.3/400.7 MB 901.3 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 314.6/400.7 MB 904.5 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 314.8/400.7 MB 905.7 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 314.8/400.7 MB 905.7 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 315.1/400.7 MB 893.6 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 315.1/400.7 MB 893.6 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 315.4/400.7 MB 896.9 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 315.9/400.7 MB 906.2 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 315.9/400.7 MB 906.2 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 316.1/400.7 MB 909.5 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.4/400.7 MB 908.8 kB/s eta 0:01:33\n",
      "     ---------------------------- ------- 316.7/400.7 MB 884.4 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 316.7/400.7 MB 884.4 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 316.7/400.7 MB 884.4 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 316.9/400.7 MB 880.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 316.9/400.7 MB 880.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 316.9/400.7 MB 880.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 317.2/400.7 MB 888.2 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.2/400.7 MB 888.2 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.2/400.7 MB 888.2 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.5/400.7 MB 891.9 kB/s eta 0:01:34\n",
      "     ---------------------------- ------- 317.7/400.7 MB 876.6 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 317.7/400.7 MB 876.6 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 317.7/400.7 MB 876.6 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 318.0/400.7 MB 872.8 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 318.0/400.7 MB 872.8 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 318.0/400.7 MB 872.8 kB/s eta 0:01:35\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.2/400.7 MB 867.2 kB/s eta 0:01:36\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.5/400.7 MB 835.4 kB/s eta 0:01:39\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 318.8/400.7 MB 753.4 kB/s eta 0:01:49\n",
      "     ---------------------------- ------- 319.0/400.7 MB 721.4 kB/s eta 0:01:54\n",
      "     ---------------------------- ------- 319.0/400.7 MB 721.4 kB/s eta 0:01:54\n",
      "     ---------------------------- ------- 319.0/400.7 MB 721.4 kB/s eta 0:01:54\n",
      "     ---------------------------- ------- 319.0/400.7 MB 721.4 kB/s eta 0:01:54\n",
      "     ---------------------------- ------- 319.0/400.7 MB 721.4 kB/s eta 0:01:54\n",
      "     ---------------------------- ------- 319.3/400.7 MB 681.9 kB/s eta 0:02:00\n",
      "     ---------------------------- ------- 319.3/400.7 MB 681.9 kB/s eta 0:02:00\n",
      "     ---------------------------- ------- 319.3/400.7 MB 681.9 kB/s eta 0:02:00\n",
      "     ---------------------------- ------- 319.3/400.7 MB 681.9 kB/s eta 0:02:00\n",
      "     ---------------------------- ------- 319.3/400.7 MB 681.9 kB/s eta 0:02:00\n",
      "     ---------------------------- ------- 319.6/400.7 MB 641.6 kB/s eta 0:02:07\n",
      "     ---------------------------- ------- 319.6/400.7 MB 641.6 kB/s eta 0:02:07\n",
      "     ---------------------------- ------- 319.6/400.7 MB 641.6 kB/s eta 0:02:07\n",
      "     ---------------------------- ------- 319.6/400.7 MB 641.6 kB/s eta 0:02:07\n",
      "     ---------------------------- ------- 319.8/400.7 MB 613.9 kB/s eta 0:02:12\n",
      "     ---------------------------- ------- 319.8/400.7 MB 613.9 kB/s eta 0:02:12\n",
      "     ---------------------------- ------- 320.1/400.7 MB 600.4 kB/s eta 0:02:15\n",
      "     ---------------------------- ------- 320.1/400.7 MB 600.4 kB/s eta 0:02:15\n",
      "     ---------------------------- ------- 320.3/400.7 MB 579.1 kB/s eta 0:02:19\n",
      "     ---------------------------- ------- 320.3/400.7 MB 579.1 kB/s eta 0:02:19\n",
      "     ---------------------------- ------- 320.6/400.7 MB 560.4 kB/s eta 0:02:23\n",
      "     ---------------------------- ------- 320.6/400.7 MB 560.4 kB/s eta 0:02:23\n",
      "     ---------------------------- ------- 320.9/400.7 MB 546.6 kB/s eta 0:02:26\n",
      "     ---------------------------- ------- 320.9/400.7 MB 546.6 kB/s eta 0:02:26\n",
      "     ---------------------------- ------- 320.9/400.7 MB 546.6 kB/s eta 0:02:26\n",
      "     ---------------------------- ------- 321.1/400.7 MB 533.6 kB/s eta 0:02:30\n",
      "     ---------------------------- ------- 321.1/400.7 MB 533.6 kB/s eta 0:02:30\n",
      "     ---------------------------- ------- 321.1/400.7 MB 533.6 kB/s eta 0:02:30\n",
      "     ---------------------------- ------- 321.4/400.7 MB 518.0 kB/s eta 0:02:34\n",
      "     ---------------------------- ------- 321.4/400.7 MB 518.0 kB/s eta 0:02:34\n",
      "     ---------------------------- ------- 321.4/400.7 MB 518.0 kB/s eta 0:02:34\n",
      "     ---------------------------- ------- 321.4/400.7 MB 518.0 kB/s eta 0:02:34\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.7/400.7 MB 500.2 kB/s eta 0:02:38\n",
      "     ---------------------------- ------- 321.9/400.7 MB 434.5 kB/s eta 0:03:02\n",
      "     ---------------------------- ------- 321.9/400.7 MB 434.5 kB/s eta 0:03:02\n",
      "     ---------------------------- ------- 321.9/400.7 MB 434.5 kB/s eta 0:03:02\n",
      "     ---------------------------- ------- 321.9/400.7 MB 434.5 kB/s eta 0:03:02\n",
      "     ---------------------------- ------- 321.9/400.7 MB 434.5 kB/s eta 0:03:02\n",
      "     ---------------------------- ------- 322.2/400.7 MB 429.3 kB/s eta 0:03:03\n",
      "     ---------------------------- ------- 322.2/400.7 MB 429.3 kB/s eta 0:03:03\n",
      "     ---------------------------- ------- 322.2/400.7 MB 429.3 kB/s eta 0:03:03\n",
      "     ---------------------------- ------- 322.2/400.7 MB 429.3 kB/s eta 0:03:03\n",
      "     ---------------------------- ------- 322.2/400.7 MB 429.3 kB/s eta 0:03:03\n",
      "     ---------------------------- ------- 322.4/400.7 MB 410.7 kB/s eta 0:03:11\n",
      "     ---------------------------- ------- 322.4/400.7 MB 410.7 kB/s eta 0:03:11\n",
      "     ---------------------------- ------- 322.4/400.7 MB 410.7 kB/s eta 0:03:11\n",
      "     ---------------------------- ------- 322.4/400.7 MB 410.7 kB/s eta 0:03:11\n",
      "     ---------------------------- ------- 322.4/400.7 MB 410.7 kB/s eta 0:03:11\n",
      "     ---------------------------- ------- 322.7/400.7 MB 402.2 kB/s eta 0:03:14\n",
      "     ---------------------------- ------- 322.7/400.7 MB 402.2 kB/s eta 0:03:14\n",
      "     ---------------------------- ------- 322.7/400.7 MB 402.2 kB/s eta 0:03:14\n",
      "     ----------------------------- ------ 323.0/400.7 MB 402.8 kB/s eta 0:03:13\n",
      "     ----------------------------- ------ 323.0/400.7 MB 402.8 kB/s eta 0:03:13\n",
      "     ----------------------------- ------ 323.0/400.7 MB 402.8 kB/s eta 0:03:13\n",
      "     ----------------------------- ------ 323.2/400.7 MB 394.4 kB/s eta 0:03:17\n",
      "     ----------------------------- ------ 323.2/400.7 MB 394.4 kB/s eta 0:03:17\n",
      "     ----------------------------- ------ 323.2/400.7 MB 394.4 kB/s eta 0:03:17\n",
      "     ----------------------------- ------ 323.5/400.7 MB 385.5 kB/s eta 0:03:21\n",
      "     ----------------------------- ------ 323.5/400.7 MB 385.5 kB/s eta 0:03:21\n",
      "     ----------------------------- ------ 323.5/400.7 MB 385.5 kB/s eta 0:03:21\n",
      "     ----------------------------- ------ 323.7/400.7 MB 370.5 kB/s eta 0:03:28\n",
      "     ----------------------------- ------ 323.7/400.7 MB 370.5 kB/s eta 0:03:28\n",
      "     ----------------------------- ------ 323.7/400.7 MB 370.5 kB/s eta 0:03:28\n",
      "     ----------------------------- ------ 324.0/400.7 MB 368.2 kB/s eta 0:03:29\n",
      "     ----------------------------- ------ 324.0/400.7 MB 368.2 kB/s eta 0:03:29\n",
      "     ----------------------------- ------ 324.0/400.7 MB 368.2 kB/s eta 0:03:29\n",
      "     ----------------------------- ------ 324.3/400.7 MB 359.4 kB/s eta 0:03:33\n",
      "     ----------------------------- ------ 324.3/400.7 MB 359.4 kB/s eta 0:03:33\n",
      "     ----------------------------- ------ 324.3/400.7 MB 359.4 kB/s eta 0:03:33\n",
      "     ----------------------------- ------ 324.3/400.7 MB 359.4 kB/s eta 0:03:33\n",
      "     ----------------------------- ------ 324.3/400.7 MB 359.4 kB/s eta 0:03:33\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.5/400.7 MB 324.7 kB/s eta 0:03:55\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 324.8/400.7 MB 264.8 kB/s eta 0:04:47\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.1/400.7 MB 237.5 kB/s eta 0:05:19\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.3/400.7 MB 227.5 kB/s eta 0:05:32\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.6/400.7 MB 228.6 kB/s eta 0:05:29\n",
      "     ----------------------------- ------ 325.8/400.7 MB 222.0 kB/s eta 0:05:37\n",
      "     ----------------------------- ------ 325.8/400.7 MB 222.0 kB/s eta 0:05:37\n",
      "     ----------------------------- ------ 325.8/400.7 MB 222.0 kB/s eta 0:05:37\n",
      "     ----------------------------- ------ 325.8/400.7 MB 222.0 kB/s eta 0:05:37\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.1/400.7 MB 223.1 kB/s eta 0:05:35\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.4/400.7 MB 218.6 kB/s eta 0:05:40\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.6/400.7 MB 201.8 kB/s eta 0:06:07\n",
      "     ----------------------------- ------ 326.9/400.7 MB 185.0 kB/s eta 0:06:39\n",
      "     ----------------------------- ------ 326.9/400.7 MB 185.0 kB/s eta 0:06:39\n",
      "     ----------------------------- ------ 326.9/400.7 MB 185.0 kB/s eta 0:06:39\n",
      "     ----------------------------- ------ 326.9/400.7 MB 185.0 kB/s eta 0:06:39\n",
      "     ----------------------------- ------ 327.2/400.7 MB 185.0 kB/s eta 0:06:38\n",
      "     ----------------------------- ------ 327.2/400.7 MB 185.0 kB/s eta 0:06:38\n",
      "     ----------------------------- ------ 327.2/400.7 MB 185.0 kB/s eta 0:06:38\n",
      "     ----------------------------- ------ 327.2/400.7 MB 185.0 kB/s eta 0:06:38\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.4/400.7 MB 191.7 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 327.7/400.7 MB 184.6 kB/s eta 0:06:36\n",
      "     ----------------------------- ------ 327.7/400.7 MB 184.6 kB/s eta 0:06:36\n",
      "     ----------------------------- ------ 327.7/400.7 MB 184.6 kB/s eta 0:06:36\n",
      "     ----------------------------- ------ 327.7/400.7 MB 184.6 kB/s eta 0:06:36\n",
      "     ----------------------------- ------ 327.9/400.7 MB 187.0 kB/s eta 0:06:29\n",
      "     ----------------------------- ------ 327.9/400.7 MB 187.0 kB/s eta 0:06:29\n",
      "     ----------------------------- ------ 327.9/400.7 MB 187.0 kB/s eta 0:06:29\n",
      "     ----------------------------- ------ 328.2/400.7 MB 188.4 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 328.2/400.7 MB 188.4 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 328.2/400.7 MB 188.4 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 328.2/400.7 MB 188.4 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 328.5/400.7 MB 192.6 kB/s eta 0:06:15\n",
      "     ----------------------------- ------ 328.5/400.7 MB 192.6 kB/s eta 0:06:15\n",
      "     ----------------------------- ------ 328.5/400.7 MB 192.6 kB/s eta 0:06:15\n",
      "     ----------------------------- ------ 328.7/400.7 MB 192.7 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 328.7/400.7 MB 192.7 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 328.7/400.7 MB 192.7 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 328.7/400.7 MB 192.7 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 329.0/400.7 MB 186.5 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 329.0/400.7 MB 186.5 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 329.0/400.7 MB 186.5 kB/s eta 0:06:25\n",
      "     ----------------------------- ------ 329.3/400.7 MB 187.0 kB/s eta 0:06:22\n",
      "     ----------------------------- ------ 329.3/400.7 MB 187.0 kB/s eta 0:06:22\n",
      "     ----------------------------- ------ 329.3/400.7 MB 187.0 kB/s eta 0:06:22\n",
      "     ----------------------------- ------ 329.5/400.7 MB 186.1 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 329.5/400.7 MB 186.1 kB/s eta 0:06:23\n",
      "     ----------------------------- ------ 329.8/400.7 MB 187.5 kB/s eta 0:06:19\n",
      "     ----------------------------- ------ 329.8/400.7 MB 187.5 kB/s eta 0:06:19\n",
      "     ----------------------------- ------ 329.8/400.7 MB 187.5 kB/s eta 0:06:19\n",
      "     ----------------------------- ------ 329.8/400.7 MB 187.5 kB/s eta 0:06:19\n",
      "     ----------------------------- ------ 330.0/400.7 MB 189.2 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 330.0/400.7 MB 189.2 kB/s eta 0:06:14\n",
      "     ----------------------------- ------ 330.3/400.7 MB 195.5 kB/s eta 0:06:00\n",
      "     ----------------------------- ------ 330.3/400.7 MB 195.5 kB/s eta 0:06:00\n",
      "     ----------------------------- ------ 330.6/400.7 MB 201.1 kB/s eta 0:05:49\n",
      "     ----------------------------- ------ 330.6/400.7 MB 201.1 kB/s eta 0:05:49\n",
      "     ----------------------------- ------ 330.8/400.7 MB 231.5 kB/s eta 0:05:02\n",
      "     ----------------------------- ------ 330.8/400.7 MB 231.5 kB/s eta 0:05:02\n",
      "     ----------------------------- ------ 330.8/400.7 MB 231.5 kB/s eta 0:05:02\n",
      "     ----------------------------- ------ 331.1/400.7 MB 237.4 kB/s eta 0:04:54\n",
      "     ----------------------------- ------ 331.1/400.7 MB 237.4 kB/s eta 0:04:54\n",
      "     ----------------------------- ------ 331.4/400.7 MB 242.7 kB/s eta 0:04:46\n",
      "     ----------------------------- ------ 331.4/400.7 MB 242.7 kB/s eta 0:04:46\n",
      "     ----------------------------- ------ 331.6/400.7 MB 248.4 kB/s eta 0:04:38\n",
      "     ----------------------------- ------ 331.6/400.7 MB 248.4 kB/s eta 0:04:38\n",
      "     ----------------------------- ------ 331.6/400.7 MB 248.4 kB/s eta 0:04:38\n",
      "     ----------------------------- ------ 331.9/400.7 MB 253.4 kB/s eta 0:04:32\n",
      "     ----------------------------- ------ 331.9/400.7 MB 253.4 kB/s eta 0:04:32\n",
      "     ----------------------------- ------ 331.9/400.7 MB 253.4 kB/s eta 0:04:32\n",
      "     ----------------------------- ------ 332.1/400.7 MB 256.3 kB/s eta 0:04:28\n",
      "     ----------------------------- ------ 332.1/400.7 MB 256.3 kB/s eta 0:04:28\n",
      "     ----------------------------- ------ 332.1/400.7 MB 256.3 kB/s eta 0:04:28\n",
      "     ----------------------------- ------ 332.4/400.7 MB 260.9 kB/s eta 0:04:22\n",
      "     ----------------------------- ------ 332.4/400.7 MB 260.9 kB/s eta 0:04:22\n",
      "     ----------------------------- ------ 332.7/400.7 MB 265.5 kB/s eta 0:04:17\n",
      "     ----------------------------- ------ 332.7/400.7 MB 265.5 kB/s eta 0:04:17\n",
      "     ----------------------------- ------ 332.7/400.7 MB 265.5 kB/s eta 0:04:17\n",
      "     ----------------------------- ------ 332.9/400.7 MB 289.8 kB/s eta 0:03:54\n",
      "     ----------------------------- ------ 332.9/400.7 MB 289.8 kB/s eta 0:03:54\n",
      "     ----------------------------- ------ 333.2/400.7 MB 294.3 kB/s eta 0:03:50\n",
      "     ----------------------------- ------ 333.2/400.7 MB 294.3 kB/s eta 0:03:50\n",
      "     ----------------------------- ------ 333.4/400.7 MB 299.3 kB/s eta 0:03:45\n",
      "     ----------------------------- ------ 333.4/400.7 MB 299.3 kB/s eta 0:03:45\n",
      "     ----------------------------- ------ 333.7/400.7 MB 303.9 kB/s eta 0:03:41\n",
      "     ----------------------------- ------ 333.7/400.7 MB 303.9 kB/s eta 0:03:41\n",
      "     ------------------------------ ----- 334.0/400.7 MB 308.8 kB/s eta 0:03:36\n",
      "     ------------------------------ ----- 334.2/400.7 MB 315.0 kB/s eta 0:03:31\n",
      "     ------------------------------ ----- 334.2/400.7 MB 315.0 kB/s eta 0:03:31\n",
      "     ------------------------------ ----- 334.5/400.7 MB 320.6 kB/s eta 0:03:27\n",
      "     ------------------------------ ----- 334.5/400.7 MB 320.6 kB/s eta 0:03:27\n",
      "     ------------------------------ ----- 334.8/400.7 MB 324.3 kB/s eta 0:03:24\n",
      "     ------------------------------ ----- 335.0/400.7 MB 349.5 kB/s eta 0:03:08\n",
      "     ------------------------------ ----- 335.3/400.7 MB 356.4 kB/s eta 0:03:04\n",
      "     ------------------------------ ----- 335.3/400.7 MB 356.4 kB/s eta 0:03:04\n",
      "     ------------------------------ ----- 335.5/400.7 MB 360.5 kB/s eta 0:03:01\n",
      "     ------------------------------ ----- 335.5/400.7 MB 360.5 kB/s eta 0:03:01\n",
      "     ------------------------------ ----- 335.8/400.7 MB 365.1 kB/s eta 0:02:58\n",
      "     ------------------------------ ----- 335.8/400.7 MB 365.1 kB/s eta 0:02:58\n",
      "     ------------------------------ ----- 336.3/400.7 MB 376.6 kB/s eta 0:02:51\n",
      "     ------------------------------ ----- 336.3/400.7 MB 376.6 kB/s eta 0:02:51\n",
      "     ------------------------------ ----- 336.6/400.7 MB 382.7 kB/s eta 0:02:48\n",
      "     ------------------------------ ----- 336.9/400.7 MB 386.7 kB/s eta 0:02:45\n",
      "     ------------------------------ ----- 336.9/400.7 MB 386.7 kB/s eta 0:02:45\n",
      "     ------------------------------ ----- 337.1/400.7 MB 418.5 kB/s eta 0:02:32\n",
      "     ------------------------------ ----- 337.4/400.7 MB 424.4 kB/s eta 0:02:30\n",
      "     ------------------------------ ----- 337.6/400.7 MB 429.9 kB/s eta 0:02:27\n",
      "     ------------------------------ ----- 337.6/400.7 MB 429.9 kB/s eta 0:02:27\n",
      "     ------------------------------ ----- 337.9/400.7 MB 434.0 kB/s eta 0:02:25\n",
      "     ------------------------------ ----- 338.2/400.7 MB 439.3 kB/s eta 0:02:23\n",
      "     ------------------------------ ----- 338.4/400.7 MB 444.6 kB/s eta 0:02:20\n",
      "     ------------------------------ ----- 338.7/400.7 MB 451.5 kB/s eta 0:02:18\n",
      "     ------------------------------ ----- 339.0/400.7 MB 457.3 kB/s eta 0:02:15\n",
      "     ------------------------------ ----- 339.2/400.7 MB 464.3 kB/s eta 0:02:13\n",
      "     ------------------------------ ----- 339.7/400.7 MB 476.1 kB/s eta 0:02:08\n",
      "     ------------------------------ ----- 340.0/400.7 MB 481.6 kB/s eta 0:02:06\n",
      "     ------------------------------ ----- 340.3/400.7 MB 512.9 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 340.3/400.7 MB 512.9 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 340.5/400.7 MB 512.8 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 340.5/400.7 MB 512.8 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 340.8/400.7 MB 514.1 kB/s eta 0:01:57\n",
      "     ------------------------------ ----- 340.8/400.7 MB 514.1 kB/s eta 0:01:57\n",
      "     ------------------------------ ----- 340.8/400.7 MB 514.1 kB/s eta 0:01:57\n",
      "     ------------------------------ ----- 340.8/400.7 MB 514.1 kB/s eta 0:01:57\n",
      "     ------------------------------ ----- 340.8/400.7 MB 514.1 kB/s eta 0:01:57\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.0/400.7 MB 508.1 kB/s eta 0:01:58\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.3/400.7 MB 516.1 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.6/400.7 MB 515.8 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.6/400.7 MB 515.8 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.6/400.7 MB 515.8 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.8/400.7 MB 514.3 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.8/400.7 MB 514.3 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 341.8/400.7 MB 514.3 kB/s eta 0:01:55\n",
      "     ------------------------------ ----- 342.1/400.7 MB 528.6 kB/s eta 0:01:51\n",
      "     ------------------------------ ----- 342.1/400.7 MB 528.6 kB/s eta 0:01:51\n",
      "     ------------------------------ ----- 342.4/400.7 MB 529.3 kB/s eta 0:01:51\n",
      "     ------------------------------ ----- 342.4/400.7 MB 529.3 kB/s eta 0:01:51\n",
      "     ------------------------------ ----- 342.6/400.7 MB 529.8 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.6/400.7 MB 529.8 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.6/400.7 MB 529.8 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.6/400.7 MB 529.8 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.6/400.7 MB 529.8 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.9/400.7 MB 527.3 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.9/400.7 MB 527.3 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 342.9/400.7 MB 527.3 kB/s eta 0:01:50\n",
      "     ------------------------------ ----- 343.1/400.7 MB 531.5 kB/s eta 0:01:49\n",
      "     ------------------------------ ----- 343.1/400.7 MB 531.5 kB/s eta 0:01:49\n",
      "     ------------------------------ ----- 343.4/400.7 MB 560.8 kB/s eta 0:01:43\n",
      "     ------------------------------ ----- 343.4/400.7 MB 560.8 kB/s eta 0:01:43\n",
      "     ------------------------------ ----- 343.4/400.7 MB 560.8 kB/s eta 0:01:43\n",
      "     ------------------------------ ----- 343.7/400.7 MB 559.5 kB/s eta 0:01:42\n",
      "     ------------------------------ ----- 343.7/400.7 MB 559.5 kB/s eta 0:01:42\n",
      "     ------------------------------ ----- 343.9/400.7 MB 561.4 kB/s eta 0:01:42\n",
      "     ------------------------------ ----- 343.9/400.7 MB 561.4 kB/s eta 0:01:42\n",
      "     ------------------------------ ----- 344.2/400.7 MB 561.6 kB/s eta 0:01:41\n",
      "     ------------------------------ ----- 344.2/400.7 MB 561.6 kB/s eta 0:01:41\n",
      "     ------------------------------ ----- 344.5/400.7 MB 560.7 kB/s eta 0:01:41\n",
      "     ------------------------------ ----- 344.5/400.7 MB 560.7 kB/s eta 0:01:41\n",
      "     ------------------------------ ----- 344.7/400.7 MB 568.1 kB/s eta 0:01:39\n",
      "     ------------------------------ ----- 344.7/400.7 MB 568.1 kB/s eta 0:01:39\n",
      "     ------------------------------ ----- 344.7/400.7 MB 568.1 kB/s eta 0:01:39\n",
      "     ------------------------------ ----- 345.0/400.7 MB 571.7 kB/s eta 0:01:38\n",
      "     ------------------------------ ----- 345.0/400.7 MB 571.7 kB/s eta 0:01:38\n",
      "     ------------------------------ ----- 345.0/400.7 MB 571.7 kB/s eta 0:01:38\n",
      "     ------------------------------ ----- 345.0/400.7 MB 571.7 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.2/400.7 MB 570.5 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.2/400.7 MB 570.5 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.2/400.7 MB 570.5 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.2/400.7 MB 570.5 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.5/400.7 MB 566.3 kB/s eta 0:01:38\n",
      "     ------------------------------- ---- 345.8/400.7 MB 569.7 kB/s eta 0:01:37\n",
      "     ------------------------------- ---- 346.0/400.7 MB 580.4 kB/s eta 0:01:35\n",
      "     ------------------------------- ---- 346.0/400.7 MB 580.4 kB/s eta 0:01:35\n",
      "     ------------------------------- ---- 346.0/400.7 MB 580.4 kB/s eta 0:01:35\n",
      "     ------------------------------- ---- 346.3/400.7 MB 579.1 kB/s eta 0:01:34\n",
      "     ------------------------------- ---- 346.3/400.7 MB 579.1 kB/s eta 0:01:34\n",
      "     ------------------------------- ---- 346.6/400.7 MB 584.9 kB/s eta 0:01:33\n",
      "     ------------------------------- ---- 346.8/400.7 MB 589.4 kB/s eta 0:01:32\n",
      "     ------------------------------- ---- 347.1/400.7 MB 593.8 kB/s eta 0:01:31\n",
      "     ------------------------------- ---- 347.1/400.7 MB 593.8 kB/s eta 0:01:31\n",
      "     ------------------------------- ---- 347.1/400.7 MB 593.8 kB/s eta 0:01:31\n",
      "     ------------------------------- ---- 347.3/400.7 MB 593.5 kB/s eta 0:01:30\n",
      "     ------------------------------- ---- 347.6/400.7 MB 596.4 kB/s eta 0:01:29\n",
      "     ------------------------------- ---- 347.6/400.7 MB 596.4 kB/s eta 0:01:29\n",
      "     ------------------------------- ---- 347.9/400.7 MB 604.9 kB/s eta 0:01:28\n",
      "     ------------------------------- ---- 348.1/400.7 MB 606.4 kB/s eta 0:01:27\n",
      "     ------------------------------- ---- 348.1/400.7 MB 606.4 kB/s eta 0:01:27\n",
      "     ------------------------------- ---- 348.1/400.7 MB 606.4 kB/s eta 0:01:27\n",
      "     ------------------------------- ---- 348.1/400.7 MB 606.4 kB/s eta 0:01:27\n",
      "     ------------------------------- ---- 348.4/400.7 MB 600.8 kB/s eta 0:01:28\n",
      "     ------------------------------- ---- 348.4/400.7 MB 600.8 kB/s eta 0:01:28\n",
      "     ------------------------------- ---- 348.4/400.7 MB 600.8 kB/s eta 0:01:28\n",
      "     ------------------------------- ---- 348.7/400.7 MB 598.9 kB/s eta 0:01:27\n",
      "     ------------------------------- ---- 348.9/400.7 MB 602.0 kB/s eta 0:01:26\n",
      "     ------------------------------- ---- 349.2/400.7 MB 606.1 kB/s eta 0:01:25\n",
      "     ------------------------------- ---- 349.2/400.7 MB 606.1 kB/s eta 0:01:25\n",
      "     ------------------------------- ---- 349.4/400.7 MB 607.1 kB/s eta 0:01:25\n",
      "     ------------------------------- ---- 349.4/400.7 MB 607.1 kB/s eta 0:01:25\n",
      "     ------------------------------- ---- 349.7/400.7 MB 606.7 kB/s eta 0:01:24\n",
      "     ------------------------------- ---- 349.7/400.7 MB 606.7 kB/s eta 0:01:24\n",
      "     ------------------------------- ---- 350.0/400.7 MB 611.8 kB/s eta 0:01:23\n",
      "     ------------------------------- ---- 350.2/400.7 MB 613.3 kB/s eta 0:01:23\n",
      "     ------------------------------- ---- 350.2/400.7 MB 613.3 kB/s eta 0:01:23\n",
      "     ------------------------------- ---- 350.5/400.7 MB 619.1 kB/s eta 0:01:22\n",
      "     ------------------------------- ---- 350.5/400.7 MB 619.1 kB/s eta 0:01:22\n",
      "     ------------------------------- ---- 350.7/400.7 MB 620.7 kB/s eta 0:01:21\n",
      "     ------------------------------- ---- 350.7/400.7 MB 620.7 kB/s eta 0:01:21\n",
      "     ------------------------------- ---- 351.0/400.7 MB 622.0 kB/s eta 0:01:20\n",
      "     ------------------------------- ---- 351.0/400.7 MB 622.0 kB/s eta 0:01:20\n",
      "     ------------------------------- ---- 351.3/400.7 MB 624.7 kB/s eta 0:01:20\n",
      "     ------------------------------- ---- 351.3/400.7 MB 624.7 kB/s eta 0:01:20\n",
      "     ------------------------------- ---- 351.3/400.7 MB 624.7 kB/s eta 0:01:20\n",
      "     ------------------------------- ---- 351.5/400.7 MB 626.3 kB/s eta 0:01:19\n",
      "     ------------------------------- ---- 351.5/400.7 MB 626.3 kB/s eta 0:01:19\n",
      "     ------------------------------- ---- 351.8/400.7 MB 623.6 kB/s eta 0:01:19\n",
      "     ------------------------------- ---- 351.8/400.7 MB 623.6 kB/s eta 0:01:19\n",
      "     ------------------------------- ---- 352.1/400.7 MB 624.3 kB/s eta 0:01:18\n",
      "     ------------------------------- ---- 352.1/400.7 MB 624.3 kB/s eta 0:01:18\n",
      "     ------------------------------- ---- 352.3/400.7 MB 627.3 kB/s eta 0:01:18\n",
      "     ------------------------------- ---- 352.3/400.7 MB 627.3 kB/s eta 0:01:18\n",
      "     ------------------------------- ---- 352.6/400.7 MB 627.6 kB/s eta 0:01:17\n",
      "     ------------------------------- ---- 352.8/400.7 MB 629.1 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 352.8/400.7 MB 629.1 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 353.1/400.7 MB 626.6 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 353.4/400.7 MB 629.6 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 353.4/400.7 MB 629.6 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 353.6/400.7 MB 630.5 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 353.6/400.7 MB 630.5 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 353.9/400.7 MB 621.7 kB/s eta 0:01:16\n",
      "     ------------------------------- ---- 354.2/400.7 MB 624.6 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 354.2/400.7 MB 624.6 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 354.4/400.7 MB 624.3 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 354.4/400.7 MB 624.3 kB/s eta 0:01:15\n",
      "     ------------------------------- ---- 354.7/400.7 MB 625.6 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 354.9/400.7 MB 624.0 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 355.2/400.7 MB 621.7 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 355.2/400.7 MB 621.7 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 355.2/400.7 MB 621.7 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 355.5/400.7 MB 618.4 kB/s eta 0:01:14\n",
      "     ------------------------------- ---- 355.7/400.7 MB 616.2 kB/s eta 0:01:13\n",
      "     ------------------------------- ---- 356.0/400.7 MB 616.5 kB/s eta 0:01:13\n",
      "     ------------------------------- ---- 356.0/400.7 MB 616.5 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 356.3/400.7 MB 614.9 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 356.3/400.7 MB 614.9 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 356.5/400.7 MB 607.7 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 356.5/400.7 MB 607.7 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 356.8/400.7 MB 605.1 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 357.0/400.7 MB 604.5 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 357.0/400.7 MB 604.5 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 357.3/400.7 MB 596.0 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 357.6/400.7 MB 587.0 kB/s eta 0:01:14\n",
      "     -------------------------------- --- 357.6/400.7 MB 587.0 kB/s eta 0:01:14\n",
      "     -------------------------------- --- 357.8/400.7 MB 584.3 kB/s eta 0:01:14\n",
      "     -------------------------------- --- 357.8/400.7 MB 584.3 kB/s eta 0:01:14\n",
      "     -------------------------------- --- 358.1/400.7 MB 584.9 kB/s eta 0:01:13\n",
      "     -------------------------------- --- 358.4/400.7 MB 588.2 kB/s eta 0:01:12\n",
      "     -------------------------------- --- 358.6/400.7 MB 600.8 kB/s eta 0:01:10\n",
      "     -------------------------------- --- 358.6/400.7 MB 600.8 kB/s eta 0:01:10\n",
      "     -------------------------------- --- 358.9/400.7 MB 600.8 kB/s eta 0:01:10\n",
      "     -------------------------------- --- 358.9/400.7 MB 600.8 kB/s eta 0:01:10\n",
      "     -------------------------------- --- 358.9/400.7 MB 600.8 kB/s eta 0:01:10\n",
      "     -------------------------------- --- 359.1/400.7 MB 617.0 kB/s eta 0:01:08\n",
      "     -------------------------------- --- 359.4/400.7 MB 620.7 kB/s eta 0:01:07\n",
      "     -------------------------------- --- 359.7/400.7 MB 624.3 kB/s eta 0:01:06\n",
      "     -------------------------------- --- 359.9/400.7 MB 627.6 kB/s eta 0:01:05\n",
      "     -------------------------------- --- 359.9/400.7 MB 627.6 kB/s eta 0:01:05\n",
      "     -------------------------------- --- 360.2/400.7 MB 647.7 kB/s eta 0:01:03\n",
      "     -------------------------------- --- 360.4/400.7 MB 651.2 kB/s eta 0:01:02\n",
      "     -------------------------------- --- 360.7/400.7 MB 656.3 kB/s eta 0:01:01\n",
      "     -------------------------------- --- 361.0/400.7 MB 661.4 kB/s eta 0:01:00\n",
      "     -------------------------------- --- 361.0/400.7 MB 661.4 kB/s eta 0:01:00\n",
      "     -------------------------------- --- 361.2/400.7 MB 662.6 kB/s eta 0:01:00\n",
      "     -------------------------------- --- 361.2/400.7 MB 662.6 kB/s eta 0:01:00\n",
      "     -------------------------------- --- 361.5/400.7 MB 665.8 kB/s eta 0:00:59\n",
      "     -------------------------------- --- 361.8/400.7 MB 668.6 kB/s eta 0:00:59\n",
      "     -------------------------------- --- 361.8/400.7 MB 668.6 kB/s eta 0:00:59\n",
      "     -------------------------------- --- 362.0/400.7 MB 675.0 kB/s eta 0:00:58\n",
      "     -------------------------------- --- 362.0/400.7 MB 675.0 kB/s eta 0:00:58\n",
      "     -------------------------------- --- 362.3/400.7 MB 673.9 kB/s eta 0:00:57\n",
      "     -------------------------------- --- 362.3/400.7 MB 673.9 kB/s eta 0:00:57\n",
      "     -------------------------------- --- 362.3/400.7 MB 673.9 kB/s eta 0:00:57\n",
      "     -------------------------------- --- 362.5/400.7 MB 672.5 kB/s eta 0:00:57\n",
      "     -------------------------------- --- 362.5/400.7 MB 672.5 kB/s eta 0:00:57\n",
      "     -------------------------------- --- 362.8/400.7 MB 683.3 kB/s eta 0:00:56\n",
      "     -------------------------------- --- 362.8/400.7 MB 683.3 kB/s eta 0:00:56\n",
      "     -------------------------------- --- 363.1/400.7 MB 683.9 kB/s eta 0:00:55\n",
      "     -------------------------------- --- 363.3/400.7 MB 685.5 kB/s eta 0:00:55\n",
      "     -------------------------------- --- 363.6/400.7 MB 696.8 kB/s eta 0:00:54\n",
      "     -------------------------------- --- 363.6/400.7 MB 696.8 kB/s eta 0:00:54\n",
      "     -------------------------------- --- 363.9/400.7 MB 695.0 kB/s eta 0:00:53\n",
      "     -------------------------------- --- 363.9/400.7 MB 695.0 kB/s eta 0:00:53\n",
      "     -------------------------------- --- 364.1/400.7 MB 698.7 kB/s eta 0:00:53\n",
      "     -------------------------------- --- 364.4/400.7 MB 700.5 kB/s eta 0:00:52\n",
      "     -------------------------------- --- 364.6/400.7 MB 706.8 kB/s eta 0:00:51\n",
      "     -------------------------------- --- 364.6/400.7 MB 706.8 kB/s eta 0:00:51\n",
      "     -------------------------------- --- 364.9/400.7 MB 707.1 kB/s eta 0:00:51\n",
      "     -------------------------------- --- 365.2/400.7 MB 709.3 kB/s eta 0:00:51\n",
      "     -------------------------------- --- 365.4/400.7 MB 715.6 kB/s eta 0:00:50\n",
      "     -------------------------------- --- 365.7/400.7 MB 719.9 kB/s eta 0:00:49\n",
      "     -------------------------------- --- 365.7/400.7 MB 719.9 kB/s eta 0:00:49\n",
      "     -------------------------------- --- 366.0/400.7 MB 725.2 kB/s eta 0:00:48\n",
      "     -------------------------------- --- 366.2/400.7 MB 726.4 kB/s eta 0:00:48\n",
      "     -------------------------------- --- 366.2/400.7 MB 726.4 kB/s eta 0:00:48\n",
      "     -------------------------------- --- 366.5/400.7 MB 727.5 kB/s eta 0:00:47\n",
      "     -------------------------------- --- 366.5/400.7 MB 727.5 kB/s eta 0:00:47\n",
      "     -------------------------------- --- 366.7/400.7 MB 729.8 kB/s eta 0:00:47\n",
      "     -------------------------------- --- 366.7/400.7 MB 729.8 kB/s eta 0:00:47\n",
      "     -------------------------------- --- 367.0/400.7 MB 738.0 kB/s eta 0:00:46\n",
      "     -------------------------------- --- 367.3/400.7 MB 740.2 kB/s eta 0:00:46\n",
      "     -------------------------------- --- 367.3/400.7 MB 740.2 kB/s eta 0:00:46\n",
      "     --------------------------------- -- 367.5/400.7 MB 753.2 kB/s eta 0:00:44\n",
      "     --------------------------------- -- 367.5/400.7 MB 753.2 kB/s eta 0:00:44\n",
      "     --------------------------------- -- 367.8/400.7 MB 751.7 kB/s eta 0:00:44\n",
      "     --------------------------------- -- 368.1/400.7 MB 754.6 kB/s eta 0:00:44\n",
      "     --------------------------------- -- 368.3/400.7 MB 757.0 kB/s eta 0:00:43\n",
      "     --------------------------------- -- 368.3/400.7 MB 757.0 kB/s eta 0:00:43\n",
      "     --------------------------------- -- 368.6/400.7 MB 756.6 kB/s eta 0:00:43\n",
      "     --------------------------------- -- 368.8/400.7 MB 761.0 kB/s eta 0:00:42\n",
      "     --------------------------------- -- 368.8/400.7 MB 761.0 kB/s eta 0:00:42\n",
      "     --------------------------------- -- 369.1/400.7 MB 759.8 kB/s eta 0:00:42\n",
      "     --------------------------------- -- 369.4/400.7 MB 763.4 kB/s eta 0:00:41\n",
      "     --------------------------------- -- 369.6/400.7 MB 764.2 kB/s eta 0:00:41\n",
      "     --------------------------------- -- 369.9/400.7 MB 767.8 kB/s eta 0:00:41\n",
      "     --------------------------------- -- 370.1/400.7 MB 771.5 kB/s eta 0:00:40\n",
      "     --------------------------------- -- 370.1/400.7 MB 771.5 kB/s eta 0:00:40\n",
      "     --------------------------------- -- 370.4/400.7 MB 771.4 kB/s eta 0:00:40\n",
      "     --------------------------------- -- 370.7/400.7 MB 772.2 kB/s eta 0:00:39\n",
      "     --------------------------------- -- 370.9/400.7 MB 775.8 kB/s eta 0:00:39\n",
      "     --------------------------------- -- 370.9/400.7 MB 775.8 kB/s eta 0:00:39\n",
      "     --------------------------------- -- 371.2/400.7 MB 777.5 kB/s eta 0:00:38\n",
      "     --------------------------------- -- 371.2/400.7 MB 777.5 kB/s eta 0:00:38\n",
      "     --------------------------------- -- 371.2/400.7 MB 777.5 kB/s eta 0:00:38\n",
      "     --------------------------------- -- 371.5/400.7 MB 781.2 kB/s eta 0:00:38\n",
      "     --------------------------------- -- 371.7/400.7 MB 783.0 kB/s eta 0:00:37\n",
      "     --------------------------------- -- 372.0/400.7 MB 792.1 kB/s eta 0:00:37\n",
      "     --------------------------------- -- 372.0/400.7 MB 792.1 kB/s eta 0:00:37\n",
      "     --------------------------------- -- 372.2/400.7 MB 788.5 kB/s eta 0:00:37\n",
      "     --------------------------------- -- 372.5/400.7 MB 790.1 kB/s eta 0:00:36\n",
      "     --------------------------------- -- 372.5/400.7 MB 790.1 kB/s eta 0:00:36\n",
      "     --------------------------------- -- 372.8/400.7 MB 788.5 kB/s eta 0:00:36\n",
      "     --------------------------------- -- 373.0/400.7 MB 792.6 kB/s eta 0:00:35\n",
      "     --------------------------------- -- 373.0/400.7 MB 792.6 kB/s eta 0:00:35\n",
      "     --------------------------------- -- 373.3/400.7 MB 795.2 kB/s eta 0:00:35\n",
      "     --------------------------------- -- 373.6/400.7 MB 796.0 kB/s eta 0:00:35\n",
      "     --------------------------------- -- 373.8/400.7 MB 797.7 kB/s eta 0:00:34\n",
      "     --------------------------------- -- 373.8/400.7 MB 797.7 kB/s eta 0:00:34\n",
      "     --------------------------------- -- 374.1/400.7 MB 797.2 kB/s eta 0:00:34\n",
      "     --------------------------------- -- 374.3/400.7 MB 801.4 kB/s eta 0:00:33\n",
      "     --------------------------------- -- 374.3/400.7 MB 801.4 kB/s eta 0:00:33\n",
      "     --------------------------------- -- 374.6/400.7 MB 802.3 kB/s eta 0:00:33\n",
      "     --------------------------------- -- 374.9/400.7 MB 804.8 kB/s eta 0:00:33\n",
      "     --------------------------------- -- 375.1/400.7 MB 809.4 kB/s eta 0:00:32\n",
      "     --------------------------------- -- 375.1/400.7 MB 809.4 kB/s eta 0:00:32\n",
      "     --------------------------------- -- 375.4/400.7 MB 810.2 kB/s eta 0:00:32\n",
      "     --------------------------------- -- 375.7/400.7 MB 816.7 kB/s eta 0:00:31\n",
      "     --------------------------------- -- 375.9/400.7 MB 819.5 kB/s eta 0:00:31\n",
      "     --------------------------------- -- 376.2/400.7 MB 821.4 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 376.2/400.7 MB 821.4 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 376.2/400.7 MB 821.4 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 376.4/400.7 MB 822.5 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 376.4/400.7 MB 822.5 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 376.7/400.7 MB 822.6 kB/s eta 0:00:30\n",
      "     --------------------------------- -- 377.0/400.7 MB 822.7 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.0/400.7 MB 822.7 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.2/400.7 MB 821.2 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.2/400.7 MB 821.2 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.2/400.7 MB 821.2 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.5/400.7 MB 816.5 kB/s eta 0:00:29\n",
      "     --------------------------------- -- 377.7/400.7 MB 819.0 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 377.7/400.7 MB 819.0 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 377.7/400.7 MB 819.0 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.0/400.7 MB 811.5 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.0/400.7 MB 811.5 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.0/400.7 MB 811.5 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.0/400.7 MB 811.5 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.3/400.7 MB 801.0 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.3/400.7 MB 801.0 kB/s eta 0:00:28\n",
      "     --------------------------------- -- 378.3/400.7 MB 801.0 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 378.5/400.7 MB 789.3 kB/s eta 0:00:29\n",
      "     ---------------------------------- - 378.8/400.7 MB 789.7 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 378.8/400.7 MB 789.7 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 378.8/400.7 MB 789.7 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 379.1/400.7 MB 787.2 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 379.3/400.7 MB 788.5 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 379.3/400.7 MB 788.5 kB/s eta 0:00:28\n",
      "     ---------------------------------- - 379.6/400.7 MB 786.4 kB/s eta 0:00:27\n",
      "     ---------------------------------- - 379.6/400.7 MB 786.4 kB/s eta 0:00:27\n",
      "     ---------------------------------- - 379.8/400.7 MB 786.3 kB/s eta 0:00:27\n",
      "     ---------------------------------- - 380.1/400.7 MB 789.3 kB/s eta 0:00:27\n",
      "     ---------------------------------- - 380.4/400.7 MB 791.0 kB/s eta 0:00:26\n",
      "     ---------------------------------- - 380.6/400.7 MB 791.0 kB/s eta 0:00:26\n",
      "     ---------------------------------- - 380.6/400.7 MB 791.0 kB/s eta 0:00:26\n",
      "     ---------------------------------- - 380.9/400.7 MB 791.4 kB/s eta 0:00:25\n",
      "     ---------------------------------- - 380.9/400.7 MB 791.4 kB/s eta 0:00:25\n",
      "     ---------------------------------- - 381.2/400.7 MB 788.9 kB/s eta 0:00:25\n",
      "     ---------------------------------- - 381.2/400.7 MB 788.9 kB/s eta 0:00:25\n",
      "     ---------------------------------- - 381.4/400.7 MB 790.5 kB/s eta 0:00:25\n",
      "     ---------------------------------- - 381.7/400.7 MB 791.4 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 381.7/400.7 MB 791.4 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 381.7/400.7 MB 791.4 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 381.9/400.7 MB 782.6 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 381.9/400.7 MB 782.6 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.2/400.7 MB 779.7 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.2/400.7 MB 779.7 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.2/400.7 MB 779.7 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.2/400.7 MB 779.7 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.5/400.7 MB 766.2 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.5/400.7 MB 766.2 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 382.7/400.7 MB 765.8 kB/s eta 0:00:24\n",
      "     ---------------------------------- - 383.0/400.7 MB 769.0 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 383.0/400.7 MB 769.0 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 383.3/400.7 MB 763.4 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 383.5/400.7 MB 762.2 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 383.8/400.7 MB 760.6 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 383.8/400.7 MB 760.6 kB/s eta 0:00:23\n",
      "     ---------------------------------- - 384.0/400.7 MB 762.6 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.0/400.7 MB 762.6 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.3/400.7 MB 757.8 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.3/400.7 MB 757.8 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.3/400.7 MB 757.8 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.3/400.7 MB 757.8 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.6/400.7 MB 752.5 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.6/400.7 MB 752.5 kB/s eta 0:00:22\n",
      "     ---------------------------------- - 384.8/400.7 MB 756.1 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 384.8/400.7 MB 756.1 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.1/400.7 MB 755.4 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.1/400.7 MB 755.4 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.4/400.7 MB 753.0 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.4/400.7 MB 753.0 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.6/400.7 MB 751.5 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.6/400.7 MB 751.5 kB/s eta 0:00:21\n",
      "     ---------------------------------- - 385.9/400.7 MB 745.8 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 385.9/400.7 MB 745.8 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 386.1/400.7 MB 744.7 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 386.1/400.7 MB 744.7 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 386.4/400.7 MB 739.0 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 386.4/400.7 MB 739.0 kB/s eta 0:00:20\n",
      "     ---------------------------------- - 386.7/400.7 MB 737.4 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 386.7/400.7 MB 737.4 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 386.9/400.7 MB 735.9 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 386.9/400.7 MB 735.9 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.2/400.7 MB 725.3 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.2/400.7 MB 725.3 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.2/400.7 MB 725.3 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.4/400.7 MB 715.6 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.4/400.7 MB 715.6 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.4/400.7 MB 715.6 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.7/400.7 MB 709.6 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 387.7/400.7 MB 709.6 kB/s eta 0:00:19\n",
      "     ---------------------------------- - 388.0/400.7 MB 709.4 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.0/400.7 MB 709.4 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.2/400.7 MB 707.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.2/400.7 MB 707.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.2/400.7 MB 707.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.2/400.7 MB 707.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.5/400.7 MB 695.0 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.5/400.7 MB 695.0 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.5/400.7 MB 695.0 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.5/400.7 MB 695.0 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.5/400.7 MB 695.0 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.8/400.7 MB 672.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.8/400.7 MB 672.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 388.8/400.7 MB 672.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 389.0/400.7 MB 665.8 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 389.3/400.7 MB 657.4 kB/s eta 0:00:18\n",
      "     ---------------------------------- - 389.3/400.7 MB 657.4 kB/s eta 0:00:18\n",
      "     -----------------------------------  389.5/400.7 MB 651.4 kB/s eta 0:00:18\n",
      "     -----------------------------------  389.5/400.7 MB 651.4 kB/s eta 0:00:18\n",
      "     -----------------------------------  389.5/400.7 MB 651.4 kB/s eta 0:00:18\n",
      "     -----------------------------------  389.8/400.7 MB 648.3 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.1/400.7 MB 647.3 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.1/400.7 MB 647.3 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.1/400.7 MB 647.3 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.3/400.7 MB 641.2 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.3/400.7 MB 641.2 kB/s eta 0:00:17\n",
      "     -----------------------------------  390.6/400.7 MB 640.9 kB/s eta 0:00:16\n",
      "     -----------------------------------  390.9/400.7 MB 641.6 kB/s eta 0:00:16\n",
      "     -----------------------------------  391.1/400.7 MB 640.6 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.1/400.7 MB 640.6 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.4/400.7 MB 642.2 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.4/400.7 MB 642.2 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.4/400.7 MB 642.2 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.6/400.7 MB 634.1 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.6/400.7 MB 634.1 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.6/400.7 MB 634.1 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.9/400.7 MB 624.3 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.9/400.7 MB 624.3 kB/s eta 0:00:15\n",
      "     -----------------------------------  391.9/400.7 MB 624.3 kB/s eta 0:00:15\n",
      "     -----------------------------------  392.2/400.7 MB 612.0 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.2/400.7 MB 612.0 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.2/400.7 MB 612.0 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.4/400.7 MB 605.1 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.4/400.7 MB 605.1 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.7/400.7 MB 597.6 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.7/400.7 MB 597.6 kB/s eta 0:00:14\n",
      "     -----------------------------------  392.7/400.7 MB 597.6 kB/s eta 0:00:14\n",
      "     -----------------------------------  393.0/400.7 MB 590.4 kB/s eta 0:00:14\n",
      "     -----------------------------------  393.0/400.7 MB 590.4 kB/s eta 0:00:14\n",
      "     -----------------------------------  393.2/400.7 MB 580.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.2/400.7 MB 580.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.2/400.7 MB 580.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.5/400.7 MB 578.5 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.5/400.7 MB 578.5 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.5/400.7 MB 578.5 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.7/400.7 MB 571.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.7/400.7 MB 571.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  393.7/400.7 MB 571.0 kB/s eta 0:00:13\n",
      "     -----------------------------------  394.0/400.7 MB 564.8 kB/s eta 0:00:12\n",
      "     -----------------------------------  394.0/400.7 MB 564.8 kB/s eta 0:00:12\n",
      "     -----------------------------------  394.0/400.7 MB 564.8 kB/s eta 0:00:12\n",
      "     -----------------------------------  394.3/400.7 MB 562.2 kB/s eta 0:00:12\n",
      "     -----------------------------------  394.3/400.7 MB 562.2 kB/s eta 0:00:12\n",
      "     -----------------------------------  394.5/400.7 MB 564.3 kB/s eta 0:00:11\n",
      "     -----------------------------------  394.5/400.7 MB 564.3 kB/s eta 0:00:11\n",
      "     -----------------------------------  394.5/400.7 MB 564.3 kB/s eta 0:00:11\n",
      "     -----------------------------------  394.8/400.7 MB 563.0 kB/s eta 0:00:11\n",
      "     -----------------------------------  394.8/400.7 MB 563.0 kB/s eta 0:00:11\n",
      "     -----------------------------------  394.8/400.7 MB 563.0 kB/s eta 0:00:11\n",
      "     -----------------------------------  395.1/400.7 MB 562.8 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.1/400.7 MB 562.8 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.1/400.7 MB 562.8 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.1/400.7 MB 562.8 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.3/400.7 MB 560.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.3/400.7 MB 560.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.3/400.7 MB 560.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.3/400.7 MB 560.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.6/400.7 MB 553.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.8/400.7 MB 525.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.8/400.7 MB 525.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  395.8/400.7 MB 525.1 kB/s eta 0:00:10\n",
      "     -----------------------------------  396.1/400.7 MB 508.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.1/400.7 MB 508.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.1/400.7 MB 508.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.4/400.7 MB 501.5 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.4/400.7 MB 501.5 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.4/400.7 MB 501.5 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.4/400.7 MB 501.5 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.6/400.7 MB 493.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.6/400.7 MB 493.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.6/400.7 MB 493.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.6/400.7 MB 493.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.6/400.7 MB 493.4 kB/s eta 0:00:09\n",
      "     -----------------------------------  396.9/400.7 MB 488.5 kB/s eta 0:00:08\n",
      "     -----------------------------------  396.9/400.7 MB 488.5 kB/s eta 0:00:08\n",
      "     -----------------------------------  396.9/400.7 MB 488.5 kB/s eta 0:00:08\n",
      "     -----------------------------------  396.9/400.7 MB 488.5 kB/s eta 0:00:08\n",
      "     -----------------------------------  396.9/400.7 MB 488.5 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.1/400.7 MB 472.9 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.4/400.7 MB 443.6 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.7/400.7 MB 420.7 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.7/400.7 MB 420.7 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.7/400.7 MB 420.7 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.7/400.7 MB 420.7 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.7/400.7 MB 420.7 kB/s eta 0:00:08\n",
      "     -----------------------------------  397.9/400.7 MB 406.0 kB/s eta 0:00:07\n",
      "     -----------------------------------  397.9/400.7 MB 406.0 kB/s eta 0:00:07\n",
      "     -----------------------------------  397.9/400.7 MB 406.0 kB/s eta 0:00:07\n",
      "     -----------------------------------  398.2/400.7 MB 404.7 kB/s eta 0:00:07\n",
      "     -----------------------------------  398.2/400.7 MB 404.7 kB/s eta 0:00:07\n",
      "     -----------------------------------  398.2/400.7 MB 404.7 kB/s eta 0:00:07\n",
      "     -----------------------------------  398.2/400.7 MB 404.7 kB/s eta 0:00:07\n",
      "     -----------------------------------  398.5/400.7 MB 395.3 kB/s eta 0:00:06\n",
      "     -----------------------------------  398.5/400.7 MB 395.3 kB/s eta 0:00:06\n",
      "     -----------------------------------  398.5/400.7 MB 395.3 kB/s eta 0:00:06\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  398.7/400.7 MB 388.7 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.0/400.7 MB 371.1 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.0/400.7 MB 371.1 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.0/400.7 MB 371.1 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.0/400.7 MB 371.1 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.0/400.7 MB 371.1 kB/s eta 0:00:05\n",
      "     -----------------------------------  399.2/400.7 MB 365.9 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.2/400.7 MB 365.9 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.2/400.7 MB 365.9 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.5/400.7 MB 369.2 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.5/400.7 MB 369.2 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.5/400.7 MB 369.2 kB/s eta 0:00:04\n",
      "     -----------------------------------  399.8/400.7 MB 368.1 kB/s eta 0:00:03\n",
      "     -----------------------------------  399.8/400.7 MB 368.1 kB/s eta 0:00:03\n",
      "     -----------------------------------  399.8/400.7 MB 368.1 kB/s eta 0:00:03\n",
      "     -----------------------------------  399.8/400.7 MB 368.1 kB/s eta 0:00:03\n",
      "     -----------------------------------  399.8/400.7 MB 368.1 kB/s eta 0:00:03\n",
      "     -----------------------------------  400.0/400.7 MB 360.1 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.0/400.7 MB 360.1 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.0/400.7 MB 360.1 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.0/400.7 MB 360.1 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.3/400.7 MB 351.7 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.3/400.7 MB 351.7 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.3/400.7 MB 351.7 kB/s eta 0:00:02\n",
      "     -----------------------------------  400.6/400.7 MB 349.5 kB/s eta 0:00:01\n",
      "     -----------------------------------  400.6/400.7 MB 349.5 kB/s eta 0:00:01\n",
      "     -----------------------------------  400.6/400.7 MB 349.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 400.7/400.7 MB 339.7 kB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\arink\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg --timeout 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629a77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "014ca9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= nlp(\"arin kumar is greate and he one of the best code in wordl\")\n",
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef80243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector']=df['Text'].apply(lambda x:nlp(x).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53b51407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.103623025, 0.17802684, -0.11873861, -0.034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0063406364, 0.16712041, -0.06661373, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.122753024, 0.17192385, -0.024732638, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.027337318, 0.12501417, -0.0073965387, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.032708026, 0.093958504, -0.03287002, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num  \\\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0   \n",
       "1  U.S. conservative leader optimistic of common ...  Real          1   \n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1   \n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0   \n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.103623025, 0.17802684, -0.11873861, -0.034...  \n",
       "1  [-0.0063406364, 0.16712041, -0.06661373, 0.017...  \n",
       "2  [-0.122753024, 0.17192385, -0.024732638, -0.06...  \n",
       "3  [-0.027337318, 0.12501417, -0.0073965387, -0.0...  \n",
       "4  [-0.032708026, 0.093958504, -0.03287002, -0.00...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.vector.values, df.label_num, test_size=0.2, random_state=2022\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2804d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "X_train_2d=np.stack(X_train)\n",
    "X_test_2d=np.stack(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea0dbe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02370346,  0.14819953, -0.05906299, ..., -0.06582212,\n",
       "        -0.05378761,  0.08668853],\n",
       "       [-0.01595326,  0.15394837, -0.10800642, ..., -0.03003666,\n",
       "        -0.04334445,  0.03076661],\n",
       "       [-0.04449651,  0.11169833, -0.04756551, ..., -0.10499363,\n",
       "        -0.00837316,  0.06351685],\n",
       "       ...,\n",
       "       [ 0.02167883,  0.12635042, -0.01003216, ..., -0.08063941,\n",
       "        -0.06881595,  0.04882506],\n",
       "       [-0.07091133,  0.08315557, -0.06580248, ..., -0.06301989,\n",
       "         0.02095402,  0.09888683],\n",
       "       [-0.08993341,  0.14425951, -0.14141384, ..., -0.03444797,\n",
       "         0.02387965,  0.06281336]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e488f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scale_train_embed=scaler.fit_transform(X_train_2d)\n",
    "scale_test_embed=scaler.fit_transform(X_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c21c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf=MultinomialNB()\n",
    "\n",
    "clf.fit(scale_train_embed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43de3d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      1024\n",
      "           1       0.97      0.93      0.95       956\n",
      "\n",
      "    accuracy                           0.95      1980\n",
      "   macro avg       0.95      0.95      0.95      1980\n",
      "weighted avg       0.95      0.95      0.95      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict=clf.predict(scale_test_embed)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a10a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1024\n",
      "           1       0.97      0.99      0.98       956\n",
      "\n",
      "    accuracy                           0.98      1980\n",
      "   macro avg       0.98      0.98      0.98      1980\n",
      "weighted avg       0.98      0.98      0.98      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=5,metric='euclidean')\n",
    "clf.fit(X_train_2d,y_train)\n",
    "y_predt=clf.predict(X_test_2d)\n",
    "\n",
    "print(classification_report(y_test,y_predt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316e8e6",
   "metadata": {},
   "source": [
    "## Overview of Gensim and Word Embeddings\n",
    "\n",
    "- **Gensim** is a Python NLP library primarily used for topic modeling, similar to SpaCy, but with a convenient API for word vectors, making it a good choice for working with word embeddings   .\n",
    "- Word embeddings in Gensim can be loaded using the `API.load()` method, where you specify the dataset or model type, such as the Google News pretrained Word2Vec model   .\n",
    "- The **Google News Word2Vec model** is large (~1.6 GB), trained on 100 billion words, and contains about 3 million word vectors, making it suitable for large-scale NLP tasks   .\n",
    "- Smaller models are available too, such as those trained on Twitter data (~199 MB, 1.1 million vectors) using different algorithms like GloVe or Word2Vec, useful for lightweight or domain-specific analysis   .\n",
    "\n",
    "## Understanding Word Vector Similarity in Gensim\n",
    "\n",
    "- The `similarity()` function measures the similarity between two words based on their contexts, not strict synonyms; for example, \"great\" and \"good\" have a similarity score around 0.7   .\n",
    "- Similarity reflects how often words appear in similar contexts, not semantic equivalence, which explains why antonyms like \"good\" and \"bad\" can have high similarity if they share surrounding words in training corpora   .\n",
    "- This contextual similarity is a key feature of word embeddings, distinguishing them from traditional models like TF-IDF or bag-of-words that lack semantic understanding   .\n",
    "\n",
    "## Practical Examples of Word Embedding Use\n",
    "\n",
    "- Words like \"dog,\" \"puppy,\" and \"Golden Retriever\" cluster closely because they appear in similar contexts; similarly, \"cat\" and \"dog\" are similar due to shared usage contexts in training data  .\n",
    "- Vector arithmetic on embeddings can reveal meaningful relationships, e.g., $$\\text{France} - \\text{Paris} + \\text{Berlin} = \\text{Germany}$$, illustrating how embeddings capture semantic structure beyond word co-occurrence   .\n",
    "- Gensim's `most_similar()` method supports positive and negative examples to perform such vector arithmetic and returns results ranked by similarity, e.g., Queen is the result of $$\\text{King} - \\text{Man} + \\text{Woman}$$   .\n",
    "\n",
    "## Additional Gensim APIs for Semantic Tasks\n",
    "\n",
    "- The `doesnt_match()` method identifies the odd word out in a list based on semantic fit, e.g., in a list of company names with \"cat\" included, \"cat\" would be identified as not matching  .\n",
    "- This demonstrates Gensim's ability to understand language context and semantic categories beyond simple word matching  .\n",
    "\n",
    "## Comparing Models and Their Contextual Influence\n",
    "\n",
    "- Loading different pretrained models (e.g., Twitter 25 model trained on 2 billion tweets) can yield different similarity results due to domain-specific training data influencing word contexts   .\n",
    "- For example, similarity results for \"good\" differ significantly between Google News and Twitter models, reflecting the different language use in news articles vs. tweets  .\n",
    "- Semantic tasks such as `most_similar()` and `doesnt_match()` work across models but results depend on the underlying training corpus and embedding technique (Word2Vec vs. GloVe)   .\n",
    "\n",
    "## Important Concepts and Clarifications\n",
    "\n",
    "> **Similarity in word embeddings refers to contextual similarity, not synonymy or antonymy.** Words appearing in similar contexts have high similarity scores, regardless of their dictionary meanings.  \n",
    "   \n",
    "\n",
    "- Word embeddings are trained using self-supervised learning on large text corpora, generating training samples automatically from context windows without labeled data  .\n",
    "- Using pretrained embeddings in NLP models helps capture semantic relationships and generalize better to unseen but semantically related words, unlike traditional sparse vector models  .\n",
    "- Multiple libraries (Gensim, SpaCy, PyTorch) support loading pretrained embeddings, but Gensim offers a convenient API for vector operations and semantic queries   .\n",
    "- Different embedding algorithms (Word2Vec, GloVe) and datasets (Google News, Twitter) affect the size, vocabulary, and quality of the embeddings, so choosing the right model depends on the application domain    .\n",
    "\n",
    "---\n",
    "\n",
    "| Concept                  | Description                                                                                      | Example/Note                                         |\n",
    "|--------------------------|------------------------------------------------------------------------------------------------|-----------------------------------------------------|\n",
    "| Word2Vec                 | An algorithm for training word embeddings based on predicting context words                      | Google News Word2Vec model (3M vectors, 1.6 GB)     |\n",
    "| GloVe                    | Another embedding algorithm based on matrix factorization of word co-occurrence statistics      | Twitter GloVe model (smaller, domain-specific)       |\n",
    "| Similarity               | Measures how often two words share similar contexts, not dictionary meaning                     | \"good\" vs. \"bad\" similarity ~0.7 due to similar context usage |\n",
    "| Vector Arithmetic        | Adding and subtracting word vectors to find semantic relations                                  | $$\\text{King} - \\text{Man} + \\text{Woman} = \\text{Queen}$$ |\n",
    "| `most_similar()`         | Finds words closest in vector space to given words or vector expressions                        | Find words similar to \"good\" or \"France - Paris + Berlin\" |\n",
    "| `doesnt_match()`         | Identifies the word that semantically doesn't fit in a list                                    | In [\"dog\", \"cat\", \"lion\", \"Microsoft\"], \"Microsoft\" is the odd one out |\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------] 0.1% 2.4/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv=api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4815c",
   "metadata": {},
   "source": [
    "## Overview of News Classification Task Using Gensim Word Vectors\n",
    "- The goal is to classify news articles into \"fake\" or \"real\" categories using a dataset sourced from Kaggle, containing approximately 9900 samples with balanced classes, avoiding class imbalance issues.   \n",
    "- Gensim's pre-trained Word2Vec model on Google News is used to obtain 300-dimensional word embeddings, which can be accessed like a Python dictionary to get vectors for individual words.   \n",
    "\n",
    "## Data Preparation and Label Encoding\n",
    "- The dataset is loaded into a pandas DataFrame, and the target labels (\"fake\", \"real\") are converted into numeric format: fake = 0, real = 1, enabling machine learning compatibility.   \n",
    "\n",
    "## Text Preprocessing and Vectorization\n",
    "- A combined preprocessing and vectorization function is created to:\n",
    "  - Remove stopwords and punctuation.\n",
    "  - Lemmatize tokens to their base forms.\n",
    "  - Convert processed tokens into a sentence embedding by averaging their word vectors.    \n",
    "- Example: The sentence embedding is computed by averaging vectors of individual words (e.g., vectors for \"worry\" and \"understand\" averaged to form a 300-dimensional sentence vector). This is a common NLP practice to create sentence-level representations from word embeddings.   \n",
    "\n",
    "## Using Gensim's get_mean_vector Method\n",
    "- Gensim's `get_mean_vector` method simplifies averaging word vectors from a list of tokens to produce the sentence embedding, with an option for normalization to improve machine learning performance.    \n",
    "- Normalization helps by scaling the vectors consistently, which typically enhances model results.   \n",
    "\n",
    "## Applying Vectorization to the Dataset\n",
    "- The preprocessing and vectorization function is applied to each news article, creating a new column in the DataFrame with the 300-dimensional sentence vectors. This operation is computationally intensive and may take significant time to complete.   \n",
    "\n",
    "## Preparing Data for Model Training\n",
    "- Data is split into training (80%) and testing (20%) sets.\n",
    "- Sentence vectors are converted from arrays of arrays into a native 2D NumPy array using `np.stack` to allow compatibility with machine learning classifiers.   \n",
    "\n",
    "## Model Training and Evaluation\n",
    "- A Gradient Boosting Classifier is trained on the vectorized news data, a method frequently used in the author's machine learning tutorials.  \n",
    "- The model achieves approximately 98% accuracy, with high precision, recall, and F1 scores in both classes (fake and real), indicating strong classification performance.  \n",
    "\n",
    "## Prediction and Confusion Matrix Interpretation\n",
    "- The model is tested on new news samples, correctly predicting their categories.\n",
    "- The confusion matrix shows most predictions on the diagonal (correct classifications):\n",
    "  - 965 real news correctly predicted as real.\n",
    "  - 972 fake news correctly predicted as fake.\n",
    "- Misclassifications include 28 fake news predicted as real, and 15 real news predicted as fake, indicating some classification errors but overall strong performance.   \n",
    "\n",
    "> **üí° Key Insight:** Averaging word embeddings to create sentence vectors is an effective and widely used technique for text classification tasks, especially when combined with powerful classifiers like Gradient Boosting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv(r\"C:\\Users\\arink\\Downloads\\fake_and_real_news.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f60997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_num\"]=df.label.map({'Fake':1,'Real':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd18613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def preprocessing_and_vectorizing(text):\n",
    "    doc=nlp(text)\n",
    "    filter_token=[]\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_stop:\n",
    "            continue\n",
    "        filter_token.append(token.lemma_)\n",
    "\n",
    "    return filter_token    \n",
    "\n",
    "preprocessing_and_vectorizing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector']=df['text'].apply(lambda text:preprocessing_and_vectorizing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2717b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.vector.values, \n",
    "    df.label_num, \n",
    "    test_size=0.2, \n",
    "    random_state=2022,\n",
    "    stratify=df.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train before reshaping: \", X_train.shape)\n",
    "print(\"Shape of X_test before reshaping: \", X_test.shape)\n",
    "\n",
    "\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "\n",
    "print(\"Shape of X_train after reshaping: \", X_train_2d.shape)\n",
    "print(\"Shape of X_test after reshaping: \", X_test_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ba023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = [\n",
    "    \"Michigan governor denies misleading U.S. House on Flint water (Reuters) - Michigan Governor Rick Snyder denied Thursday that he had misled a U.S. House of Representatives committee last year over testimony on Flint√¢‚Ç¨‚Ñ¢s water crisis after lawmakers asked if his testimony had been contradicted by a witness in a court hearing. The House Oversight and Government Reform Committee wrote Snyder earlier Thursday asking him about published reports that one of his aides, Harvey Hollins, testified in a court hearing last week in Michigan that he had notified Snyder of an outbreak of Legionnaires√¢‚Ç¨‚Ñ¢ disease linked to the Flint water crisis in December 2015, rather than 2016 as Snyder had testified. √¢‚Ç¨≈ìMy testimony was truthful and I stand by it,√¢‚Ç¨¬ù Snyder told the committee in a letter, adding that his office has provided tens of thousands of pages of records to the committee and would continue to cooperate fully.  Last week, prosecutors in Michigan said Dr. Eden Wells, the state√¢‚Ç¨‚Ñ¢s chief medical executive who already faced lesser charges, would become the sixth current or former official to face involuntary manslaughter charges in connection with the crisis. The charges stem from more than 80 cases of Legionnaires√¢‚Ç¨‚Ñ¢ disease and at least 12 deaths that were believed to be linked to the water in Flint after the city switched its source from Lake Huron to the Flint River in April 2014. Wells was among six current and former Michigan and Flint officials charged in June. The other five, including Michigan Health and Human Services Director Nick Lyon, were charged at the time with involuntary manslaughter\",\n",
    "    \" WATCH: Fox News Host Loses Her Sh*t, Says Investigating Russia For Hacking Our Election Is Unpatriotic This woman is insane.In an incredibly disrespectful rant against President Obama and anyone else who supports investigating Russian interference in our election, Fox News host Jeanine Pirro said that anybody who is against Donald Trump is anti-American. Look, it s time to take sides,  she began.\",\n",
    "    \" Sarah Palin Celebrates After White Man Who Pulled Gun On Black Protesters Goes Unpunished (VIDEO) Sarah Palin, one of the nigh-innumerable  deplorables  in Donald Trump s  basket,  almost outdid herself in terms of horribleness on Friday.\"\n",
    "]\n",
    "\n",
    "test_news_vectors = [preprocess_and_vectorize(n) for n in test_news]\n",
    "clf.predict(test_news_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4def19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35249c3",
   "metadata": {},
   "source": [
    "## Overview of Word Embedding Techniques\n",
    "\n",
    "- Word2Vec uses two main architectures for word embeddings: Continuous Bag of Words (CBOW) predicts a target word from its context, while Skip-gram predicts context words from a target word. Both train a neural network whose weights serve as word embeddings    .\n",
    "- Word2Vec treats the **word as the smallest unit** for training the neural network, which can cause problems with out-of-vocabulary (OOV) words if they do not appear in the training corpus  .\n",
    "\n",
    "## Introduction to FastText\n",
    "\n",
    "- FastText is similar to Word2Vec but differs by training on **character n-grams** instead of whole words, capturing subword information (e.g., for \"capable\" with n=3, n-grams include \"cap\", \"apa\", \"pab\", etc.)   .\n",
    "- This subword modeling enables FastText to handle OOV words effectively because it can infer embeddings from known character n-grams even if the full word was unseen during training  .\n",
    "- FastText is often the **first choice for training custom embeddings in specialized domains** due to its efficiency and ability to handle domain-specific vocabulary better than Word2Vec or BERT    .\n",
    "\n",
    "## FastText: Technique and Library\n",
    "\n",
    "- FastText is both a **technique and a library** developed by Facebook Research, providing pre-trained models and Python modules for easy use   .\n",
    "- Pre-trained FastText models are available for many languages and are trained on large corpora such as Wikipedia and Common Crawl, enabling them to capture general language properties   .\n",
    "\n",
    "## Using Pre-trained FastText Models\n",
    "\n",
    "- The FastText Python API allows loading pre-trained models and accessing word vectors, nearest neighbors, and analogies   .\n",
    "- Nearest neighbors in FastText embeddings reflect **contextual similarity, not synonyms or antonyms**; e.g., \"good\" and \"bad\" appear close because they occur in similar contexts   .\n",
    "- FastText vectors have a typical dimension of 300, and methods like `get_word_vector` and `get_analogies` demonstrate semantic relationships (e.g., Berlin:Germany :: Delhi:India)   .\n",
    "\n",
    "## Domain-Specific Training Example: Indian Food Recipes\n",
    "\n",
    "- Pre-trained general models may perform poorly on domain-specific terms (e.g., Indian food items like \"chutney\" or regional terms like \"saragua\") because such terms are rare or absent in general corpora   .\n",
    "- Training a FastText model on a specialized dataset (e.g., Indian food recipes) involves cleaning the text data (removing special characters, extra spaces, converting to lowercase) using regular expressions for better quality input   .\n",
    "- Exporting the cleaned recipe text into a plain text file with one recipe per line prepares the data for unsupervised FastText training   .\n",
    "- Training uses FastText‚Äôs `train_unsupervised` method, typically with skip-gram by default, which generates word embeddings tailored to the domain-specific language   .\n",
    "\n",
    "## Benefits of Custom FastText Models\n",
    "\n",
    "- Custom-trained FastText models provide **more meaningful nearest neighbor results** for domain-specific terms compared to general models, reflecting better understanding of the domain vocabulary and relationships    .\n",
    "- This approach helps improve NLP tasks in specialized fields by producing embeddings that capture relevant semantic nuances not present in general language models   .\n",
    "\n",
    "## FastText Model Hyperparameters and Further Exploration\n",
    "\n",
    "- FastText training supports tuning of hyperparameters such as model type (`cbow` or `skipgram`), embedding dimension, number of epochs, learning rate, and number of threads to optimize performance for specific datasets   .\n",
    "- Users are encouraged to experiment with these parameters to improve model quality when training on custom data  .\n",
    "- The FastText library website provides tutorials and resources for deeper exploration, including supervised tasks like text classification  .\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Key Insight:** FastText‚Äôs use of character n-grams significantly reduces the out-of-vocabulary problem inherent in word-level embeddings, making it highly suitable for custom domain-specific NLP applications.  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6bf0f",
   "metadata": {},
   "source": [
    "## Text Preprocessing Using Regular Expressions\n",
    "\n",
    "- To prepare text for fastText modeling, all punctuation and special characters must be removed, and text converted to lowercase without extra whitespace. This cleaning is done using regular expressions (regex)   .\n",
    "- In regex, `\\s` matches any whitespace character (spaces, tabs), and `\\w` matches any word character (letters, digits, underscore). Characters that are neither word characters nor whitespace can be targeted for removal or substitution   .\n",
    "- Using `re.sub()`, all special characters can be substituted with a space. For example, the pattern `[^\\w\\s]` matches any character that is not a word character or whitespace, enabling their removal to clean the text   .\n",
    "- To reduce multiple consecutive spaces to a single space, the pattern `\\s+` (one or more spaces) is replaced with a single space. The `strip()` method removes leading/trailing spaces, and `lower()` converts text to lowercase for normalization    .\n",
    "- This preprocessing step is encapsulated in a function that can be applied to an entire pandas DataFrame column using `map()` or `apply()`, enabling batch cleaning of text data   .\n",
    "\n",
    "## Preparing Data for fastText Training\n",
    "\n",
    "- After preprocessing, the dataset is split into training and testing sets, typically using an 80/20 split, resulting in two DataFrames with separate labeled samples for model training and evaluation   .\n",
    "- fastText‚Äôs supervised learning function, `train_supervised()`, requires input data in a specific text file format where each line contains a label and the corresponding item description. The training and test DataFrames are saved as CSV files without headers or indices to meet this format   .\n",
    "\n",
    "## Training and Evaluating the fastText Model\n",
    "\n",
    "- The `train_supervised()` method in fastText is used for text classification. Unlike previous use for word embeddings, here it combines embedding generation with classification training  .\n",
    "- Model evaluation is done on the test set using precision and recall metrics. Precision indicates the percentage of correct predictions out of all predictions made, with a typical example showing 96% precision, indicating strong model performance  .\n",
    "- The model‚Äôs prediction API is simple: given an input string, it predicts the most likely category label (e.g., Electronics, Books, Clothing) based on the learned embeddings and classification model  .\n",
    "\n",
    "## Additional Model Capabilities and Practical Advice\n",
    "\n",
    "- The trained fastText model retains word embeddings, allowing tasks like finding nearest neighbors to a word (similar words) using `model.get_nearest_neighbors()`. This can reveal semantically related terms learned during training  .\n",
    "- Practical learning requires active coding practice alongside video tutorials to reinforce understanding and skills in text classification with fastText   .\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Key Insight:** Preprocessing text by removing punctuation, normalizing case, and reducing whitespace is essential to prepare clean input data that improves fastText classification accuracy.     \n",
    "> **üí° Key Insight:** fastText integrates word embedding learning and supervised classification, enabling efficient and accurate text categorization from raw text data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50f9a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50425, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catogry</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     catogry                                        description\n",
       "0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
       "2  Household  SAF 'UV Textured Modern Art Print Framed' Pain..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv((r\"C:\\Users\\arink\\Downloads\\ecommerce_dataset.csv\"),names=[\"catogry\",\"description\"],header=None)\n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d6e95b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catogry\n",
       "__label__Household               19313\n",
       "__label__Books                   11820\n",
       "__label__Electronics             10621\n",
       "__label__Clothing_Accessories     8670\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.catogry.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd5d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50424, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1d1d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Household', 'Books', 'Clothing & Accessories', 'Electronics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.catogry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0bd2362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arink\\AppData\\Local\\Temp\\ipykernel_15788\\605014798.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data.catogry.replace(\"Clothing & Accessories\", \"Clothing_Accessories\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.catogry.replace(\"Clothing & Accessories\", \"Clothing_Accessories\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9113d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Household', 'Books', 'Clothing_Accessories', 'Electronics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.catogry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b352be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catogry</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              catogry                                        description\n",
       "0  __label__Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1  __label__Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
       "2  __label__Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
       "3  __label__Household  SAF Flower Print Framed Painting (Synthetic, 1...\n",
       "4  __label__Household  Incredible Gifts India Wooden Happy Birthday U..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['catogry'] = '__label__' + data['catogry'].astype(str)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c617ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catogry</th>\n",
       "      <th>description</th>\n",
       "      <th>catogry_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "      <td>__label__Household Paper Plane Design Framed W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "      <td>__label__Household SAF 'Floral' Framed Paintin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "      <td>__label__Household SAF 'UV Textured Modern Art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              catogry                                        description  \\\n",
       "0  __label__Household  Paper Plane Design Framed Wall Hanging Motivat...   \n",
       "1  __label__Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...   \n",
       "2  __label__Household  SAF 'UV Textured Modern Art Print Framed' Pain...   \n",
       "\n",
       "                                 catogry_description  \n",
       "0  __label__Household Paper Plane Design Framed W...  \n",
       "1  __label__Household SAF 'Floral' Framed Paintin...  \n",
       "2  __label__Household SAF 'UV Textured Modern Art...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['catogry_description'] = data['catogry'] + ' ' + data['description']\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df959fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"viki's bookcase bookshelf 3 shelf shelve white hi\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"  VIKI's | Bookcase/Bookshelf (3-Shelf/Shelve, White) | ? . hi\"\n",
    "text = re.sub(r'[^\\w\\s\\']',' ', text)\n",
    "text = re.sub(' +', ' ', text)\n",
    "text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s\\']',' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip().lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4dcce37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catogry</th>\n",
       "      <th>description</th>\n",
       "      <th>catogry_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "      <td>__label__household paper plane design framed w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "      <td>__label__household saf 'floral' framed paintin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "      <td>__label__household saf 'uv textured modern art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "      <td>__label__household saf flower print framed pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__Household</td>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "      <td>__label__household incredible gifts india wood...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              catogry                                        description  \\\n",
       "0  __label__Household  Paper Plane Design Framed Wall Hanging Motivat...   \n",
       "1  __label__Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...   \n",
       "2  __label__Household  SAF 'UV Textured Modern Art Print Framed' Pain...   \n",
       "3  __label__Household  SAF Flower Print Framed Painting (Synthetic, 1...   \n",
       "4  __label__Household  Incredible Gifts India Wooden Happy Birthday U...   \n",
       "\n",
       "                                 catogry_description  \n",
       "0  __label__household paper plane design framed w...  \n",
       "1  __label__household saf 'floral' framed paintin...  \n",
       "2  __label__household saf 'uv textured modern art...  \n",
       "3  __label__household saf flower print framed pai...  \n",
       "4  __label__household incredible gifts india wood...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['catogry_description'] = data['catogry_description'].map(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6589abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de64e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748cb4eb",
   "metadata": {},
   "source": [
    "## Types of Chatbots\n",
    "\n",
    "- **Flow-based (Rule-based) Chatbots:** These operate via fixed menu options or decision trees where users select predefined choices step-by-step, requiring no machine learning, only simple if-else programming. Examples include Verizon Wireless and PNC Bank chatbots that guide users through hierarchical options like plan management or payments    .\n",
    "- **NLP-based Flow Chatbots:** These understand free-form English queries within a specific domain, parsing user intent and responding accordingly. Examples include Capital One‚Äôs Eno chatbot, which understands varied phrasings about credit and transactions, and Domino‚Äôs chatbot, which processes natural language orders and validates inputs like addresses    .\n",
    "- **Open-ended Chatbots:** These support unrestricted, multi-topic conversations without fixed domains. ChatGPT exemplifies this, capable of handling diverse questions and changing topics fluidly, unlike rule-based bots with limited scope    .\n",
    "\n",
    "## Implementation Approaches\n",
    "\n",
    "| Aspect                    | Flow-based Chatbots                       | NLP-based Chatbots (e.g., using OpenAI API)              |\n",
    "|---------------------------|------------------------------------------|----------------------------------------------------------|\n",
    "| Setup and Configuration   | Relatively easy, no ML needed             | More technical knowledge required, involves ML components|\n",
    "| Natural Language Understanding (NLU) | Handled natively through rule-based logic | Requires custom intent/entity extraction and training    |\n",
    "| Training Data             | Not needed                               | Fine-tuning or custom training often necessary           |\n",
    "| Context Management        | Built-in context handling                 | Needs extensive customization                             |\n",
    "| Integration               | Easier integration with platforms like Slack | More complex integration, custom development needed      |\n",
    "| Cost                      | Relatively cheaper                       | Can be costly due to usage-based pricing                  |\n",
    "\n",
    "This comparison highlights why frameworks like Google Dialogflow remain relevant despite the rise of LLMs like ChatGPT     .\n",
    "\n",
    "## Frameworks and Tools for NLP Chatbots\n",
    "\n",
    "- Popular chatbot frameworks include Google Dialogflow, Rasa, IBM Watson Assistant, Amazon Lex, and Microsoft Azure Bot Service.\n",
    "- Dialogflow is favored for its ease of use, built-in NLU features, context management, and integration capabilities.\n",
    "- Custom implementations can leverage APIs from OpenAI (GPT models), open-source LLMs (Hugging Face, Bloom), or cloud foundational models (AWS Bedrock) for flexibility and advanced capabilities   .\n",
    "\n",
    "## Advantages of Chatbots over Human Customer Support\n",
    "\n",
    "- **Scalability:** Easily scale to handle growing customer bases by deploying cloud resources, unlike human staffing which has practical limits.\n",
    "- **24/7 Availability:** Chatbots operate continuously without breaks or shift changes.\n",
    "- **Cost Efficiency:** Cheaper to maintain than large human support teams.\n",
    "- **Improved Customer Experience:** Instant responses prevent delays common in human call centers during peak loads    .\n",
    "\n",
    "## Summary of Chatbot Use Cases and Next Steps\n",
    "\n",
    "- Chatbots are widely used in customer service, ordering systems, banking, and travel booking.\n",
    "- Flow-based chatbots suit structured tasks with limited scope.\n",
    "- NLP and open-ended chatbots handle more complex, natural language interactions.\n",
    "- Upcoming tutorials will build an end-to-end Dialogflow-based chatbot for a real business use case, covering data collection, cleaning, and deployment   .\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Key Insight:** Despite the excitement around large language models like ChatGPT, traditional chatbot frameworks remain vital due to their ease of setup, cost-effectiveness, and integration strengths in many real-world applications. Custom AI solutions require more technical expertise and investment, making them complementary rather than replacement technologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd66043",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
